# Logistic Regression paper with Paul

**Caution: in a highly developmental stage! See Section  \@ref(caution).**


The focus of this paper is on medical studies with a binary response,
such as survival/death or healthy/diseased. Typically, we are more
interested in the probability of the worse outcome and therefore refer
to the probability as "risk", denoted by $\pi$. Ultimately, we would
like to know how certain covariatessuch as smokingaffect the risk. As a
means of comparison, it is common to define "exposure" and "baseline"
levels of each covariate. For instance, a smoker (exposed) as compared
to a non-smoker (baseline) in the case of a binary variable, or exposure
to a certain amount of lead as compared to the population average
baseline in the case of a continuous variable. Some popular
interpretable quantities (IQ's) which compare exposure risk $\pi_{E}$ to
baseline (unexposed) risk $\pi_{B}$ are

1.  the *risk difference*, $\pi_{E}-\pi_{B}$,

2.  the reciprocal risk difference, or *number needed to treat* (NNT)
    (or sometimes *number needed to harm*),

3.  the *relative risk*, $\pi_{E}/\pi_{B}$, and

4.  the *odds ratio*,

These IQ's consider all other factors to be equal.

The techniques outlined in this paper are generic to either experimental
or observational studies, though a strong focus is put on observational
studies. In these situations, we must take into account some confounders
which would have otherwise been controlled for in an experimental study.
As such, we would like to compute the IQ's after adjusting for the
confounders, which can be done by including the confounders as other
covariates in the model. To distinguish between the types of covariates,
the non-confounder explanatory variables will be called interaction
variables.

Let $Y$ denote the binary response. It is convenient to express this
variable as $1$ for the outcome of interest (such as diseased) and $0$
for the other outcome, because then its expectation is equal to the
risk. Then we have
$$Y|X=x \sim \text{Bernoulli}\left(\pi\left(x\right)\right),$$
where
$\pi\left(x\right)=E\left(Y\left|X=x\right.\right)$.
The difficulty lies in specifying a model for
$\pi\left(x\right)$. Traditionally, it is modelled using a
generalized linear model (GLM), (EQUATION),
where $g$ is some increasing function known as the link function, and
$\boldsymbol{\beta}=\left(\beta_{1},\ldots,\beta_{p}\right)^{T}$ is a
vector of regression parameters. Note that for our purposes it is more
convenient to keep the absolute term $\beta_{0}$ separate from the
vector $\boldsymbol{\beta}$. We will discuss two GLM models in Section
2, then discuss the LEXPIT approach of Kovalchik and others (2013) in
Section 3. Section 4 is reserved for an appraisal of the LEXPIT method,
and a simulation in Section 5 evaluates the LEXPIT model empirically. 

## The Traditional Approach

### The Linear Probability Model

A first inclination may be to model the mean as one would in the case of
multiple linear regression that is, as a linear combination of the
covariates. The link function $g$ is the identity, and the model becomes (EQUATION).
Kovalchik and others (2013) refer to this as the "Binomial Linear
Model", or BLM, though it is more commonly known as the "Linear
Probability Model", or LPM (see, for example, Aldrich and Nelson, 1984;
Amemiya, 1977; Horrace and Oaxaca, 2006). In this paper, the model is
referred to as the LPM.

Before proceeding with any further discussion, the validity of this
model must be enforced. The Bernoulli distribution requires
$0\leq\pi(X)\leq1$ for all $x$
$\boldsymbol{x}\in XX$ to be a valid distribution. Validity
can be ensured by restricting the parameter space of
$\left(\beta_{0},\boldsymbol{\beta}\right)$ to .
However, the parameter space can be severely restricted depending on the
covariate space. For example, if predictor $k$ of
is unbounded, then the only allowable value for
$\beta_{k}$ is zero. In other words, any covariate in the LPM that has
an unbounded range cannot technically be included in the LPM. Further,
even if component $k$ is bounded, if it has a large
range, then the slope is restricted to be small.

One reason why the LPM is used, despite the above restrictions, is for
access to *constant interpretable quantities* that is, IQ's discussed in
section 1 which do not depend on other covariates. In an LPM, the risk
difference by increasing $X_{k}$ by one unit is simply given by
$\beta_{k}$, and the NNT is $1/\beta_{k}$. However, the relative risk
and odds ratio are non-constant, as they are functions of the other
covariates.

Since the LPM is just a multiple linear regression model, the regression
parameters can be estimated without bias by ordinary least squares
(OLS). However, we do not necessarily have homoskedastic errors, since $Var(Y|X=x)$
differs with the covariates. As such, the efficiency of the OLS
estimator can be improved by the weighted least squares estimator with
weights $1/\sigma\left(\boldsymbol{x}\right)$. Since these weights are
unknown, an iterative algorithm is used, which calculates weights using
the fitted probabilities
from parameter estimates of the previous step to compute a new
"re-weighted" estimator. Iterating this beginning with the OLS estimator
converges to the *iteratively re-weighted least squares* (IRLS)
estimator. Amemiya (1977) shows that the IRLS is identical to the
maximum likelihood estimator (MLE).

An alternative model which is sometimes confused for the LPM (for
example, see Horrace and Oaxaca, 2006) is to allow for an arbitrary
parameter space by taking
$\pi(x)$ to be zero when $\eta$ is less than
zero, and unity otherwise. This model, which I call the "truncated LPM"
(TLPM), is (EQUATION)
where the inverse-link function $T$ is the ramp function (actually, $T$ is not
quite an inverse-link function because it is non-invertible, but this is
unimportant). However, as one can see by the differing link function,
this is not the LPM, although it is often mistaken for the LPM. Horrace
and Oaxaca (2006) mistake the TLPM for the LPM, and in doing so, show
that estimation of the model parameters through OLS or IRLS provide
biased and inconsistent estimators. This is a good reason why the TLPM
should not be used unless a different method of estimation is
considered.

### The Logistic Model

To rid the parameter space of restrictions, one may
consider link functions similar to the ramp function (preferably smooth) to ensure
$0\leq\pi(x)\leq1$. Popular choices are logit, probit, the inverse Gumbel distribution function, or the angular function (Cox and Snell, 1989). Each of these
link functions ensures a valid probability for an arbitrary parameter
space. The logit
link function is a popular choice because it has the best
interpretability. It models the log-odds as a linear function of the
covariates that is, (EQUATION).
This model is known as the logistic regression model, and can be written
equivalently as ,
where 
is the inverse logit function. The logistic model
stands out over models with other link functions because a constant IQ
can be obtained from it the odds ratio by increasing $X_{k}$ by one unit
is simply $\exp\left(\beta_{k}\right)$. However, of the interpretable
quantities discussed in Section 1, the odds ratio is the most difficult
to interpret. Though, if both risks are smallthe "rare disease
assumption" with risks under $0.1$ then the odds ratio is a good
approximation to the relative risk, which is easier to interpret. The
lack of an easy interpretable constant IQ is why some researchers will
opt for the LPM instead of the logistic model when the rare disease
assumption is invalid. Indeed, this is one major reason behind the study
done by Kovalchik and others (2013). One other method to decide is
through Goodness of Fit criteria, which was the other deciding factor of
Kovalchik and others.

Conveniently, the log odds appears in the likelihood of the logistic
model, which simplifies some computations. This leads to the MLE which
solves the equation .
However, occasionally it is possible that no MLE exists when there is a
$\left(b_{0},\boldsymbol{b}\right)\in\mathcal{F}$ such that
$b_{0}+\boldsymbol{x}_{i}^{T}\boldsymbol{b}>0$ has $Y_{i}=1$ and
$b_{0}+\boldsymbol{x}_{i}^{T}\boldsymbol{b}<0$ has $Y_{i}=0$ for each
$i=1,\ldots,n$ (Albert and Anderson, 1984). This is called the case of
"complete separation", and the likelihood has no maximum, so a "perfect
fit" is made by infinitely pushing the covariate data to the tails of
the expit curve. This is not an issue with the LPM
model.
