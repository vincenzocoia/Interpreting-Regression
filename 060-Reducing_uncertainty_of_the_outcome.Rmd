# Prediction: harnessing the signal {-}

```{r}
suppressPackageStartupMessages(library(tidyverse))
```


# Reducing uncertainty of the outcome: including predictors

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

## Variable terminology

In supervised learning:

- The output is a random variable, typically denoted $Y$. 
- The input(s) variables (which may or may not be random), if there are $p$ of them, are typically denoted $X_1$, ..., $X_p$ -- or just $X$ if there's one. 

There are many names for the input and output variables. Here are some (there are more, undoubtedly):

- __Output__: response, dependent variable. 
- __Input__: predictors, covariates, features, independent variables, explanatory variables, regressors. 

In BAIT 509, we will use the terminology _predictors_ and _response_.

### Variable types

Terminology surrounding variable types can be confusing, so it's worth going over it. Here are some non-technical definitions. 

- A __numeric__ variable is one that has a quantity associated with it, such as age or height. Of these, a numeric variable can be one of two things:
- A __categorical__ variable, as the name suggests, is a variable that can be one of many categories. For example, type of fruit; success or failure.  


## Irreducible Error

The concept of __irreducible error__ is paramount to supervised learning. Next time, we'll look at the concept of _reducible_ error. 

When building a supervised learning model (like linear regression), we can never build a perfect forecaster -- even if we have infinite data!

Let's explore this notion. When we hypothetically have an infinite amount of data to train a model with, what we actually have is the _probability distribution_ of $Y$ given any value of the predictors. The uncertainty in this probability distribution is the __irreducible error__.

__Example__: Let's say $(X,Y)$ follows a (known) bivariate Normal distribution. Then, for any input of $X$, $Y$ has a _distribution_. Here are some examples of this distribution for a few values of the predictor variable (these are called _conditional_ distributions, because they're conditional on observing particular values of the predictors).

```{r, fig.width=6, echo=FALSE}
xvals <- c(-1.5, 0, 3)
ggplot(tibble(x=c(-5,7)), aes(x)) +
    stat_function(fun=function(x) dnorm(x, xvals[1]),
                  mapping=aes(fill=paste0("x=", xvals[1])), 
                  geom="area", alpha=0.5) +
    stat_function(fun=function(x) dnorm(x, xvals[2]),
                  mapping=aes(fill=paste0("x=", xvals[2])), 
                  geom="area", alpha=0.5) +
    stat_function(fun=function(x) dnorm(x, xvals[3]),
                  mapping=aes(fill=paste0("x=", xvals[3])), 
                  geom="area", alpha=0.5) +
    theme_bw() +
    scale_fill_brewer("Predictor\nvalue", palette="Dark2") +
    labs(x="y", y="Density Function") +
    theme(legend.position="top")
```

This means we cannot know what $Y$ will be, no matter what! What's one to do?

- In __regression__ (i.e., when $Y$ is numeric, as above), the go-to standard is to predict the _mean_ as our best guess. 
    - We typically measure error with the __mean squared error__ = average of (observed-predicted)^2. 
- In __classification__, the conditional distributions are categorical variables, so the go-to standard is to predict the _mode_ as our best guess (i.e., the category having the highest probability). 
    - A typical measurement of error is the __error rate__ = proportion of incorrect predictions.
    - A more "complete" picture of error is the __entropy__, or equivalently, the __information measure__. 

In Class Meeting 07, we'll look at different options besides the mean and the mode.

An important concept is that _predictors give us more information about the response_, leading to a more certain distribution. In the above example, let's try to make a prediction when we don't have knowledge of predictors. Here's what the distribution of the response looks like:

```{r, fig.width=6, echo=FALSE}
ggplot(tibble(x=c(-5,7)), aes(x)) +
    stat_function(fun=function(x) dnorm(x, sd=2),
                  geom="area", fill="gray", alpha=0.5) +
    theme_bw() +
    labs(x="y", y="Density Function")
```

This is much more uncertain than in the case where we have predictors!

## In-class Exercises: Irreducible Error

**NOT REQUIRED FOR PARTICIPATION**

### Oracle regression

Suppose you have two independent predictors, $X_1, X_2 \sim N(0,1)$, and the conditional distribution of $Y$ is
$$ Y \mid (X_1=x_1, X_2=x_2) \sim N(5-x_1+2x_2, 1). $$
From this, it follows that:

- The conditional distribution of $Y$ given _only_ $X_1$ is
$$ Y \mid X_1=x_1 \sim N(5-x_1, 5). $$
- The conditional distribution of $Y$ given _only_ $X_2$ is
$$ Y \mid X_2=x_2 \sim N(5+2x_2, 2). $$
- The (marginal) distribution of $Y$ (not given any of the predictors) is
$$ Y \sim N(5, 6). $$

The following R function generates data from the joint distribution of $(X_1, X_2, Y)$. It takes a single positive integer as an input, representing the sample size, and returns a `tibble` (a fancy version of a data frame) with columns named `x1`, `x2`, and `y`, corresponding to the random vector $(X_1, X_2, Y)$, with realizations given in the rows. 

```
genreg <- function(n){
    x1 <- rnorm(n)
    x2 <- rnorm(n)
    eps <- rnorm(n)
    y <- 5-x1+2*x2+eps
    tibble(x1=x1, x2=x2, y=y)
}
```


1. Generate data -- as much as you'd like.

```
dat <- genreg(1000)
```


2. For now, ignore the $Y$ values. Use the means from the distributions listed above to predict $Y$ under four circumstances:
    1. Using both the values of $X_1$ and $X_2$.
    2. Using only the values of $X_1$.
    3. Using only the values of $X_2$.
    4. Using neither the values of $X_1$ nor $X_2$. (Your predictions in this case will be the same every time -- what is that number?)
    
```
dat <- mutate(dat,
       yhat = FILL_THIS_IN,
       yhat1 = FILL_THIS_IN,
       yhat2 = FILL_THIS_IN,
       yhat12 = FILL_THIS_IN)
```
    

3. Now use the actual outcomes of $Y$ to calculate the mean squared error (MSE) for each of the four situations. 
    - Try re-running the simulation with a new batch of data. Do your MSE's change much? If so, choose a larger sample so that these numbers are more stable.
    
```
(mse <- mean((dat$FILL_THIS_IN - dat$y)^2))
(mse1 <- mean((dat$FILL_THIS_IN - dat$y)^2))
(mse2 <- mean((dat$FILL_THIS_IN - dat$y)^2))
(mse12 <- mean((dat$FILL_THIS_IN - dat$y)^2))
knitr::kable(tribble(
    ~ Case, ~ MSE,
    "No predictors", mse,
    "Only X1", mse1,
    "Only X2", mse2,
    "Both X1 and X2", mse12
))
```

    
4. Order the situations from "best forecaster" to "worst forecaster". Why do we see this order?


### Oracle classification

Consider a categorical response that can take on one of three categories: _A_, _B_, or _C_. The conditional probabilities are:
$$ P(Y=A \mid X=x) = 0.2, $$
$$ P(Y=B \mid X=x) = 0.8/(1+e^{-x}), $$

To help you visualize this, here is a plot of $P(Y=B \mid X=x)$ vs $x$ (notice that it is bounded above by 0.8, and below by 0).

```{r}
ggplot(tibble(x=c(-7, 7)), aes(x)) +
    stat_function(fun=function(x) 0.8/(1+exp(-x))) +
    ylim(c(0,1)) +
    geom_hline(yintercept=c(0,0.8), linetype="dashed", alpha=0.5) +
    theme_bw() +
    labs(y="P(Y=B|X=x)")
```

Here's an R function to generate data for you, where $X\sim N(0,1)$. As before, it accepts a positive integer as its input, representing the sample size, and returns a tibble with column names `x` and `y` corresponding to the predictor and response. 

```
gencla <- function(n) {
    x <- rnorm(n) 
    pB <- 0.8/(1+exp(-x))
    y <- map_chr(pB, function(t) 
            sample(LETTERS[1:3], size=1, replace=TRUE,
                   prob=c(0.2, t, 1-t-0.2)))
    tibble(x=x, y=y)
}
```


1. Calculate the probabilities of each category when $X=1$. What about when $X=-2$? With this information, what would you classify $Y$ as in both cases?
    - BONUS: Plot these two conditional distributions. 

```
## X=1:
(pB <- FILL_THIS_IN)
(pA <- FILL_THIS_IN)
(pC <- FILL_THIS_IN)
ggplot(tibble(p=c(pA,pB,pC), y=LETTERS[1:3]), aes(y, p)) +
    geom_col() +
    theme_bw() +
    labs(y="Probabilities", title="X=1")
## X=-2
(pB <- FILL_THIS_IN)
(pA <- FILL_THIS_IN)
(pC <- FILL_THIS_IN)
ggplot(tibble(p=c(pA,pB,pC), y=LETTERS[1:3]), aes(y, p)) +
    geom_col() +
    theme_bw() +
    labs("Probabilities", title="X=-2")
```

2. In general, when would you classify $Y$ as _A_? _B_? _C_?

### (BONUS) Random prediction

You might think that, if we know the conditional distribution of $Y$ given some predictors, why not take a random draw from that distribution as our prediction? After all, this would be simulating nature.

The problem is, this prediction doesn't do well. 

Re-do the regression exercise above (feel free to only do Case 1 to prove the point), but this time, instead of using the mean as a prediction, use a random draw from the conditional distributions. Calculate the MSE. How much worse is it? How does this error compare to the original Case 1-4 errors?

### (BONUS) A more non-standard regression

The regression example given above is your perfect, everything-is-linear-and-Normal world. Let's see an example of a joint distribution of $(X,Y)$ that's _not_ Normal. 

The joint distribution in question can be respresented as follows:
$$ Y|X=x \sim \text{Beta}(e^{-x}, 1/x), $$
$$ X \sim \text{Exp}(1). $$

Write a formula that gives a prediction of $Y$ from $X$ (you might have to look up the formula for the mean of a Beta random variable). Generate data, and evaluate the MSE. Plot the data, and the conditional mean as a function of $x$ overtop. 

### (BONUS) Oracle MSE

What statistical quantity does the mean squared error (MSE) reduce to when we know the true distribution of the data? Hint: if each conditional distribution has a certain variance, what then is the MSE?

What is the error rate in the classification setting?

--------

## Learning Objectives

From today's class, students are expected to be able to:

- Calculate conditional distributions when giving a full distribution.
- Calculate marginal distributions from a joint distribution.
- Obtain the marginal mean from conditional means and marginal probabilities, using the law of total expectation.
- Use the law of total probability to convert between conditional + marginal distributions, and joint distributions.
- Describe the consequences of independent random variables.
- Calculate and describe the pros and cons of dependence measures: covariance, correlation, and kendall's tau.


## Conditional Distributions (15 min)

Probability distributions describe an uncertain outcome, but what if we have partial information?

Consider the example of ships arriving at the port of Vancouver again. Each ship will stay at port for a random number of days, which we'll call the _length of stay_ (LOS) or $D$, according to the following (made up) distribution:

```{r, echo = FALSE, results = "as.is"}
los <- tribble(
	~ ndays, ~ p,
	1, 0.25,
	2, 0.35,
	3, 0.2,
	4, 0.1,
	5, 0.1
)
knitr::kable(los, col.names = c("Length of Stay (LOS)", "Probability"))
```

```{r, echo = FALSE}
ggplot(los, aes(ndays, p)) +
	geom_col(fill = "maroon") +
	theme_bw() +
	labs(x = "Length of stay (D)",
		 y = "Probability")
```


Suppose a ship has been at port for 2 days now, and it'll be staying longer. What's the distribution of length-of-stay now? Using symbols, this is written as $P(D = d \mid D > 2)$, where the bar "|" reads as "given" or "conditional on", and this distribution is called a __conditional distribution__. We can calculate a conditional distribution in two ways: a "table approach" and a "formula approach".

__Table approach__:

1. Subset the pmf table to only those outcomes that satisfy the _condition_ ($D > 2$ in this case). You'll end up with a "sub table".
2. Re-normalize the remaining probabilities so that they add up to 1. You'll end up with the _conditional distribution_ under that condition.

__Formula approach__: In general for events $A$ and $B$, the conditional probability formula is $$P(A \mid B) = \frac{P(A \cap B)}{P(B)}.$$ 

For the ship example, the event $A$ is $D = d$ (for all possible $d$'s), and the event $B$ is $D > 2$. Plugging this in, we get 
$$P(D = d \mid D > 2) = \frac{P(D = d \cap D > 2)}{P(D > 2)} = \frac{P(D = d)}{P(D > 2)} \text{ for } d = 3,4,5.$$

The only real "trick" is the numerator. How did we reduce the convoluted event $D = d \cap D > 2$ to the simple event $D = d$ for $d = 3,4,5$? The trick is to go through all outcomes and check which ones satisfy the requirement $D = d \cap D > 2$. This reduces to $D = d$, as long as $d = 3,4,5$.

## Joint Distributions (25 min)

So far we've only considered one random variable at a time. Its distribution is called *univariate* because there's just one variable. But, we very often have more than one random variable. 

Let's start by considering ... We can visualize this as a _joint distribution_:


Don't be fooled, though! This is not really any different from what we've already seen. We can still write this a univariate distribution with four categories. This is useful to remember when we're calculating probabilities.

| Outcome | Probability |
|------|------|
| `HH` | 0.25 |
| `HT` | 0.25 |
| `TH` | 0.25 |
| `TT` | 0.25 |

Viewing the distribution as a (2-dimensional) matrix instead of a (1-dimensional) vector turns out to be more useful when determining properties of individual random variables.

### Example: Length of Stay vs. Gang Demand

Throughout today's class, we'll be working with the following joint distribution of _length of stay_ of a ship, and its _gang demand_. 

```{r, echo = FALSE}
here::here("supplementary", "los_gang_joint.R") %>% source()
j <- joint(-0.75)
j_mat <- j %>% 
	mutate(los  = str_c("__LOS = ", los, "__"),
		   gang = str_c("Gangs = ", gang)) %>% 
	pivot_wider(id_cols = los, names_from = gang, values_from = p) %>% 
	column_to_rownames("los") %>% 
	as.matrix()
knitr::kable(j_mat, digits = 4)
```

The joint distribution is stored in "tidy format" in an R variable named `j`:

```{r}
DT::datatable(j, rownames = FALSE)
```

### Marginal Distributions

We've just specified a joint distribution of _length of stay_ and _gang request_. But, we've previously specified a distribution for these variables individually. These are not things that can be specified separately:

- If you have a joint distribution, then the distribution of each individual variable follows as a consequence.
- If you have the distribution of each individual variable, you still don't have enough information to form the joint distribution between the variables.

The distribution of an individual variable is called the __marginal distribution__ (sometimes just "marginal" or "margin"). The word "marginal" is not really needed when we're talking about a random variable -- there's no difference between the "marginal distribution of length of stay" and the "distribution of length of stay", we just use the word "marginal" if we want to emphasize the distribution is being considered _in isolation_ from other related variables. 

### Calculating Marginals from the Joint

There's no special way of calculating a marginal distribution from a joint distribution. As usual, it just involves adding up the probabilities corresponding to relevant outcomes.

For example, to compute the marginal distribution of length of stay (LOS), we'll first need to calculate $P(\text{LOS} = 1)$. Using the joint distribution of _length of stay_ and _gang request_, the outcomes that satisfy this requirement are the entire first row of the probability table. 
It follows that the marginal distribution of LOS can be obtained by adding up each row. For the marginal of gang requests, just add up the columns.

Here's the marginal of LOS (don't worry about the code, you'll learn more about this code in DSCI 523 next block). Notice that the distribution of LOS is the same as before!

```{r}
j %>% 
	group_by(los) %>% 
	summarize(p = sum(p)) %>% 
	knitr::kable(col.names = c("Length of Stay", "Probability"))
```

Similarly, the distribution of gang request is the same as from last lecture:

```{r}
j %>% 
	group_by(gang) %>% 
	summarize(p = sum(p)) %>% 
	knitr::kable(col.names = c("Gang request", "Probability"))
```

### Conditioning on one Variable

What's usually more interesting than a joint distribution are conditional distributions, when other variables are fixed. This is a special type of conditional distribution and an extremely important type of distribution in data science. 

For example, a ship is arriving, and they've told you they'll only be staying for 1 day. What's the distribution of their gang demand under this information? That is, what is $P(\text{gang} = g \mid \text{LOS} = 1)$ for all possible $g$?

__Table approach__:

1. Isolating the outcomes satisfying the condition ($\text{LOS} = 1$), we obtain the first row:

```{r, echo = FALSE}
j_mat[1, , drop = FALSE] %>% 
	knitr::kable(digits = 4, col.names = str_c("Gangs: ", 1:4), row.names = FALSE)
```

2. Now, re-normalize the probabilities so that they add up to 1, by dividing them by their sum, which is `r sum(j_mat[1, ])`:

```{r, echo = FALSE}
(j_mat[1, , drop = FALSE] / sum(j_mat[1, ])) %>% 
	knitr::kable(digits = 4, col.names = str_c("Gangs: ", 1:4), row.names = FALSE)
```

__Formula Approach__: Applying the formula for conditional probabilities, we get 
$$P(\text{gang} = g \mid \text{LOS} = 1) = \frac{P(\text{gang} = g, \text{LOS} = 1)}{P(\text{LOS} = 1)},$$
which is exactly row 1 divided by `r sum(j_mat[1, ])`.

Here's a plot of this distribution. For comparison, we've also reproduced its marginal distribution.

```{r, echo = FALSE}
j_compare <- j %>% 
	group_by(gang) %>% 
	summarize(Marginal = sum(p),
			  Conditional = p[los == 1]) %>% 
	mutate(Conditional = Conditional / sum(Conditional)) %>% 
	pivot_longer(cols = c("Marginal", "Conditional"), 
				 names_to = "type", 
				 values_to = "p")
ggplot(j_compare, aes(gang, p)) +
	facet_wrap(~ type) +
	geom_col(fill = "maroon") +
	theme_bw() +
	labs(x = "Gang requests",
		 y = "Probability")
```

Interpretation: given information, about length of stay, we get an updated picture of the distribution of gang requests. Useful for decision making!

### Law of Total Probability/Expectation

Quite often, we know the conditional distributions, but don't directly have the marginals. In fact, most of regression and machine learning is about seeking conditional means!

For example, suppose you have the following conditional means of gang request given the length of stay of a ship.

```{r, echo = FALSE}
j_cond <- j %>% 
	group_by(los) %>% 
	summarize(mean   = sum(gang * p) / sum(p),
			  p_marg = sum(p))
ggplot(j_cond, aes(los, mean)) +
	geom_point() +
	geom_line() +
	labs(x = "Length of Stay",
		 y = "Mean Gang Request\nGiven Length of Stay") +
	theme_bw()
```

This curve is called a __model function__, and is useful if we want to predict a ship's daily gang request if we know their length of stay. But what if we don't know their length of stay, and we want to produce an expected gang request? We can use the marginal mean of gang request!

In general, a marginal mean can be computed from the _conditional means_ and the _probabilities of the conditioning variable_. The formula, known as the __law of total expectation__, is
$$E(Y) = \sum_x E(Y \mid X = x) P(X = x).$$

Here's a table that outlines the relevant values:

```{r, echo = FALSE}
j_cond %>% 
	knitr::kable(col.names = c("Length of Stay (LOS)", "E(Gang | LOS)", "P(LOS)"))
```

Multiplying the last two columns together, and summing, gives us the marginal expectation: `r sum(j_cond$mean * j_cond$p_marg)`.

Also, remember that probabilities are just means, so the result extends to probabilities:
$$P(Y = y) = \sum_x P(Y = y \mid X = x) P(X = x)$$
This is actually a generalization of the law of total probability we saw before: $P(Y=y)=\sum_x P(Y = y, X = x)$.

### Exercises (10 min)

In pairs, come to a consensus with the following three questions.

1. Given the conditional means of gang requests, and the marginal probabilities of LOS in the above table, what's the expected gang requests, given that the ship captain says they won't be at port any longer than 2 days? In symbols, $$E(\text{Gang} \mid \text{LOS} \leq 2).$$

2. What's the probability that a new ship's _total_ gang demand equals 4? In symbols, $$P(\text{Gang} \times \text{LOS} = 4).$$

3. What's the probability that a new ship's _total_ gang demand equals 4, given that the ship won't stay any longer than 2 days? In symbols, $$P(\text{Gang} \times \text{LOS} = 4 \mid \text{LOS} \leq 2).$$


## Multivariate Densities/pdf's

Recall the joint _pmf_ (discrete) from Lecture 4, between gang demand and length-of-stay:

```{r}
here::here("supplementary", "los_gang_joint.R") %>% source()
j <- joint(-0.75)
j_mat <- j %>% 
	mutate(los  = str_c("__LOS = ", los, "__"),
		   gang = str_c("Gangs = ", gang)) %>% 
	pivot_wider(id_cols = los, names_from = gang, values_from = p) %>% 
	column_to_rownames("los") %>% 
	as.matrix()
knitr::kable(j_mat, digits = 4)
```

Each entry in the table corresponds to the probability of that unique row (LOS value) and column (Gang value). These probabilities add to 1.

For the _continuous_ case, instead of rows and columns, we have an x- and y-axis for our two variables, defining a region of possible values. For example, if two marathon runners can only finish a marathon between 5.0 and 5.5 hours each, and their end times are totally random, then the possible values are indicated by a square in the following plot:

```{r}
marathon_space <- tibble(x1 = 5, x2 = 5.5, y1 = 5, y2 = 5.5) %>% 
	ggplot() +
	geom_rect(aes(xmin = x1, xmax = x2, ymin = y1, ymax = y2), alpha = 0.5) +
	scale_x_continuous("Marathon time:\nRunner 1", limits = c(4, 6)) +
	scale_y_continuous("Marathon time:\nRunner 2", limits = c(4, 6)) +
	theme_bw()
marathon_space + 
	ggtitle("Marathon runners: Outcome space")
```

Each point in the square is like an entry in the joint pmf table in the discrete case, except now instead of holding a probability, it holds a _density_. The density _function_, then, is a _surface_ overtop of this square (or in general, the outcome space). That is, it's a function that takes two variables (marathon time for Runner 1 and Runner 2), and calculates a single density value from those two points. This function is called a __bivariate density function__.

Here's an example of what a 2D pdf might look like: https://scipython.com/blog/visualizing-the-bivariate-gaussian-distribution/

__Notation__: For two random variables $X$ and $Y$, their joint density/pdf evaluated at the points $x$ and $y$ is usually denoted
$$f_{X,Y}(x,y),$$
or sometimes less rigorously, as just
$$f(x, y).$$

### Conditional Distributions, revisited (15 min) 

Remember the formula for conditional probabilities: for events $A$ and $B$,
$$P(A \mid B) = \frac{P(A \cap B)}{P(B)}.$$
But, this is only true if $P(B) \neq 0$, and it's not useful if $P(A) = 0$ -- two situations we're faced with in the continuous world!

#### When $P(A) = 0$

To describe this situation, let's use a univariate continuous example: the example of monthly expenses. 

Suppose the month is half-way over, and you find that you only have \$2500 worth of expenses so far! What's the distribution of this month's total expenditures now, given this information? If we use the law of conditional probability, we would get a formula that's not useful: letting $X = \text{Expense}$,
$$P(X = x \mid X \geq 2500) = \frac{P(X = x)}{P(X \geq 2500)} \ \ \ \text{(no!)}$$

This is no good, because the outcome $x$ has a probability of 0. This equation just simplies to 0 = 0, which is not useful.

Instead, in general, we replace probabilities with densities. In this case, what we actually have is:
$$f(x \mid X \geq 2500) = \frac{f(x)}{P(X \geq 2500)} \ \text{ for } x \geq 2500,$$
and $f(x \mid X \geq 2500) = 0$ for $x < 2500$.

Notice from the formula that the resulting density is just the original density confined to $x \geq 2500$, and re-normalized to have area 1. This is what we did in the discrete case!

The monthly expense example has expenditures $X \sim$ LN(`r expense$meanlog`, `r expense$sdlog`). Here is its marginal distribution and the conditional distribution. Notice the conditional distribution is just a segment of the marginal, and then re-normalized to have area 1.

```{r, fig.width = 5, fig.height = 2}
expense$dcond <- function(x) if_else(x < 2500, 0, expense$ddist(x) / (1 - expense$pdist(2500)))
tibble(x = seq(0, 10000, length.out = 1000)) %>% 
	mutate(Marginal    = expense$ddist(x),
		   Conditional = expense$dcond(x)) %>% 
	pivot_longer(Marginal:Conditional, names_to = "distribution", values_to = "Density") %>% 
	ggplot(aes(x, Density)) +
	facet_wrap(~ distribution) +
	geom_line() +
	theme_bw()
```

#### When $P(B) = 0$

To describe this situation, let's use the marathon runners' example again. 

Runner 1 ended up finishing in 5.2 hours. What's the distribution of Runner 2's time? Letting $X$ be the time for Runner 1, and $Y$ for Runner 2, we're asking for $f_{Y|X}(y \mid X = 5.2)$.

But wait! Didn't we say earlier that $P(X = 5.2) = 0$? This is the bizarre nature of continuous random variables. Although no outcome is possible, we must observe some outcome in the end. In this case, the stopwatch used to calculate run time has rounded the true run time to 5.2h, even though in reality, it would have been something like 5.2133843789373... hours.

As before, plugging in the formula for conditional probabilities won't work. But, as the case when $P(A) = 0$, we can in general replace probabilities with densities. We end up with
$$f_{Y|X}(y \mid 5.2) = \frac{f_{Y,X}(y, 5.2)}{f_X(5.2)}.$$

This formula is true in general
$$f_{Y|X}(y \mid x) = \frac{f_{Y,X}(y, x)}{f_X(x)}.$$
In fact, this formula is even true for both pdf's and pmf's!


## Dependence concepts

A big part of data science is about harvesting the relationship between $X$ and $Y$, often called the _dependence_ between $X$ and $Y$.

### Independence (5 min)

Informally, $X$ and $Y$ are __independent__ if knowing something about one tells us nothing about the other.

Formally, the definition of $X$ and $Y$ being independent is: 
$$P(X = x \cap Y = y) = P(X = x) P(Y = y).$$ More usefully and intuitively, it's better to think of independence such that conditioning on an independent variable tells us nothing:

$$P(Y = y \mid X = x) = P(Y = y).$$

This is far less interesting than when there's dependence, which implies that there are relationships between variables. 

### Measures of dependence (15 min)

When there is dependence, it's often useful to measure the strength of dependence. Here are some measurements. 

#### Covariance and Pearson's Correlation

__Covariance__ is one common way of measuring dependence between two random variables. The idea is to take the average "signed area" of rectangles constructed between a sampled point and the mean, with the sign being determined by "concordance" relative to the mean:

- Concordant means $x < \mu_x$ and $y < \mu_y$, OR $x > \mu_x$ and $y > \mu_y$ -- gets positive area.
- Discordant means $x < \mu_x$ and $y > \mu_y$, OR $x > \mu_x$ and $y < \mu_y$ -- gets negative area. 

Here is a random sample of 10 points, with the 10 rectangles constructed with respect to the mean. Sign is indicated by colour. The covariance is the mean signed area.

```{r, echo = FALSE}
set.seed(4)
mu_los <- j %>% 
	group_by(los) %>% 
	summarize(p = sum(p)) %>% 
	summarize(mean = sum(los * p)) %>% 
	pull(mean)
mu_gang <- j %>% 
	group_by(gang) %>% 
	summarize(p = sum(p)) %>% 
	summarize(mean = sum(gang * p)) %>% 
	pull(mean)
j_sample <- j %>% 
	sample_n(10, replace = TRUE, weight = p)
j_sample %>% 
	mutate(Sign = if_else((los > mu_los & gang > mu_gang) | 
						  	(los < mu_los & gang < mu_gang), 
						  "Positive", 
						  "Negative")) %>% 
	ggplot(aes(los, gang)) +
	geom_rect(aes(xmax = los, ymax = gang, fill = Sign), 
			  xmin = mu_los, ymin = mu_gang,
			  alpha = 0.1, colour = "black") +
	geom_point(aes(x = los, y = gang)) +
	geom_point(x = mu_los, y = mu_gang, colour = "red") +
	theme_bw() +
	labs(x = "Length of Stay (days)",
		 y = "Gang Request") +
	lims(x = c(1, 5),
		 y = c(1, 4))
```

Formally, the definition is
$$\mathrm{Cov(X, Y)} = E[(X-\mu_X)(Y-\mu_Y)],$$
where $\mu_Y=E(Y)$ and $\mu_X=E(X)$. This reduces to a more convenient form,
$$\text{Cov}(X,Y)=E(XY)-E(X)E(Y)$$

In R, you can calculate the empirical covariance using the `cov` function:

```{r}
cov(j_sample$los, j_sample$gang)
```

In the above example, the boxes are more often negative, so the covariance (and the "direction of dependence") is negative. For the above example, the larger the LOS, the smaller the gang demand -- this inverse relationship is indicative of negative covariance. Other interpretations of the sign:

- Positive covariance indicates that an increase in one variable is associated with an increase in the other variable. 
- Zero covariance indicates that there is no _linear_ trend -- but this does not necessarily mean that $X$ and $Y$ are independent!

It turns out covariance by itself isn't very interpretable, because it depends on the scale (actually, spread) of $X$ and $Y$. For example, multiply $X$ by 10, and suddenly the box sizes increase by a factor of 10, too, influencing the covariance. 

__Pearson's correlation__ fixes the scale problem by standardizing the distances according to standard deviations $\sigma_X$ and $\sigma_Y$, defined as
$$\text{Corr}(X, Y)
= E\left[ 
   \left(\frac{X-\mu_X}{\sigma_X}\right) 
   \left(\frac{Y-\mu_Y}{\sigma_Y}\right)
 \right] 
=\frac{\text{Cov}(X, Y)}{\sqrt{\text{Var}(X)\text{Var}(Y)}}.$$
As a result, it turns out that
$$-1 \leq \text{Corr}(X, Y) \leq 1.$$

The Pearson's correlation measures the _strength of **linear** dependence_:

- -1 means perfect negative linear relationship between $X$ and $Y$.
- 0 means no linear relationship (Note: this does not mean independent!)
- 1 means perfect positive linear relationship.

In R, you can calculate the empirical Pearson's correlation using the `cor` function:

```{r}
cor(j_sample$los, j_sample$gang)
```

Pearson's correlation is ubiquitous, and is often what is meant when "correlation" is referred to.


#### Kendall's tau

Although Pearson's correlation is ubiquitous, its forced adherance to measuring _linear_ dependence is a big downfall, especially because many relationships between real world variables are not linear.

An improvement is __Kendall's tau__ ($\tau_K$):

- Instead of measuring concordance between each observation $(x, y)$ and the mean $(\mu_x, \mu_y)$, it measures concordance between each _pair_ of observation $(x_i, y_i)$ and $(x_j, y_j)$.
- Instead of averaging the area of the boxes, it averages the amount of concordance and discordance by taking the difference between number of concordant and number of discordant pairs.

Visually plotting the $10 \choose 2$ boxes for the above sample from the previous section:

```{r, echo = FALSE}
ids <- t(combn(1:10, 2))
j_comb <- with(j_sample, 
	 tibble(
	 	los1  = los[ids[, 1]],
	 	gang1 = gang[ids[, 1]],
	 	los2  = los[ids[, 2]],
	 	gang2 = gang[ids[, 2]]
	 )
)
j_comb %>% 
	mutate(Sign = if_else((los1 > los2 & gang1 > gang2) | 
						  	(los1 < los2 & gang1 < gang2), 
						  "Positive", 
						  "Negative")) %>% 
	ggplot() +
	geom_rect(aes(xmin = los1, xmax = los2, 
				  ymin = gang1, ymax = gang2, 
				  fill = Sign),
			  alpha = 2/nrow(j_comb)) +
	geom_point(aes(x = los1, y = gang1)) +
	theme_bw() +
	labs(x = "Length of Stay (days)",
		 y = "Gang Request") +
	lims(x = c(1, 5),
		 y = c(1, 4))
```

The formal definition is
$$\frac{\text{Number of concordant pairs} - \text{Number of discordant pairs}}{{n \choose 2}},$$
with the "true" Kendall's tau value obtainined by sending $n \rightarrow \infty$. Note that several ways have been proposed for dealing with ties, but this doesn't matter when we're dealing with continuous variables (Weeks 3 and 4).

In R, the empirical version can be calculated using the `cor()` function with `method = "kendall"`:

```{r}
cor(j_sample$los, j_sample$gang, method = "kendall")
```

Like Pearson's correlation, Kendall's tau is also between -1 and 1, and also measures strength (and direction) of dependence.

For example, consider the two correlation measures for the following data set. Note that the empirical Pearson's correlation for the following data is not 1!

```{r, echo = FALSE}
nonlinear <- tribble(
	~x, ~y,
	0, 0,
	1, 4,
	2, 5.5,
	3, 6,
	4, 6.5
)
ggplot(nonlinear, aes(x, y)) +
	geom_point() +
	theme_bw() +
	theme(axis.title.y = element_text(vjust = 0.5, angle = 0))
```

```{r, echo = FALSE, results = "as.is"}
tribble(
	~ Pearson, ~ Kendall,
	round(cor(nonlinear)[1,2], 4), round(cor(nonlinear, method = "kendall")[1,2], 4)
) %>% 
	knitr::kable()
```


But, Kendall's tau still only measures the strength of _monotonic dependence_. This means that patterns like a parabola, which are not monotonically increasing or decreasing, will not be captured by Kendall's tau either:

```{r, echo = FALSE}
parabola <- tibble(x = -4:4, y = x^2)
ggplot(parabola, aes(x, y)) +
	geom_point() +
	theme_bw() +
	theme(axis.title.y = element_text(vjust = 0.5, angle = 0))
```

```{r, echo = FALSE, results = "as.is"}
tribble(
	~ Pearson, ~ Kendall,
	round(cor(parabola)[1,2], 4), round(cor(parabola, method = "kendall")[1,2], 4)
) %>% 
	knitr::kable()
```

Even though both dependence measures are 0, there's actually deterministic dependence here ($X$ _determines_ $Y$). But, luckily, there are many monotonic relationships in practice, making Kendall's tau a very useful measure of dependence. 

### Dependence as separate from the marginals 

The amount of monotonic dependence in a joint distribution, as measured by kendall's tau, has _nothing to do with the marginal distributions_. This can be a mind-boggling phenomenon, so don't fret if you need to think this over several times. 

To demonstrate, here are joint distributions between LOS and gang demand having the same marginals, but different amounts of dependence. 

```{r, fig.width = 8, echo = FALSE}
p_base <- ggplot(mapping = aes(los, gang, fill = p)) +
	theme_minimal() +
	labs(x = "Length of Stay (days)",
		 y = "Gang demand") +
	scale_fill_continuous(limits = c(0, 0.25)) 
cowplot::plot_grid(
	p_base + geom_tile(data = joint(-0.8)) + ggtitle("Strong negative"),
	p_base + geom_tile(data = joint(0))    + ggtitle("Independence"),
	p_base + geom_tile(data = joint(0.8))  + ggtitle("Strong positive"), 
	nrow = 1
)
```


### Dependence as giving us more information 

Let's return to the computation of the conditional distribution of gang requests given that a ship will only stay at port for one day. Let's compare the marginal distribution (the case where we know nothing) to the conditional distributions _for different levels of dependence_ (like we saw in the previous section). The means for each distribution are indicated as a vertical line: 

```{r, echo = FALSE, fig.width = 8, fig.height = 3}
multi <- purrr::map(c(-0.9, -0.3, 0, 0.3, 0.9), 
					~ joint(.x) %>% mutate(dep = .x)) %>% 
	bind_rows() %>% 
	group_by(dep, gang) %>% 
	summarize(p = p[los == 1]) %>% 
	mutate(p = p / sum(p)) %>% 
	ungroup() %>% 
	mutate(dep_str = str_c("Dependence: ", dep))

gang_marg <- j %>%
	group_by(gang) %>% 
	summarize(p = sum(p)) %>% 
	mutate(dep = 0,
		   dep_str = "Marginal") %>% 
	select(dep, gang, p, dep_str)

multi <- bind_rows(multi, gang_marg) %>% 
	mutate(dep_str = factor(dep_str) %>% 
		   	fct_relevel("Dependence: 0.9",
		   				"Dependence: 0.3",
		   				"Dependence: 0",
		   				"Dependence: -0.9",
		   				"Dependence: -0.3",
		   				"Marginal"
		   	))
multi %>% 
	group_by(dep_str) %>% 
	mutate(mean = sum(p * gang)) %>% 
	ggplot(aes(gang, p)) +
	facet_wrap(~ dep_str) +
	geom_col(aes(fill = dep_str == "Marginal")) +
	geom_vline(aes(xintercept = mean), colour = "black") +
	theme_bw() +
	labs(x = "Gang requests",
		 y = "Probability") +
	guides(fill = FALSE)
```

What's of particular importance is comparing the _uncertainty_ in these distributions. Let's look at how the uncertainty measurements compare between marginal and conditional distributions (marginal measurements indicated as horizontal line):

```{r, echo = FALSE, fig.height = 3}
multi %>% 
	group_by(dep) %>% 
	summarize(entropy = -sum(p * log(p)),
			  mean = sum(gang * p),
			  variance = sum((gang - mean)^2 * p)) %>%
	select(-mean) %>% 
	pivot_longer(cols = c("entropy", "variance"), names_to = "uncertainty", values_to = "value") %>% 
	group_by(uncertainty) %>% 
	mutate(marg_val = max(value)) %>% 
	ggplot(aes(dep, value)) +
	facet_wrap(~ uncertainty, scales = "free_y", ncol = 1) +
	theme_bw() +
	geom_line() +
	geom_point() +
	geom_hline(aes(yintercept = marg_val), colour = "maroon") +
	labs(x = "Dependence amount",
		 y = "Uncertainty measurement")
```

Moral of the story: more dependence (in either direction) gives us more certainty in the conditional distributions! This makes intuitive sense, because the more related $X$ and $Y$ are, the more that knowing what $X$ is will inform what $Y$ is.


## Harvesting Dependence (20 min)

The opposite of independence is _dependence_: when knowing something about $X$ tells us something about $Y$ (or vice versa). Extracting this "signal" that $X$ contains about $Y$ is at the heart of supervised learning (regression and classification), covered in DSCI 571/561 and beyond.

Usually, we reserve the letter $X$ to be the variable that we know something about (usually an exact value), and $Y$ to be the variable that we want to learn about. These variables [go by many names](https://ubc-mds.github.io/resources_pages/terminology/#equivalence-classes) -- usually, $Y$ is called the __response variable__, and $X$ is sometimes called a __feature__, or __explanatory variable__, or __predictor__, etc.

To extract the information that $X$ holds about $Y$, we simply use the _conditional distribution of $Y$ given what we know about $X$_. This is as opposed to just using the marginal distribution of $Y$, which corresponds to the case where we don't know anything about $X$.

Sometimes it's enough to just communicate the resulting conditional distribution of $Y$, but usually we reduce this down to some of the distributional properties that we saw earlier, like mean, median, or quantiles. We communicate uncertainty also using methods we saw earlier, like prediction intervals and standard deviation. 

Let's look at an example.

### Example: River Flow

In the Rocky Mountains, snowmelt $X$ is a major driver of river flow $Y$. Suppose the joint density can be depicted as follows:

```{r, fig.width = 4, fig.height = 3}
snowflow$ggframe +
	geom_contour(aes(z = z, colour = ..level..)) +
	scale_colour_continuous(guide = FALSE)
```

Every day, a measurement of snowmelt is obtained. To predict the river flow, usually the conditional mean of river flow given snowmelt is used as a prediction, but median is also used. Here are the two quantities as a function of snow melt:

```{r, fig.width = 5, fig.height = 3}
snowflow$ggframe +
	geom_contour(aes(z = z), colour = "black", alpha = 1/3) +
	stat_function(fun = function(x) snowflow$qcond(0.5, x), aes(colour = "median")) +
	stat_function(fun = snowflow$meancond, aes(colour = "mean")) +
	scale_color_discrete("") +
	ggtitle("Model Functions")
```

These functions are called __model functions__, and there are a ton of methods out there to help us directly estimate these model functions _without knowing the density_. This is the topic of supervised learning -- even advanced supervised learning methods like deep learning are just finding a model function like this (although, usually when there are more than one $X$ variable).

```{r}
p <- 0.8
```

It's also quite common to produce prediction intervals. Here is an example of an `r p*100`% prediction interval, using the `r (1-p)/2`- and `r 1-(1-p)/2`-quantiles as the lower and upper limits:

```{r, fig.width = 5, fig.height = 3}
snowflow$ggframe +
	geom_contour(aes(z = z), colour = "black", alpha = 1/3) +
	geom_ribbon(aes(ymin = snowflow$qcond(0.9, x),
					ymax = snowflow$qcond(0.1, x)),
				alpha = 0.2, fill = "blue") +
	stat_function(fun = function(x) snowflow$qcond(1-(1-p)/2, x), colour = "blue") +
	stat_function(fun = function(x) snowflow$qcond((1-p)/2, x),   colour = "blue") +
	ggtitle(str_c(p*100, "% Prediction Interval"))
```



```{r}
x0 <- 1
```

As a concrete example, consider the case where we know there's been `r x0`mm of snowmelt. To obtain the conditional distribution of flow ($Y$) given this information, we just "slice" the joint density at $x =$ `r x0`, and renormalize. Here is that density (which is now univariate!), compared with the marginal distribution of $Y$ (representing the case where we know nothing about snowmelt, $X$):

```{r, fig.width = 5, fig.height = 2}
tibble(y = c((1 - p)/2, 1 - (1 - p) / 2) %>% flow$qdist()) %>% 
	ggplot(aes(y)) +
	stat_function(fun = flow$ddist, aes(colour = "Marginal")) +
	stat_function(fun = function(y) snowflow$dcond(y, x0), 
				  aes(colour = str_c("Conditional (X=", x0, ")"))) +
	theme_bw() +
	scale_colour_discrete("") +
	labs(x = expression(paste("River Flow ", (m^3/s))),
		 y = "Density")
```

The following table presents some properties of these distributions:

```{r}
tribble(
	~Quantity, ~Marginal, ~Conditional,
	"Mean", round(flow$mean, 2), round(snowflow$meancond(x0), 2),
	"Median", round(flow$qdist(0.5), 2), round(snowflow$qcond(0.5, x0), 2),
	str_c(p*100, "% PI"), 
	str_c("[", round(flow$qdist((1-p)/2), 2), ", ", 
		  round(flow$qdist(1 - (1-p)/2), 2), "]"),
	str_c("[", round(snowflow$qcond((1-p)/2, x0), 2), ", ", 
		  round(snowflow$qcond(1 - (1-p)/2, x0), 2), "]")
) %>% 
	knitr::kable()
```


Notice that we actually only need the conditional distribution of $Y$ given $X=x$ for each value of $x$ to produce these plots! In practice, we usually just specify these conditional distributions. So, having the joint density is actually "overkill".


### Direction of Dependence

Two variables can be dependent in a multitude of ways, but usually there's an overall direction of dependence:

- __Positively related__ random variables tend to increase together. That is, larger values of $X$ are associated with larger values of $Y$.
- __Negatively related__ random variables have an inverse relationship. That is, larger values of $X$ are associated with smaller values of $Y$.

We've already seen some measures of dependence in the discrete setting: covariance, correlation, and Kendall's tau. These definitions carry over. It's a little easier to visualize the definition of covariance as the signed sum of rectangular area:

```{r, fig.width = 5, fig.height = 3}
set.seed(14)
snowflow$rdist(30) %>% 
	mutate(Sign = if_else((x > snow$mean & y > flow$mean) | 
						  	(x < snow$mean & y < flow$mean), 
						  "Positive", 
						  "Negative")) %>% 
	ggplot(aes(x, y)) +
	geom_rect(aes(xmax = x, ymax = y, fill = Sign), 
			  xmin = snow$mean, ymin = flow$mean,
			  alpha = 0.1, colour = "black") +
	geom_point(aes(x = x, y = y)) +
	geom_point(x = snow$mean, y = flow$mean, colour = "red") +
	theme_bw() +
	labs(x = "Snowmelt (mm)",
		 y = expression(paste("River Flow ", (m^3/s))))
```

Correlation, remember, is also the signed sum of rectangles, but after converting $X$ and $Y$ to have variances of 1. 

Here are two positively correlated variables, because there is overall tendency of the contour lines to point up and to the right (or down and to the left):

```{r, fig.width = 4, fig.height = 3}
dfrklnln <- function(x, y, cpar) {
	dfrk(plnorm(x), pnorm(y), cpar) * dlnorm(x) * dnorm(y)
}
qcondfrklnln <- function(p, x, cpar) {
	u <- plnorm(x)
	v <- qcondfrk(p, u, cpar)
	qlnorm(v)
}
cpar <- 3
crossing(x = seq(0, 3, length.out = 100),
		 y = seq(-3, 3, length.out = 100)) %>% 
	mutate(z = dfrklnln(x, y, cpar)) %>% 
	ggjointmarg(dlnorm, dnorm)
```


Here are two negatively correlated variables, because there is overall tendency for the contour lines to point down and to the right (or up and to the left):


```{r, fig.width = 4, fig.height = 3}
dgumnn <- function(x, y, cpar) dgum(pnorm(x), pnorm(y), cpar) * dnorm(x) * dnorm(y)
crossing(x = seq(-3, 3, length.out = 100),
		 y = seq(-3, 3, length.out = 100)) %>% 
	mutate(z = dgumnn(x, y, 2),
		   x = -x) %>% 
	ggjointmarg(dnorm, dnorm)
```


Another example of negative correlation. Although contour lines aren't pointing in any one direction, there's more density along a line that points down and to the right (or up and to the left) than there is any other direction.

```{r, fig.width = 4, fig.height = 3}
dmixmarg <- function(x) dnorm(x - 2) / 2 + dnorm(x + 2) / 2
dmixnn <- function(x, y) map2_dbl(x, y, ~ dbvn(c(.x - 2, .y + 2), 0) / 2 + dbvn(c(.x + 2, .y - 2), 0) / 2)
crossing(x = seq(-5, 5, length.out = 100),
		 y = seq(-5, 5, length.out = 100)) %>% 
	mutate(z = dmixnn(x, y)) %>% 
	ggjointmarg(dmixmarg, dmixmarg)
```

Here are two random variables that are dependent, yet have 0 correlation (both Pearson's and Kendall's) because the overall trend is flat (pointing left or right). You can think of this in terms of slicing: slicing at $x = -2$ would result in a highly peaked distribution near $y = 0$, whereas slicing at $x = 1$ would result in a distribution with a much wider spread -- these are not densities that are multiples of each other! Prediction intervals would get wider with larger $x$.

```{r, fig.width = 4, fig.height = 3}
dhetnn <- function(x, y) dnorm(y, sd = exp(x)) * dnorm(x)
dy <- function(y) (1 - exp(-y^2/2)) / sqrt(2*pi) / y^2 
dy <- Vectorize(function(y) integrate(function(x) dhetnn(x, y), lower = -100, upper = 100)$value)
crossing(x = seq(-2, 2, length.out = 100),
		 y = seq(-3, 3, length.out = 100)) %>% 
	mutate(z = dhetnn(x, y)) %>% 
	ggjointmarg(dnorm, dy)
```

Note that the marginal distributions have _nothing to do with the dependence_ between random variables. Here are some examples of joint distributions that all have the same marginals ($N(0,1)$), but different dependence structures and strengths of dependence:

```{r, fig.width = 6, fig.height = 4}
crossing(x = seq(-3, 3, length.out = 100),
		 y = seq(-3, 3, length.out = 100)) %>% 
	mutate(
		u = pnorm(x),
		v = pnorm(y),
		d = dnorm(x) * dnorm(y),
		Structure1_Negative  = d * dfrk(u, v, -5),
		Structure1_Low       = d * dfrk(u, v, 5),
		Structure1_High      = d * dfrk(u, v, 10),
		Structure2_Negative  = d * dgum(1 - u, v, 1.5),
		Structure2_Low       = d * dgum(u, v, 1.5),
		Structure2_High      = d * dgum(u, v, 3),
		Structure3_Negative  = d * dbvtcop(1-u, v, c(0.5, 1.5)),
		Structure3_Low       = d * dbvtcop(u, v, c(0.5, 1.5)),
		Structure3_High      = d * dbvtcop(u, v, c(0.9, 1.5))
	) %>% 
	pivot_longer(cols = contains("structure"),
				 names_to = c("structure", "dependence"),
				 names_sep = "_",
				 values_to = "density") %>% 
	mutate(dependence = dependence %>% 
		   	fct_relevel("Negative", "Low", "High")) %>% 
	ggplot(aes(x, y)) +
	facet_grid(structure ~ dependence) +
	geom_contour(aes(z = density, colour = ..level..)) +
	theme_bw() +
	theme(axis.title.y = element_text(angle = 0, vjust = 0.5)) +
	scale_colour_continuous("", guide = FALSE)
```

## Marginal Distributions (20 min)

In the river flow example, we used snowmelt to inform river flow by communicating the conditional distribution of river flow given snowmelt. But, this requires knowledge of snowmelt! What if one day we are missing an observation on snowmelt? Then, the best we can do is communicate the marginal distribution of river flow. But how can we get at that distribution? Usually, aside from the data, we only have information about the conditional distributions. But this is enough to calculate the marginal distribution!

### Marginal Distribution from Conditional 

We can use the law of total probability to calculate a marginal density. Recall that for discrete random variables $X$ and $Y$, we have
$$P(Y = y) = \sum_x P(X = x, Y = y) = \sum_x P(Y = y \mid X = x) P(X = x).$$
The same thing applies in the continuous case, except probabilities become densities and sums become integrals (as usual in the continuous world): for continuous $X$ and $Y$,
$$f_Y(y) = \int_x f_{X,Y}(x,y)\ \text{d}x = \int_x f_{Y\mid X}(y \mid x)\ f_X(x)\ \text{d}x.$$

Notice that this is just an average of the conditional densities! If we have the conditional densities and a sample of $X$ values $x_1, \ldots, x_n$, then using the empirical approximation of the mean, we have
$$f_Y(y) \approx \frac{1}{n} \sum_{i = 1}^n f_{Y\mid X}(y \mid x_i).$$

A similar result holds for the cdf. We have
$$F_Y(y) = \int_x F_{Y \mid X}(y \mid x)\ f_X(x) \ \text{d}x,$$
and empirically,
$$F_Y(y) \approx \frac{1}{n}\sum_{i = 1}^n F_{Y\mid X}(y \mid x_i).$$

### Marginal Mean from Conditional

Perhaps more practical is finding the marginal mean, which we can obtain using the law of total expectation (similar to the discrete case we saw in a previous lecture):
$$E(Y) = \int_x m(x) \ f_{X}(x) \ \text{d}x = E(m(X)),$$
where $m(x) = E(Y \mid X = x)$ (i.e., the model function or regression curve).

When you fit a model using supervised learning, you usually end up with an estimate of $m(x)$. From the above, we can calculate the marginal mean as the mean of $m(X)$, which we can do empirically using a sample of $X$ values $x_1, \ldots, x_n$. Using the empirical mean, we have
$$E(Y) \approx \frac{1}{n} \sum_{i=1}^n m(x_i).$$

### Marginal Quantiles from Conditional

Unfortunately, if you have the $p$-quantile of $Y$ given $X = x$, then there's no convenient way of calculating the $p$-quantile of $Y$ as an average. To obtain this marginal quantity, you would need to calculate $F_Y(y)$ (as above), and then find the value of $y$ such that $F_Y(y) = p$. 

### Activity

You've observed the following data of snowmelt and river flow:

```{r}
tribble(
	~`Snowmelt (mm)`, ~`Flow (m^3/s)`,
	1, 140,
	3, 150,
	3, 155,
	2, 159,
	3, 170
) %>% 
	knitr::kable()
```

From this, you've deciphered that the mean flow given snowmelt is
$$E(\text{Flow} \mid \text{Snowmelt} = x) = 100 + 20x.$$

You also decipher that the conditional standard deviation is constant, and is:
$$SD(\text{Flow} \mid \text{Snowmelt} = x) = 15\ m^3/s$$
It also looks like the conditional distribution of river flow given snowmelt follows a Lognormal distribution. 

Part 1: A new reading of snowmelt came in, and it's 4mm. 

1. Make a prediction of river flow.
2. What distribution describes your current understanding of what the river flow will be?

Part 2: Your snowmelt-recording device is broken, so you don't know how much snowmelt there's been.

3. Make a prediction of river flow. 
4. What distribution describes your current understanding of what the river flow will be?
5. Someone tells you that a 90% prediction interval is [70, 170]. What do we know about the median?
