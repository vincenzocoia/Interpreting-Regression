# Regression under many groups: mixed effects models

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

```{r}
suppressPackageStartupMessages(library(tidyverse))
```


## Motivation for LME

Let's take a look at the `esoph` data set, to see how the number of controls `ncontrols` affects the number of cases `ncases` of cancer for each age group `agegp`. Here's what the data look like (with a tad bit of vertical jitter):

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
(p <- ggplot(esoph, aes(ncontrols, ncases, group=agegp, colour=agegp)) +
    geom_jitter(height=0.25) +
    scale_colour_discrete("Age Group") +
    ylab("Number of Cases") + xlab("Number of Controls"))
```

It seems each age group has a different relationship. Should we then fit regression lines for each group separately? Here's what we get, if we do:

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
p + geom_smooth(method="lm", se=FALSE, size=0.5)
```

But, each group has so few observations, making the regression less powerful:

```{r, echo=FALSE}
esoph %>% 
    group_by(agegp) %>% 
    summarise(n=length(ncases))
```

__Question__: can we borrow information across groups to strengthen regression, while still allowing each group to have its own regression line?

Here's another scenario: suppose we want to know the effect of `ncontrols` on the average person. Then, we would only include one common slope parameter for all individuals. Even if each individual "has their own unique slope", this model is still sensible because the common slope can be interpreted as the _average effect_. The problem with this model is that the typical estimates of standard error on our regression coefficients will be artificially small due to correlation in the data induced by the grouping.

Here is a simulation that compares the "actual" SE (or at least an approximation of it) and the SE reported by `lm`:

```{r, cache=TRUE}
# library(tidyverse)
# library(broom)
# set.seed(1000)
# ## Number of groups
# g <- 10
# ## Number of observations per group
# ng <- 10
# ## Initiate slope and SE estimates
# beta1hat <- numeric(0)
# se <- numeric(0)
# for (i in 1:1000) {
#   ## Generate intercept and slope from a joint Normal distribution
#   beta0 <- rnorm(g)
#   beta1 <- 1 + beta0 + rnorm(g)
#   ## Generate iid data from within each group
#   esoph <- tibble(group=LETTERS[1:g], beta0, beta1) %>%
#     mutate(x = map(beta0, ~ rnorm(ng))) %>%
#     unnest() %>%
#     group_by(group) %>%
#     mutate(eps = rnorm(length(x)),
#            y = beta0 + beta1 * x + eps)
#   ## Fit a linear regression, forcing a common slope
#   fit <- lm(y ~ x + group, data=esoph) %>%
#     tidy()
#   beta1hat[i] <- fit$estimate[2]
#   se[i] <- fit$std.error[2]
# }
# ## Actual SE:
# sd(beta1hat)
# ## SE given from the lm fit:
# mean(se)
# 
# ## Here's a plot of the last sample generated:
# ggplot(esoph, aes(x, y)) +
#   geom_point(aes(colour=group), alpha=0.5) +
#   theme_bw()
```

__Question__: How can we account for the dependence in the data?

Both questions can be addressed using a _Linear Mixed Effects_ (LME) model. An LME model is just a linear regression model for each group, with different slopes and intercepts, but the collection of slopes and intercepts _is assumed to come from some normal distribution_.

### Definition

With one predictor ($X$), we can write an LME as follows:
$$ Y = \left(\beta_0 + b_0\right) + \left(\beta_1 + b_1\right) X + \varepsilon,  $$
where the error term $\varepsilon$ has mean zero, and the $b_0$ and $b_1$ terms are normally distributed having a mean of zero, and some unknown variances and correlation. The $\beta$ terms are called the _fixed effects_, and the $b$ terms are called the _random effects_. Since the model has both types of effects, it's said to be a _mixed_ model -- hence the name of "LME". 

Note that we don't have to make _both_ the slope and intercept random. For example, we can remove the $b_0$ term, which would mean that each group is forced to have the same (fixed) intercept $\beta_0$. Also, we can add more predictors ($X$ variables).

### R Tools for Fitting

Two R packages exist for working with mixed effects models: `lme4` and `nlme`. We'll be using the `lme4` package (check out [this](http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models) discussion on Cross Valiesophed for a comparison of the two packages).

Let's fit the model. We need to indicate a formula first in the `lmer` function, and indicate the data set we're using.

```{r}
fit <- lme4::lmer(ncases ~ ncontrols + (ncontrols | agegp), 
            data=esoph)
```

Let's take a closer look at the _formula_, which in this case is `ncases ~ ncontrols + (ncontrols | agegp)`. 

On the left of the `~` is the response variable, as usual (just like for `lm`). On the right, we need to specify both the fixed and random effects. The fixed effects part is the same as usual: `ncontrols` indicates the explanatory variables that get a fixed effect. Then, we need to indicate which explanatory variables get a random effect. The random effects can be indicated in parentheses, separated by `+`, followed by a `|`, after which the variable(s) that you wish to group by are indicated. So `|` can be interpreted as "grouped by".

Now let's look at the model output:

```{r}
summary(fit)
```

The random and fixed effects are indicated here.

- Under the "Random effects:" section, we have the variance of each random effect, and the lower part of the correlation matrix of these random effects.
- Under the "Fixed effects:" section, we have the estimates of the fixed effects, as well as the uncertainty in the estimate (indicated by the Std. Error).

We can extract the collection of slopes and intercepts for each group using the `coef` function:

```{r}
(par_coll <- coef(fit)[[1]])
```

Let's put these regression lines on the plot:

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
## Put the slopes and intercepts with the data frame:
par_coll %>% 
    rownames_to_column("agegp") %>% 
    left_join(esoph, by="agegp")
par_coll <- rownames_to_column(par_coll)
esoph <- plyr::ddply(esoph, ~ agegp, function(df){
    pars <- subset(par_coll, rowname==unique(df$agegp))
    int <- pars$`(Intercept)`
    slp <- pars$ncontrols
    cbind(df, intercept=int, slope=slp)
})

## Plot
ggplot(esoph, aes(ncontrols, ncases, group=agegp, colour=agegp)) +
    geom_jitter(height=0.25) +
    geom_abline(aes(intercept=intercept, slope=slope, colour=agegp)) +
    scale_colour_discrete("Age Group") +
    ylab("Number of Cases") + xlab("Number of Controls")
```

So, each group still gets its own regression line, but tying the parameters together with a normal distribution gives us a more powerful regression.

## Mixed Effects Models in R: tutorial

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

Two R packages exist for working with mixed effects models: `lme4` and `nlme`. We'll be using the `lme4` package (check out [this](http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models) discussion on Cross Valiesophed for a comparison of the two packages).

In Lab 1, we compared linear regression (function `lm`) with GLM's (function `glm`). In Lab 2, we consider adding a random effect to either of these:

- A linear model with random effects is a _Linear Mixed-Effects Model_, and is fit using the `lmer` function.
- A generalized linear model with random effects is a _Generalized Linear Mixed-Effects Model_, and is fit using the `glmer` function.

We'll work with the `esoph` data set, to see how the number of controls `ncontrols` affects the number of cases `ncases` based on age group `agegp`. Here's what the data look like (with a tad bit of vertical jitter):

```{r}
p <- ggplot(esoph, aes(ncontrols, ncases)) +
    geom_jitter(aes(colour=agegp), height=0.25)
p
```

Since the response is a count variable, we'll go ahead with a Poisson regression -- a Generalized Linear Mixed-Effects Model. The model is
$$ Y_{ij} \mid X_{ij} = x_{ij} \sim \text{Poisson}\left(\lambda_{ij}\right) $$
for each observation $i$ on the $j$'th age group, where $Y_{ij}$ is the number of cases, $X_{ij}$ is the number of controls, and $\lambda_{ij}$ is the conditional mean of $Y_{ij}.$ We model $\lambda_{ij}$ as
$$ \log\left(\lambda_{ij}\right) = \left(\beta_0 + b_{0j}\right) + \left(\beta_1 + b_{1j}\right) x_{ij}, $$
where $b_{0j}$ and $b_{1j}$ are joint (bivariate) normally distributed with zero mean. 

What does this model mean? First, it means that the mean is exponential in the explanatory variable, since we chose a $\log$ link function. Second, each age group ($j$) gets its own mean curve, via its own linear predictor. But we're saying that these linear predictors are related: the collection of slopes and intercepts across age groups are centered around $\beta_0$ and $\beta_1$ (respectively, called the _fixed effects_), and the slope and intercept of each age group departs from this center according to some Gaussian random noise (the $b$ terms, called the _random effects_).

Let's fit the model. Then we'll go through the formula, and the output.

```{r}
fit <- lme4::glmer(ncases ~ ncontrols + (1 + ncontrols | agegp), 
                   data=esoph, 
                   family=poisson)
summary(fit)
```

To specify the formula, the fixed effects part is the same as usual: `ncases ~ ncontrols` gives you `ncases = beta0 + beta1 * ncontrols`. Note that the intercept is put in there by default. Then, we need to indicate which explanatory variables are getting the random effects -- including the intercept this time (with a 1), if you want it (in this case, we do). The random effects can be indicated in parentheses, separated by `+`, followed by a `|`, after which the variable(s) that you wish to group by are indicated. So `|` can be interpreted as "grouped by".

The output of the model fit is similar to what you've seen before (in `glm` for example), but the "random effects" part is new. That gives us the estimates of the joint normal distribution of the random effects -- through the variances, and correlation matrix to the right (only the lower-diagonal of the correlation matrix is given, because that matrix is symmetric anyway).

Let's see what the intercepts and slopes for each age group are, and let's plot the estimated mean curves:

```{r}
(coef_fit <- coef(fit)$agegp)
## Colours with stat_function are not nice to deal with. Do manually.
p + stat_function(aes(colour="25-34"), fun = function(x) exp(coef_fit[1,1] + coef_fit[1,2]*x)) +
    stat_function(aes(colour="35-44"), fun = function(x) exp(coef_fit[2,1] + coef_fit[2,2]*x)) +
    stat_function(aes(colour="45-54"), fun = function(x) exp(coef_fit[3,1] + coef_fit[3,2]*x)) +
    stat_function(aes(colour="55-64"), fun = function(x) exp(coef_fit[4,1] + coef_fit[4,2]*x)) +
    stat_function(aes(colour="65-74"), fun = function(x) exp(coef_fit[5,1] + coef_fit[5,2]*x)) +
    stat_function(aes(colour="75+"),   fun = function(x) exp(coef_fit[6,1] + coef_fit[6,2]*x))
```

A (response-) residual plot is somewhat sensible to look at here:

```{r}
plot(fit)
```

Looks fairly centered at zero, so the shape of the mean curves are satisfactory.


## Concepts

- A linear mixed effects (LME) model is used when iid data are collected within groups. The model is a linear regression model, where some of the regression coefficients are taken to be group-specific, where each group's coefficients are assumed to come from a joint Normal distribution with some mean and a generic covariance matrix.
- The _fixed effects_ in an LME model are the regression coefficients (if fixed) and the expected value of the regression coefficients (if random).
- The _random effects_ in an LME model are the random regression coefficients minus their means / fixed effects.
- To predict on an existing group (using a mean prediction), we find that group's regression coefficients (and therefore model function) by summing the fixed effects and (if present) the random effects, then use that model function to make predictions.
- To predict on a new group (using a mean prediction), we use the fixed effects as the regression coefficients (because the random effects have mean zero), and use that model function to make predictions. 
- If each group has different (true) regression coefficients, then: 
	- fitting a (fixed effects) linear regression model with common regression coefficients across groups does not account for the dependence/correlation contained in the data (induced by the grouping), therefore the reported SE of the regression coefficients will be smaller than they should be.
	- fitting a (fixed effects) linear regression model with separate regression coefficients across groups results in SE's (of regression coefficients) that are larger than they _could_ be if we were to borrow information across groups (as mixed effects models do). This is because this is akin to fitting separate regressions on each group, which alone has far less data than the pooled dataset.
- While the random effects are _assumed_ to follow a joint Normal distribution, this is different from the sampling distribution of the estimates of the fixed effects. 
	- The former distribution explains the spread of regression coefficients, and does not change when we collect more data (we just get a better estimate of this distribution); 
	- The latter distribution explains the uncertainty in the estimates, and gets narrower as we collect more data.  

