---
title: "Splines and Loess Regression"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=5, fig.height=3, fig.align="center")
```

This tutorial describes spline and loess regression in R.

## Splines

You can think of splines as regression between knots. We'll use the `splines` package to do this.

Here's some generated data:

```{r}
library(ggplot2)
x <- rnorm(1000)
y <- x^2 + rnorm(1000)
qplot(x, y, alpha=I(0.5)) + theme_bw()
```

First, we need to "set up" the regression by placing knots.

```{r}
library(splines)
x2 <- ns(x, knots=c(-2, 0, 2))
```

Now we can do regression between these knots, with the natural spline shape (as opposed to linear):

```{r}
fit <- lm(y ~ x2)
qplot(x, y, alpha=I(0.5)) +
    geom_line(data=data.frame(x=x, y=predict(fit)), 
              mapping=aes(x, y), colour="blue") + 
    theme_bw()
```

## Loess

Loess is "local regression", and is based on the idea of estimating the mean response based on similar observed data in the predictor space.

__Kernel smoothing__ is a method that estimates the mean using a weighted sample average, where observations that are further away in the predictor space get downweighted more, according to some kernel function.

__Local polynomials__ is a method that takes kernel smoothing one step further: instead of a weighted sample average, we use nearby data (in the predictor space) to fit a polynomial regression, and then fit a polynomial regression model.

There is a more basic version, too: a __"moving window"__, described next, before seeing how loess is done in R.

### The "Moving Window"

The "moving window" approach is a special type of kernel smoother. For a given value of the predictor $X=x$, the mean response is estimated as the sample average of all response values whose predictor values are "near" $x$ -- within some distance $h$.

For example, if you want to estimate the mean number of "runs" of a baseball team when walks=100 and hits=1000, only look at cases where "walks" is approximately 100, "hits" is approximately 1000, and then average the response.

It's a special case of kernel regression, with a kernel function of 
$$ k\left(t\right) = I\left(|t-x| < h \right), $$ 
sometimes called the "boxcar" function. 

__Note the similarity to kNN!__ kNN regression uses the nearest $k$ points, resulting in a variable distance $h$, whereas the moving window regression uses a fixed distance $h$, resulting in a variable number of points $k$ used.

### `ggplot2`

`ggplot2` comes with a fairly powerful tool for plotting a smoother, with `geom_smooth`.

```{r}
qplot(x, y) +
    geom_smooth(method="loess") + 
    theme_bw()
```

You can choose the bandwidth through the `span` argument:

```{r}
qplot(x, y) +
    geom_smooth(method="loess", span=0.1) +
    theme_bw()
```

Too wiggly. The default looks fine.

Note that `geom_smooth` can fit `lm` and `glm` fits too:

```{r}
qplot(x, y) +
    geom_smooth(method="lm", formula=y~x+I(x^2)) + 
    theme_bw()
```

### Manual method

You can use the `loess` or `ksmooth` function in R to do kernel smoothing yourself. You get more flexibility here, since you can get things such as predictions. It works just like the other regression functions:

```{r}
(fit <- loess(y ~ x))
```

We can make predictions as usual:

```{r}
yhat <- predict(fit)
qplot(x, y) +
    geom_line(data=data.frame(x=x, y=yhat), mapping=aes(x, y), 
              colour="blue") + 
    theme_bw()
```

If you want the standard errors of the predictions, you can indicate this with `se=TRUE` in the `predict` function. 

Some key things that you might want to change in your kernel smoothing regression, through arguments in the `loess` function:

- Bandwidth/smoothing parameter. Indicate through the `span` argument. 
- Degree of the local polynomial fitted. Indicate through the `degree` argument.
- Kernel function.

The kernel function is not readily specified in `loess`. But you can use the `ksmooth`, where you're allowed to specify a "box" or "normal" kernel.