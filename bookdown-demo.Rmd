--- 
title: "Interpreting Regression"
author: "Vincenzo Coia"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
biblio-style: apalike
link-citations: yes
github-repo: vincenzocoia/Interpreting-Regression
description: "My tutorials for regression analysis, in the form of a bookdown book."
---

# Preamble

## Caution {#caution}

**This book is in its very preliminary stages**. Content will be moving around and updated.

Currently, much of the book is a stitching together of previous pieces of my writing that I think might be relevant to this book. These chapters will be updated to consider the different context, audience, and content organization that's best for this book.

Stage of writing:

1. ~~Gather existing and potentially relevant pieces of writing~~
2. ~~Create a new chapter structure~~
3. Parse existing writing into new chapters
4. Write draft preambles for each chapter and part

## Purpose of the book

There's a vast and powerful statistical framework out there. This book takes a modularized approach to making this framework accessible, so that as a problem solver, the reader can make a sequences of decisions to build models that are best suited to address the problem. 

Regression analysis can help solve two main types of problems:

1. Interpreting the relationship between variables.
2. Predicting a new outcome.

This book primarily focusses on models for interpretation, and often references these two competing needs. The prediction problem is still discussed to some extent: once a model suited for interpretation is developed, it is still important to be able to use it to make predictions. For those interested primarily in prediction, check out resources on _supervised learning_, which aims to optimize predictions. 

There is another layer to interpretation that this book adopts, in terms of describing and understanding the motivation and inner workings behind each method. This is in contrast to a purely mathematical presentation of statistical methods. This book presents both an interpretation of a the high-level idea behind amethod, as well as a mathematical presentation to make these concepts precise. For example, the Kaplan-Meier estimate of the survival function is explained both intuitively and mathematically. 

Most statistical methods are built on a foundation of assumptions imposed on the data. But an assumption is almost never _exactly_ true; so we instead explore consequences based on "how close" an assumption is to being true. In some cases, we even find that an anticipated assumption is not required at all, depending on how we would like to interpret the model -- an example being the "requirement" of linear data in linear regression. Consequenty, one aim of this book is to discourage the thinking that a method either "can" or "cannot" be applied, instead thinking about pros and cons of various methods.

Methods are demonstrated using the R statistical software. This is because R has an extensive selection of packages available for statistical analysis, and the `tidyverse` and `tidymodels` meta-packages make data analysis readable and organized. Since this book does not focus on optimally flexing a model function to conform to data (non-parametric supervised learning), languages better suited for this task, such as python, are not considered.

The audience of this book is quite wide, with the aspects of __modularization__, __interpretation__, and __non-binary view of assumptions__ perhaps appealing to various readers:

1. Practitioners may find the modularization useful when making decisions when fitting models, the non-binary view of assumptions useful when evaluating the trustworthiness of their models, and the interpretation of their model useful when communicating their results.
2. Experts in the field of Statistics might benefit from the unique modularized framework of the field, as they find well-used notions such as the expected value being challenged and expanded upon.
3. Learners may find the modularization useful for compartmentalizing concepts, and the interpretation of methods useful for understanding concepts.

## Tasks that motivate Regression

Real world problems for which regression is an appropriate tool generally fall into two categories:

1. Prediction: Predicting the response of a new unit/individual, sometimes also describing uncertainty in the prediction.
2. Interpretation: Interpreting how predictors influence the response.

For example, consider a person undergoing artificial insemination.

- Prediction: Given the person's age, what is the chance of pregnancy?
- Interpretation: How does age influence the chance of pregnancy? How does time of insemination after a spike in Luteinizing hormone affect the chance of pregnancy, and how is this different for people over 40?

This book does not focus on optimizing predictions, but focusses on the other tasks. This means:

1. describing the uncertainty in predictions, or estimates in general, and
2. interpreting how predictors influence the response.

Why not focus on optimizing predictions? This is the objective of _supervised learning_, an entire discipline in itself. The scope of this book would just be too big to include this, too.

## Examples

```{r}
library(tidyverse)
library(ISLR)
suppressPackageStartupMessages(library(Lahman))
baseball <- Teams %>% tbl_df %>% 
  select(runs=R, hits=H)
cow <- suppressMessages(read_csv("data/milk_fat.csv"))
```




<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.

<!--chapter:end:index.Rmd-->

# Regression in the context of problem solving

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

(BAIT 509 Class Meeting 06)

Today, we're going to take a break from supervised learning methods, and look at the process involved to use machine learning to address a question/problem faced by an organization.

Generally, there are four parts of a machine learning analysis. In order from high to low level:

1. The Business Question/Objective
2. The Statistical Question(s)/Objective(s)
3. The data and model
4. The data product

Doing an analysis is about distilling from the highest level to the lowest level. As such, there are three distillations to keep in mind: 1-2, 2-3, and 3-4:

- 1-2 is about asking the right questions.
    - Our focus today.
- 2-3 is about building a useful model.
    - Our focus for much of the course.
- 3-4 is about communicating the results.

An analysis isn't (_and shouldn't be_) a linear progression through these as if they're "steps"; rather, the process is iterative, revisiting any of the distillations at any point after embarking on the project. This is because _none of the components are independent_. Making progress on any of the three distillations gives you more information as to what the problem is. 


In all of this, it's most beneficial to consider developing your analysis through small, simple steps. Always start with a basic approach, and gradually tweak the various components. Right off the bat, strive to get some sort of crude end product that resembles what your final product might look like. Not only is it more useful to have a "working version" of your product right away, but it gives you more information about the project, and will inform other distillations. 

We'll look at these in turn, in reverse order.

## Communicating (Distillation 3-4)

Once you have a model, it needs to be delivered and used by others. (And if you are truly the only one who will use the model, then you still need to package it up as if it will be used by others, because it will -- the "other" person here is future-you!)

This is typically called the "data product", because it can consist of a variety of things:

- a report
- a presentation
- an app
- a software package/pipeline

...and any combination of these things. These almost always (or at least, almost always _should_) contain data visualizations. 

Your client might request this outright, but their suggestion might not be the best option. Perhaps they request a report and presentation communicating your findings, when a more appropriate product also includes an interactive app that allows them to explore your findings for themselves.

Either way, the key here is communication. The challenges here include: 

- Communication with language: understanding your audience, and tailoring your language for them. This usually means no jargon allowed. The key is to talk more about the output and the general idea of your model(s), but not machine learning or statistical jargon.
- Communication with visual design: this is about choosing what visuals are the most effective for communicating (called design choice). 

It's key when you're developing a first draft to not fret about details. For example, there's no use interpreting the results of an analysis if you know the analysis will change. The idea is to set up a framework. For a report, this means outlining what you intend to write about, and where. 

Having this, and showing it to your client is useful as a sanity check to see that you're about to produce something that the client currently sees as being potentially useful. 

This course is not about developing a data product. This is a topic that can be a discipline on its own, so we will not dwell any more on this.

## Modelling (Distillation 2-3)

This involves things like kNN, loess, classification/regression trees, etc. -- things that the bulk of this course is focussed on. As such, I won't dwell here.

One thing I will say, though, is that there is temptation here to jump immediately into a complex analysis. Resist the temptation. Always start with a very basic approach -- perhaps just linear regression, for example. The amount of complex approaches far exceeds the amount of simple ones, and starting with a simple one will help you end up in an appropriate complex analysis, if one is required.


## Asking useful statistical questions (Distillation 1-2)

Usually, a company is not served up a machine learning problem, complete with data and a description of the response and predictors. Instead, they're faced with some high-level objective/question that we'll call the __business question/objective__, which needs refining to a __statistical question/objective__ -- one that is directly addressable by machine learning.

### Business objectives: examples

Examples of business objectives (for which machine learning is a relevant approach) (more examples at [this altexsoft blog post](https://www.altexsoft.com/blog/business/supervised-learning-use-cases-low-hanging-fruit-in-data-science-for-businesses/)):

- Reduce the amount of spam email received
- Early prediction of product failure
- Find undervalued mines
- Make a transit system more efficient
- Hire efficient staff 

### Statistical objectives: refining

Statistical objectives should be _specific_. For some business objectives, the distillation is not dramatic; for others, it is. Because supervised learning is about predicting a response $Y$ from predictors $X_1, \ldots, X_p$, identifying a statistical objective involves:

- Most importantly, identifying what response is the most valuable/aligned with the business objective.
- Identifying the cases that will be used for learning, and what cases you'll be predicting on.
- What predictors you plan on using, or at least where we'll be looking for these. 
    - Note: half of this is the task of _feature selection_ -- a topic we may cover last in the course -- but, largely, this is a human decision based on what we think is more informative, so is important to include in your supervised learning question/objective.

### Statistical objectives: examples

Statistical objectives corresponding to the above business objective examples might be:

- Predict monthly sales (response) from a personality test (predictors), issued to our current employees (learning cases) and prospective employees (prediction cases).
- Obtain individual-specific classifications of email spam (response) based on features present in the name and body of an email (predictors). Cases of spam will be gathered over time, as the employee manually identifies mistakes in classification (prediction cases become learning cases after emails are viewed).
- Classify a device as either faulty or not (response) based on predictors chosen by expert advice (predictors). Of those predicted faulty, check their performance for one hour (predictors, round 2) to predict lifetime of the device (response, round 2). Sample cases from our test facility will be used (learning cases) to make predictions on items sent to the shelf. 
- Predict total volume of gold and silver at a site (response) based on concentrations of various minerals found in drill samples (predictors). Use cases where total volume is known (learning cases) to make predictions on mines where total volume is not known or uncertain (prediction cases).
- Predict the time it takes a bus to travel between set stops (responses), based on time factors such as time of the day, time of the week, and time of the year (predictors). Use data from the server, available for the past 8 years (learning cases) to make predictions for 2019 (prediction cases). 

### Statistical questions are not the full picture

Almost (?) always, the business objective is more complex than the statistical objective. By refining a business objective to a statistical one, we lose part of the essence of the business objective. 

It's important to have a sense of the ways in which your statistical objective falls short, and the ways in which it's on the mark, so that you keep a sense of the big picture. For instance, you'll better understand how your machine learning model(s) fit into the big picture of the business question, and how you might ask a different statistical question to gain different insight.


### Statistical objectives unrelated to supervised learning

Naturally, for this course, we'll focus on statistical questions that require supervised learning. But in reality, you should consider other cases too. Let's look at other branches of data analysis that you might wish to consider (though, not for this course), through an example.

_Business objective_: To gain insight into the productivity of two branches of a company.

Examples of statistical questions:

- _Hypothesis testing_. Is the mean number of sick days per employee different between two branches of a company?
    - Supervised learning doesn't cover testing for differences. 
- _Unsupervised learning_. What is the mean sentiment of the internal discussion channels from both branches?
    - There are no records of the response here, as required by supervised learning (by definition). 
- _Statistical inference_. Estimate the mean difference in monthly revenue generated by both branches, along with how certain you are with that estimate. 
    - Supervised learning typically isn't concerned about communicating how certain your estimate is.


## Prerequisites to an analysis

Before going into any analysis, you need to know what data are available, and what state they're in. 

__Why__: The statistical questions you form will depend on the data that are available. For example, you can't form a statistical question about the performance of a division of your business if there are no data available on that division, or if the data you collected aren't indicators of performance. 

__How__: Always make exploratory plots to get a feel for the data -- and keep doing so to explore other aspects of the data, as you learn more about the data set by doing your analysis. This is often called _Exploratory Data Analysis_ (or EDA). 


<!--chapter:end:000-problem_solving_flow.Rmd-->

# An outcome on its own {-}

How can we get a handle on an outcome that seems random? Although the score of a Canucks game, a stock price, or river flow is uncertain, this does not mean that these quantities are futile to predict or describe. This part of the book describes how to do just that, using only observations on a single outcome, by shedding light on concepts of probability and univariate analysis as they apply to data science. 

# Distributions: Uncertainty is worth explaining

<!--chapter:end:010-Uncertainty_is_worth_explaining.Rmd-->

# Explaining an uncertain outcome: interpretable quantities

**Caution: in a highly developmental stage! See Section  \@ref(caution).**


## Probabilistic Quantities

- Sometimes confusingly called "parameters".
- Explain the quantities by their interpretation/usefulness, using examples.
	- Mean: what number would you want if you have 10 weeks worth of data, and you want to estimate the total after 52 weeks (one year)?
	- Median: 
	- Mode:
	- High Quantile:
	- Low Quantile:
	- Extreme quantile?

When you want information about an unknown quantity, it's up to you what you decide to use. 

The mean is the most commonly sought when the unknown is numeric. I suspect this is the case for two main reasons:

1. It simplifies computations.
2. It's what's taught in school.


## What is the mean, anyway?

Imagine trying to predict your total expenses for the next two years. You have monthly expenses listed for the past 12 months. What's one simple way of making your prediction? Calculate the average expense from the past 12 months, and multiply that by 24.

In general, a mean (or expected value) can be interpreted as the _long-run average_. However, the mean tends to be interpreted as a _measure of central tendency_, which has a more nebulous interpretation as a "typical" outcome, or an outcome for which most of the data will be "nearest".

## Quantiles

It's common to "default" to using the mean to make decisions. But, the mean is not always appropriate (I wrote a [blog post](https://vincenzocoia.github.io/20180218-mean/) about this):

- Sometimes it makes sense to relate the outcome to a coin toss.
    - For example, find an amount for which next month's expenditures will either exceed or be under with a 50% chance. 
- Sometimes a conservative/liberal estimate is wanted.
    - For example, a bus company wants conservative estimates so that _most_ busses fall within the estimated travel time. 

In these cases, we care about _quantiles_, not the mean. Estimating them is called __quantile regression__ (as opposed to __mean regression__).

Recall what quantiles are: the $\tau$-quantile (for $\tau$ between 0 and 1) is the number that will be exceeded by the outcome with a $(1-\tau)$ chance. In other words, there is a probability of $\tau$ that the outcome will be _below_ the $\tau$-quantile.

$\tau$ is referred to as the _quantile level_, or sometimes the _quantile index_. 

For example, a bus company might want to predict the 0.8-quantile of transit time -- 80% of busses will get to their destination within that time.

<!--chapter:end:020-Explaining_an_uncertain_outcome.Rmd-->

# Data versions of interpretable quantities

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

## Estimation of Probabilistic Quantities

What makes a formula an estimator? It should go to the actual value as the sample size increases.

It's important to note that there are many ways to estimate a probabilistic quantity. Some of these will be better than others, often depending on the situation. Typically, the bias-variance tradeoff is at play here, where some estimators sacrifice bias to reduce variance, or vice versa. 

<!--chapter:end:030-Data_versions_of_interpretable_quantities.Rmd-->

# Sampling distributions: Another layer of uncertainty added from estimation

<!--chapter:end:040-Another_layer_of_uncertainty_added_from_estimation.Rmd-->

# Improving estimator quality by parametric distributional assumptions and MLE

<!--chapter:end:050-Improving_estimator_quality_by_parametric_distributional_assumption_.Rmd-->

# Prediction: harnessing the signal {-}



# Reducing uncertainty of the outcome: including predictors

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

## Variable terminology

In supervised learning:

- The output is a random variable, typically denoted $Y$. 
- The input(s) variables (which may or may not be random), if there are $p$ of them, are typically denoted $X_1$, ..., $X_p$ -- or just $X$ if there's one. 

There are many names for the input and output variables. Here are some (there are more, undoubtedly):

- __Output__: response, dependent variable. 
- __Input__: predictors, covariates, features, independent variables, explanatory variables, regressors. 

In BAIT 509, we will use the terminology _predictors_ and _response_.

### Variable types

Terminology surrounding variable types can be confusing, so it's worth going over it. Here are some non-technical definitions. 

- A __numeric__ variable is one that has a quantity associated with it, such as age or height. Of these, a numeric variable can be one of two things:
- A __categorical__ variable, as the name suggests, is a variable that can be one of many categories. For example, type of fruit; success or failure.  


## Irreducible Error

The concept of __irreducible error__ is paramount to supervised learning. Next time, we'll look at the concept of _reducible_ error. 

When building a supervised learning model (like linear regression), we can never build a perfect forecaster -- even if we have infinite data!

Let's explore this notion. When we hypothetically have an infinite amount of data to train a model with, what we actually have is the _probability distribution_ of $Y$ given any value of the predictors. The uncertainty in this probability distribution is the __irreducible error__.

__Example__: Let's say $(X,Y)$ follows a (known) bivariate Normal distribution. Then, for any input of $X$, $Y$ has a _distribution_. Here are some examples of this distribution for a few values of the predictor variable (these are called _conditional_ distributions, because they're conditional on observing particular values of the predictors).

```{r, fig.width=6, echo=FALSE}
xvals <- c(-1.5, 0, 3)
ggplot(tibble(x=c(-5,7)), aes(x)) +
    stat_function(fun=function(x) dnorm(x, xvals[1]),
                  mapping=aes(fill=paste0("x=", xvals[1])), 
                  geom="area", alpha=0.5) +
    stat_function(fun=function(x) dnorm(x, xvals[2]),
                  mapping=aes(fill=paste0("x=", xvals[2])), 
                  geom="area", alpha=0.5) +
    stat_function(fun=function(x) dnorm(x, xvals[3]),
                  mapping=aes(fill=paste0("x=", xvals[3])), 
                  geom="area", alpha=0.5) +
    theme_bw() +
    scale_fill_brewer("Predictor\nvalue", palette="Dark2") +
    labs(x="y", y="Density Function") +
    theme(legend.position="top")
```

This means we cannot know what $Y$ will be, no matter what! What's one to do?

- In __regression__ (i.e., when $Y$ is numeric, as above), the go-to standard is to predict the _mean_ as our best guess. 
    - We typically measure error with the __mean squared error__ = average of (observed-predicted)^2. 
- In __classification__, the conditional distributions are categorical variables, so the go-to standard is to predict the _mode_ as our best guess (i.e., the category having the highest probability). 
    - A typical measurement of error is the __error rate__ = proportion of incorrect predictions.
    - A more "complete" picture of error is the __entropy__, or equivalently, the __information measure__. 

In Class Meeting 07, we'll look at different options besides the mean and the mode.

An important concept is that _predictors give us more information about the response_, leading to a more certain distribution. In the above example, let's try to make a prediction when we don't have knowledge of predictors. Here's what the distribution of the response looks like:

```{r, fig.width=6, echo=FALSE}
ggplot(tibble(x=c(-5,7)), aes(x)) +
    stat_function(fun=function(x) dnorm(x, sd=2),
                  geom="area", fill="gray", alpha=0.5) +
    theme_bw() +
    labs(x="y", y="Density Function")
```

This is much more uncertain than in the case where we have predictors!

## In-class Exercises: Irreducible Error

**NOT REQUIRED FOR PARTICIPATION**

### Oracle regression

Suppose you have two independent predictors, $X_1, X_2 \sim N(0,1)$, and the conditional distribution of $Y$ is
$$ Y \mid (X_1=x_1, X_2=x_2) \sim N(5-x_1+2x_2, 1). $$
From this, it follows that:

- The conditional distribution of $Y$ given _only_ $X_1$ is
$$ Y \mid X_1=x_1 \sim N(5-x_1, 5). $$
- The conditional distribution of $Y$ given _only_ $X_2$ is
$$ Y \mid X_2=x_2 \sim N(5+2x_2, 2). $$
- The (marginal) distribution of $Y$ (not given any of the predictors) is
$$ Y \sim N(5, 6). $$

The following R function generates data from the joint distribution of $(X_1, X_2, Y)$. It takes a single positive integer as an input, representing the sample size, and returns a `tibble` (a fancy version of a data frame) with columns named `x1`, `x2`, and `y`, corresponding to the random vector $(X_1, X_2, Y)$, with realizations given in the rows. 

```
genreg <- function(n){
    x1 <- rnorm(n)
    x2 <- rnorm(n)
    eps <- rnorm(n)
    y <- 5-x1+2*x2+eps
    tibble(x1=x1, x2=x2, y=y)
}
```


1. Generate data -- as much as you'd like.

```
dat <- genreg(1000)
```


2. For now, ignore the $Y$ values. Use the means from the distributions listed above to predict $Y$ under four circumstances:
    1. Using both the values of $X_1$ and $X_2$.
    2. Using only the values of $X_1$.
    3. Using only the values of $X_2$.
    4. Using neither the values of $X_1$ nor $X_2$. (Your predictions in this case will be the same every time -- what is that number?)
    
```
dat <- mutate(dat,
       yhat = FILL_THIS_IN,
       yhat1 = FILL_THIS_IN,
       yhat2 = FILL_THIS_IN,
       yhat12 = FILL_THIS_IN)
```
    

3. Now use the actual outcomes of $Y$ to calculate the mean squared error (MSE) for each of the four situations. 
    - Try re-running the simulation with a new batch of data. Do your MSE's change much? If so, choose a larger sample so that these numbers are more stable.
    
```
(mse <- mean((dat$FILL_THIS_IN - dat$y)^2))
(mse1 <- mean((dat$FILL_THIS_IN - dat$y)^2))
(mse2 <- mean((dat$FILL_THIS_IN - dat$y)^2))
(mse12 <- mean((dat$FILL_THIS_IN - dat$y)^2))
knitr::kable(tribble(
    ~ Case, ~ MSE,
    "No predictors", mse,
    "Only X1", mse1,
    "Only X2", mse2,
    "Both X1 and X2", mse12
))
```

    
4. Order the situations from "best forecaster" to "worst forecaster". Why do we see this order?


### Oracle classification

Consider a categorical response that can take on one of three categories: _A_, _B_, or _C_. The conditional probabilities are:
$$ P(Y=A \mid X=x) = 0.2, $$
$$ P(Y=B \mid X=x) = 0.8/(1+e^{-x}), $$

To help you visualize this, here is a plot of $P(Y=B \mid X=x)$ vs $x$ (notice that it is bounded above by 0.8, and below by 0).

```{r}
ggplot(tibble(x=c(-7, 7)), aes(x)) +
    stat_function(fun=function(x) 0.8/(1+exp(-x))) +
    ylim(c(0,1)) +
    geom_hline(yintercept=c(0,0.8), linetype="dashed", alpha=0.5) +
    theme_bw() +
    labs(y="P(Y=B|X=x)")
```

Here's an R function to generate data for you, where $X\sim N(0,1)$. As before, it accepts a positive integer as its input, representing the sample size, and returns a tibble with column names `x` and `y` corresponding to the predictor and response. 

```
gencla <- function(n) {
    x <- rnorm(n) 
    pB <- 0.8/(1+exp(-x))
    y <- map_chr(pB, function(t) 
            sample(LETTERS[1:3], size=1, replace=TRUE,
                   prob=c(0.2, t, 1-t-0.2)))
    tibble(x=x, y=y)
}
```


1. Calculate the probabilities of each category when $X=1$. What about when $X=-2$? With this information, what would you classify $Y$ as in both cases?
    - BONUS: Plot these two conditional distributions. 

```
## X=1:
(pB <- FILL_THIS_IN)
(pA <- FILL_THIS_IN)
(pC <- FILL_THIS_IN)
ggplot(tibble(p=c(pA,pB,pC), y=LETTERS[1:3]), aes(y, p)) +
    geom_col() +
    theme_bw() +
    labs(y="Probabilities", title="X=1")
## X=-2
(pB <- FILL_THIS_IN)
(pA <- FILL_THIS_IN)
(pC <- FILL_THIS_IN)
ggplot(tibble(p=c(pA,pB,pC), y=LETTERS[1:3]), aes(y, p)) +
    geom_col() +
    theme_bw() +
    labs("Probabilities", title="X=-2")
```

2. In general, when would you classify $Y$ as _A_? _B_? _C_?

### (BONUS) Random prediction

You might think that, if we know the conditional distribution of $Y$ given some predictors, why not take a random draw from that distribution as our prediction? After all, this would be simulating nature.

The problem is, this prediction doesn't do well. 

Re-do the regression exercise above (feel free to only do Case 1 to prove the point), but this time, instead of using the mean as a prediction, use a random draw from the conditional distributions. Calculate the MSE. How much worse is it? How does this error compare to the original Case 1-4 errors?

### (BONUS) A more non-standard regression

The regression example given above is your perfect, everything-is-linear-and-Normal world. Let's see an example of a joint distribution of $(X,Y)$ that's _not_ Normal. 

The joint distribution in question can be respresented as follows:
$$ Y|X=x \sim \text{Beta}(e^{-x}, 1/x), $$
$$ X \sim \text{Exp}(1). $$

Write a formula that gives a prediction of $Y$ from $X$ (you might have to look up the formula for the mean of a Beta random variable). Generate data, and evaluate the MSE. Plot the data, and the conditional mean as a function of $x$ overtop. 

### (BONUS) Oracle MSE

What statistical quantity does the mean squared error (MSE) reduce to when we know the true distribution of the data? Hint: if each conditional distribution has a certain variance, what then is the MSE?

What is the error rate in the classification setting?

<!--chapter:end:060-Reducing_uncertainty_of_the_outcome.Rmd-->

# The signal: model functions


## Linear Quantile Regression

```{r, warning=FALSE}
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(Lahman))
```



The idea here is to model
$$Q(\tau)=\beta_0(\tau) + \beta_1(\tau) X_1 + \cdots + \beta_p(\tau) X_p,$$
where $Q(\tau)$ is the $\tau$-quantile. In other words, __each quantile level gets its own line__, and are each fit independently of each other. 

Here are the 0.25-, 0.5-, and 0.75-quantile regression lines for the baseball data:

```{r}
ggplot(baseball, aes(hits, runs)) +
    geom_point(alpha=0.1, colour="orange") +
    geom_quantile(colour="black") +
    theme_bw() +
    labs(x="Number of Hits (X)",
         y="Number of Runs (Y)")
```

I did this easily with `ggplot2`, just by adding a layer `geom_quantile` to my scatterplot, specifying the quantile levels with the `quantiles=` argument. We could also use the function `rq` in the `quantreg` package in R:

```{r, echo=TRUE}
(fit_rq <- rq(runs ~ hits, data=baseball, tau=c(0.25, 0.5, 0.75)))
```

If we were to again focus on the two teams (one with 1000 hits, and one with 1500 hits), we have (by evaluating the above three lines):

```{r}
predict(fit_rq, newdata=data.frame(hits=c(1000, 1500)))
```

So, we could say that the team with 1000 hits: 

- is estimated to have a 50% chance to have between 434 and 555 runs; 
- has a 25% chance of achieving over 555 runs;
- has a 25% chance of getting less than 434 runs;
- would typically get 501 runs (median);

amongst other things. 

### Exercise

- Get a 95% prediction interval using linear quantile regression, with Y=`R` (number of runs), X=`H` (number of hits), when X=1500.
- What about a 95% PI using kNN, going back to the earlier example we did?

### Problem: Crossing quantiles

Because each quantile is allowed to have its own line, some of these lines might cross, giving an __invalid result__. Here is an example with the `iris` data set, fitting the 0.2- and 0.3-quantiles:

```{r, warning=FALSE}
ggplot(iris, aes(Sepal.Length, Sepal.Width)) +
    geom_point(alpha=0.25, colour="orange") +
    geom_quantile(aes(colour="0.2"), quantiles=0.2) +
    geom_quantile(aes(colour="0.3"), quantiles=0.3) +
    scale_colour_discrete("Quantile\nLevel") +
    theme_bw() +
    labs(x="Sepal Length",
         y="Sepal Width")
fit_iris <- rq(Sepal.Width ~ Sepal.Length, data=iris, tau=2:3/10)
b <- coef(fit_iris)
at8 <- round(predict(fit_iris, newdata=data.frame(Sepal.Length=8)), 2)
```

Quantile estimates of Sepal Width for plants with Sepal Length less than ```r round((b[1,1]-b[1,2])/(b[2,2]-b[2,1]), 2)``` are valid, but otherwise, are not. For example, for plants with a Sepal Length of 8, this model predicts 30% of such plants to have a Sepal Width of less than ```r at8[2]```, but only 20% of such plants should have Sepal Width less than ```r at8[1]```. This is an illogical statement. 

There have been several "adjustments" proposed to ensure that this doesn't happen (see below), but ultimately, this suggests an inadequacy in the model assumptions. Luckily, this usually only happens at extreme values of the predictor space, and/or for large quantile levels, so is usually not a problem. 

- Bondell HD, Reich BJ, Wang H. Noncrossing quantile regression curve estimation. Biometrika. 2010;97(4):825-838.
- Dette H, Volgushev S. Non-crossing non-parametric estimates of quantile curves. J R Stat Soc Ser B Stat Methodol. 2008;70(3):609-627.
- Tokdar ST, Kadane JB. Simultaneous linear quantile regression: a semiparametric Bayesian approach. Bayesian Anal. 2011;6(4):1-22.

### Problem: Upper quantiles

Estimates of higher quantiles usually become worse for large/small values of $\tau$. This is especially true when data are heavy-tailed. 

Check out the Chapter on Extreme Value Regression for more info.

<!--chapter:end:070-The_signal.Rmd-->

# The Model-Fitting Paradigm in R

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

Scratch notes from our in-class activities in Lecture 3. This covers the model-fitting paradigm in R.

```{r}
library(tidyverse)
```

Use the `iris` data to demonstrate. Split into training and test data:

```{r}
head(iris)
set.seed(100)
iris <- mutate(
	iris, 
	training = caTools::sample.split(Sepal.Width, SplitRatio=0.8))
#	   training = sample(1:2, replace=TRUE, prob=c(0.8, 0.2), size=nrow(iris)))
iris_train <- filter(iris, training)
iris_test <- filter(iris, !training)
#caret::createDataPartition()
```

Fit a LOESS model:

```{r}
fit <- loess(Sepal.Width ~ Petal.Width, data=iris_train)
```

Make predictions:

```{r}
yhat <- predict(fit, newdata=iris_test)
```

Calculate error:

```{r}
mean((yhat - iris_test$Sepal.Width)^2)
```

## `broom` package

```{r}
library(broom)
#tidy(fit)
#glance(fit)
augment(fit, newdata = iris_test)
```

With linear regression:

```{r}
fit2 <- lm(Sepal.Width ~ Petal.Width, data=iris_train)
#unclass(fit2)
#unclass(summary(fit2))

tidy(fit2)
augment(fit2)
glance(fit2)
```


<!--chapter:end:075-The_model_fitting_paradigm_in_r.Rmd-->

# Estimating parametric model functions


**Caution: in a highly developmental stage! See Section  \@ref(caution).**

## Writing the sample mean as an optimization problem


(DSCI 561 lab2, 2018-2019)

It's important to know that the sample mean can also be calculated by finding the value that minimizes the sum of squared errors with respect to that value. We'll explore that here.

Store some numbers in the vector y.
Calculate the sample mean of the data, stored in mu_y.
This is not worth any marks, but having it as its own question jibes better with the autograder.

We've defined sse() below, a function that takes some number and returns the sum of squared "errors" of all values of y with respect to the inputted number. An "error" is defined as the difference between two values.

We've also generated a quick plot of this function for you.

```
sse <- Vectorize(function(m) sum((y - m)^2))
curve(sse, mu_y - 2*sd(y), mu_y + 2*sd(y))
```

Your task: use the optimize() function to find the value that minimizes the sum of squared errors. 

Hint: for the interval argument, specify an interval that contains the sample mean.

Important points:

You should recognize that the sample mean minimizes this function!
You'll be seeing the sum of squared errors a lot through the program (mean squared error and R^2 are based on it). This is because the mean is a very popular quantity to model and use as a prediction.
If you're not convinced, play with different numbers to see for yourself.


## Evaluating Model Goodness: Quantiles

The question here is: if we have two or more models that predicts the $\tau$-quantile, which model is best? We'll need some way to score different models to do things such as:

- Choose which predictors to include in a model;
- Choose optimal hyperparameters;
- Estimate parameters in a quantile regression model.

\*\*__NOTE__\*\*: __Mean Squared Error is not appropriate here!!__ This is very important to remember. 

The reason is technical -- the MSE is not a _proper scoring rule_ for quantiles. In other words, the MSE does not elicit an honest prediction.

If we're predicting the __median__, then the _mean absolute error_ works. This is like the MSE, but instead of _squaring_ the errors, we take the _absolute value_.

In general, a "correct" scoring rule for the $\tau$-quantile is as follows:
$$ S = \sum_{i=1}^{n} \rho_{\tau}(Y_i - \hat{Q}_i(\tau)), $$
where $Y_i$ for $i=1,\ldots,n$ is the response data, $\hat{Q}_i(\tau)$ are the $\tau$-quantile estimates, and $\rho_{\tau}$ is the __check function__ (also known as the _absolute asymmetric deviation function_ or _tick function_), given by
$$ \rho_{\tau}(s) = (\tau - I(s<0))s $$
for real $s$. This scoring rule is __negatively oriented__, meaning the lower the score, the better. It cannot be below 0. 

Here is a plot of various check functions. Notice that, when $\tau=0.5$ (corresponding to the median), this is proportional to the absolute value:

```{r, fig.width=8, fig.height=3}
base <- ggplot(data.frame(x=c(-2,2)), aes(x)) + 
    theme_bw() +
    labs(y=expression(rho)) +
    theme(axis.title.y=element_text(angle=0, vjust=0.5)) +
    ylim(c(0, 1.5))
rho <- function(tau) function(x) (tau - (x<0))*x
cowplot::plot_grid(
    base + stat_function(fun=rho(0.2)) + 
        ggtitle(expression(paste(tau, "=0.2"))),
    base + stat_function(fun=rho(0.5)) + 
        ggtitle(expression(paste(tau, "=0.5"))),
    base + stat_function(fun=rho(0.8)) + 
        ggtitle(expression(paste(tau, "=0.8"))),
    ncol=3
)
```

For quantile regression __estimation__, we minimize the sum of scores instead of the sum of squared residuals, as in the usual (mean) linear regression.


## Simple Linear Regression

(From lab2, DSCI 561, 2018-2019)

When a predictor is categorical, it's easy to estimate the mean given a certain predictor value (i.e., given the category): just take the sample average of the data in that group.

Now let's consider a numeric predictor. Using the iris dataset again with sepal width as a response, use sepal length as the predictor. Here is a scatterplot of the data:

```
(p_numeric_x <- ggplot(iris, aes(Sepal.Length, Sepal.Width)) +
    geom_point(alpha=0.25) +
    theme_bw() +
    labs(x = "Sepal Length",
         y = "Sepal Width"))
```

How can we estimate the mean sepal width ($Y$) for any given sepal length ($X$)? Say we want the mean of $Y$ at $X=x$ (for some pre-decided $x$). Last week in DSCI 571 Lab 2 Exercise 5, you saw one way of estimating this: calculate the mean sepal width ($Y$) using only the $k$ plants having sepal lengths ($X$ values) closest to $x$ (the sepal length you're interested in).

Methods like this are very powerful estimation methods, but there's merit in assuming the mean is linear in $x$: $$E(Y \mid X=x) = \beta_0 + \beta_1 x,$$ for some numbers $\beta_0$ and $\beta_1$ (to be estimated).

How do we estimate $\beta_0$ and $\beta_1$? In other words, how do we pick an acceptable line? Since we want the line to represent the mean, choose the line that minimizes the sum of squared errors -- remember, this is another way of writing the sample average in the univariate case, and now we can generalize the univariate mean to the regression setting in this way.

Is it possible to find a line that has a smaller sum of squared errors than what you found in Exercise 3.3? Why or why not? Is it possible to find a line that has a smaller sum of absolute errors (i.e., the absolute value of the errors)? Elaborate.

### Model Specification

You might see linear regression models specified in different ways.

In this exercise, we're still working with sepal length as the only predictor of sepal width.

Denote $\beta_0$ as the true intercept of the regression line, and $\beta_1$ as the true slope. As we've said, we're assuming that the mean of $Y$ is linear in the predictor: $$E(Y \mid X=x) = \beta_0 + \beta_1 x.$$ There are other ways to write this model; i.e., different ways of saying the same thing (not to be confused with different parameterizations). We'll explore this here.

4.1
rubric={reasoning:3}

One way to write this model is to emphasize that this model holds for every single observation, instead of for a generic $Y$. Denote $Y_i$ as the random variable corresponding to the $i$'th observation of the response, and $x_i$ the corresponding observed value of the predictor. Let $n$ be the sample size.

Your task: specify what goes in the $?$ in the following equation:

$$E(Y_i | X_i = x_i) = \text{ ?}, \text{ for each } i=1,\ldots,n.$$
YOUR ANSWER HERE

4.2
rubric={reasoning:3}

We could also specify how $Y_i$ itself was supposedly calculated. Your task: specify what goes in the $?$ in the following equation.

$$Y_i = \text{ ?}, \text{ for each } i=1,\ldots,n.$$
Hint: you'll have to introduce a variable. Be sure to specify any assumptions about this variable so that your equation is equivalent to the one in Exercise 4.1 -- this means not putting more assumptions than are necessary, too!

YOUR ANSWER HERE

4.3
rubric={reasoning:3}

Instead of having to say "for each $i=1,\ldots,n$", we could just write out each of the $n$ equations. It's actually convenient to do so, but expressed as one equation by using matrix algebra.

We'll use bold-face to denote vectors. Denote $\boldsymbol{Y}$ as the vector containing $Y_1,\ldots,Y_n$ (in that order), and similarly for $\boldsymbol{X}$ and $\boldsymbol{x}$. Denote $\boldsymbol{\beta}$ as the vector containing $\beta_0$ and $\beta_1$ (in that order). Then, the same equation becomes: $$E(\boldsymbol{Y} \mid \boldsymbol{X} = \boldsymbol{x}) = \text{? }\boldsymbol{\beta}, $$ where "?" is an $n \times 2$ matrix.

Your task: specify the matrix indicated by "?" in the above equation. It's probably most convenient to describe what each column contains. Each column is worth approx. 50% of your grade for this question.

YOUR ANSWER HERE

## Linear models in general

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

(DSCI 561 lab 2, 2018-2019)

In general, linear models estimate the mean using $p$ predictors $X_1, \ldots, X_p$ (this time, the subscripts denote "predictor number" instead of "observation number", and the vectors $\boldsymbol{X}$ and $\boldsymbol{x}$ contain the predictors, not the observations), according to the following generic equation: $$E(Y \mid \boldsymbol{X}=\boldsymbol{x}) = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p.$$ We saw that:

a $K$-level categorical predictor enters the equation through $K-1$ binary predictors (relative to a "baseline" category), and
a numeric predictor enters the equation as itself.
We will now consider using both sepal length (numeric) and species (categorical) as predictors of sepal width.

6.1

Fit a linear regression line to sepal length ($X$) vs. sepal width ($Y$) for each species independently. Plot the results by facetting by species.

Note that all we're looking for here is the plot. You can bypass the lm() calls by adding the layer geom_smooth(method="lm", se=FALSE), which runs the linear regression separately in each panel.



Although these look like three separate models, it's still just one model: one specification as to how to estimate the mean. We can write a single equation that describes this specification, using the following variables: $$X_1 = \text{sepal length}$$$$X_2 = 1 \text{ if versicolor, } 0, \text{ otherwise}$$$$X_3 = 1 \text{ if virginica, } 0, \text{ otherwise}$$$$X_4 = X_2 X_1$$$$X_5 = X_3 X_1$$ The model becomes: $$E(Y \mid \boldsymbol{X} = \boldsymbol{x}) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + \beta_5 X_5 $$

Your task: specify the slope and intercept of the regression lines for each species, in terms of the $X$'s and $\beta$'s above. We've given you the answer for Setosa already. Answer by copying and pasting the below table into the answer cell, and filling in the missing table cells.

Hint: evaluate whatever $X$'s you can.

Species	Intercept	Slope
Setosa	$\beta_0$	$\beta_1$
Versicolor		
Virginica		
YOUR ANSWER HERE

6.3

$X_4$ and $X_5$ are called interaction terms, and are not present by default in the lm() function. In their absence, what are the slopes and intercepts of the regression line for each species? Answer like you did above, in terms of the $X$'s and $\beta$'s, by filling in the below table. We've given you the answer for Setosa already.

Species	Intercept	Slope
Setosa	$\beta_0$	$\beta_1$
Versicolor		
Virginica		
YOUR ANSWER HERE

6.4

Make a similar plot as in Exercise 6.1, but for the model without interaction.

## reference-treatment parameterization

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

(From DSCI 561 lab1, 2018-2019)

When data fall into groups, you already know how to estimate the population mean for each group: calculate the sample mean of the data in each group. For example, the mean Sepal.Width of the three species in the iris dataset are:

```
iris %>% 
    group_by(Species) %>% 
    summarize(mean_sepal_width = mean(Sepal.Width))
```

In this exercise, you'll start exploring a "trick" that linear regression models use so that we can obtain the same mean estimates by evaluating a line. Although it might seem silly to do, especially when we can just do a group_by() and summarize(), using this trick will be very important when we start incorporating more variables in our model, as we'll soon see in this course.

Next, using the "trick" that linear regression models use, you'll write these estimates as a line, so that we can obtain the mean estimates by evaluating the line. Here's the trick: convert the categories to a numeric variable, where one category takes the value 0, and the other takes the value 1.

Let's convert "first" to 0, and "other" to 1:

```
x_map <- c(first=0, other=1)
preg <- mutate(preg, num_x = x_map[birth_order])
head(preg)
```

What's the equation of the line that goes through the mean estimates of both groups? Specify this by storing the slope and (y-) intercept of the line in the variables preg_slope and preg_int, respectively. This line is called the regression line.

In Inf-1 lab3, you made a plot of the data, including the two means with a confidence interval. Here's the code (using asymptotics to form the CI), zooming in on the "center" of the data (uncomment preg_plot to view the plot):

```
preg_plot <- preg %>%
    group_by(birth_order, num_x) %>%
    summarize(mean = mean(prglngth),
              n    = length(prglngth),
              se   = sd(prglngth) / sqrt(n)) %>%
    ggplot(aes(x = num_x)) +
    geom_violin(data    = preg,
                mapping = aes(y = prglngth, group = birth_order), 
                adjust  = 5) +
    geom_point(aes(y = mean), colour = "red") +
    geom_errorbar(aes(ymin = mean + qnorm(alpha/2)*se,
                      ymax = mean - qnorm(alpha/2)*se),
                  colour = "red",
                  width  = 0.2) +
    theme_bw() +
    labs(x = "Birth Order", 
         y = "Pregnancy Length (weeks)") +
    ylim(c(30, 50)) +
    scale_x_continuous(breaks = enframe(x_map)$value, 
                       labels = enframe(x_map)$name)
```

Add the line to this plot.

Can we always draw a straight line through the means of two groups? Why or why not?

In this lab, we won't be exploring the trick that linear regression models use when we have multiple groups. But, you'll explore what we can't do.

For each species in the iris (three-group) data set, the code below:

calculates the mean sepal width in the column mean_sepwid, along with the standard error of the mean in the se column, placed in the data frame iris_est.
plots the raw data with a violin+jitter plot, stored in the variable iris_plot (uncomment it to view it).

```
(iris_est <- iris %>% 
    group_by(Species) %>% 
    summarize(
        mean_sepwid = mean(Sepal.Width),
        se          = sd(Sepal.Width)/sqrt(length(Sepal.Width))
    ))
iris_plot <- iris %>% 
    mutate(Species = fct_reorder(Species, Sepal.Width)) %>% 
    ggplot(aes(Species)) +
    geom_violin(aes(y = Sepal.Width)) +
    geom_jitter(aes(y = Sepal.Width), 
                alpha=0.2, width=0.1, size=0.5) +
    theme_bw() +
    labs(x = "Species", 
         y = "Sepal Width")
iris_plot
```

Your task is to add the group means and confidence intervals to the plot. You can do this by adding layers to iris_plot. You can use asymptotic theory to calculate the confidence intervals, calculated by:

$$\bar{x} \pm z_{\alpha/2} \text{SE}.$$

Can we fit a single straight line through the mean sepal widths across the three species groups? Why or why not?

### More than one category (Lab 2)

In class, we saw two "ways to store information" about groups means -- in technical terms, two parameterizations.

The first and most "direct" parameterization is cell-wise parameterization, a fancy way of saying that we're just going to consider the raw means themselves: one mean for each group. For the three species in the iris dataset, here are estimates of these parameters:

```
iris %>% 
	group_by(Species) %>% 
	summarize(mean_sepal_width = mean(Sepal.Width))
```
The mean of the response (conditional on species) can be written as a linear model, if we call the above means $\mu_0,\mu_1,\mu_2$ (respectively), and define the following three predictors:

$$X_0 = 1 \text{ if setosa, } 0 \text{ otherwise},$$ $$X_1 = 1 \text{ if versicolor, } 0 \text{ otherwise},$$ $$X_2 = 1 \text{ if virginica, } 0 \text{ otherwise}.$$
Then, the linear model is

$$E(\text{Sepal Width} \mid \text{species}) = \mu_0 X_0 + \mu_1 X_1 + \mu_2 X_2.$$
In this exercise, you'll be exploring another parameterization that's useful in linear regression: the reference-treatment parameterization.

1.1

To keep things different from lm(), let's consider the virginica species as our "reference". The reference-treatment parameterization is then: $$\theta=\mu_{\text{virginica}},$$ $$\tau_1=\mu_{\text{versicolor}}-\theta,$$ $$\tau_2=\mu_{\text{setosa}}-\theta,$$ where $\mu$ denotes the mean of that species' sepal width.

Your task: Calculate estimates of these parameters, and store the estimates in the (respective) variables theta, tau1, and tau2.




Provide an interpretation for $\theta$, $\tau_1$, and $\tau_2$. One brief sentence for each is enough.

Let's now write this information as a single (linear) equation containing:

two predictors $X_1$ and $X_2$, and
the parameters $\theta$, $\tau_1$, and $\tau_2$.
Let's focus on the predictors first. Define: $$X_1 = 1 \text{ if versicolor, } 0 \text{ otherwise},$$ $$X_2 = 1 \text{ if setosa, } 0 \text{ otherwise}.$$ We've deliberately not defined a predictor for virginica, the reference group.

Your task: What are the values of $X_1$ and $X_2$ for each species? Store the values in three length-2 vectors called x_setosa, x_versicolor, and x_virginica.

Use the predictors $X_1$ and $X_2$, along with the parameters $\theta$, $\tau_1$, and $\tau_2$, to write a linear equation that returns the species mean $\mu$. The equation should look like: $$E(Y \mid \text{species}) = (1) + (2)\times(3) + (4)\times(5)$$ To use the autograder for this question, specify the order that $X_1$, $X_2$, $\theta$, $\tau_1$, and $\tau_2$ are to appear in the equation, respectively, in a vector containing the numbers 1 through 5, named eq_order.

For example, specifying eq_order <- c(1,2,3,4,5) corresponds to the equation $E(Y \mid \text{species}) = X_1 + X_2  \theta + \tau_1  \tau_2$ (which is not the correct equation)

(Based on your answers to 1.4 and 1.5, can you see why the parameter that goes in place of (1) is also called the "intercept"?)

Now try using lm(): use the iris data with the same predictor and response (don't include -1 in the formula, so that you end up with a reference-treatment parameterization).

Your task: What's the reference species? Put the name of the species as a character in the variable iris_lm_ref_species.

<!--chapter:end:080-Estimating_parametric_model_functions.Rmd-->

# Estimating assumption-free: the world of supervised learning techniques

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

## What machine learning is

What is Machine Learning (ML) (or Statistical Learning)? As the [ISLR book](http://www-bcf.usc.edu/~gareth/ISL/) puts it, it's a "vast set of tools for understanding data". Before we explain more, we need to consider the two main types of ML:

- __Supervised learning__. (_This is the focus of BAIT 509_). Consider a "black box" that accepts some input(s), and returns some type of output. Feed it a variety of input, and write down the output each time (to obtain a _data set_). _Supervised learning_ attempts to learn from these data to re-construct this black box. That is, it's a way of building a forecaster/prediction tool. 

You've already seen examples throughout MBAN. For example, consider trying to predict someone's wage (output) based on their age (input). Using the `Wage` data set from the `ISLR` R package, here are examples of inputs and outputs:

```{r, echo=FALSE}
Wage %>%
    select(age, wage) %>%
    head %>%
    magrittr::set_rownames(1:6)
```

We try to model the relationship between age and wage so that we can predict the salary of a new individual, given their age. 

An example supervised learning technique is _linear regression_, which you've seen before in BABS 507/508. For an age `x`, let's use linear regression to make a prediction that's quadratic in `x`. Here's the fit:

```{r, echo=FALSE}
ggplot(Wage, aes(age, wage)) +
    geom_point(alpha=0.2) +
    geom_smooth(method="lm", se=FALSE,
                formula=y~poly(x,2)) +
    ylab("Wage (1000s of USD)") +
    xlab("Age") +
    theme_bw()
```

The blue curve represents our attempt to "re-construct" the black box by learning from the existing data. So, for a new individual aged 70, we would predict a salary of about \$100,000. A 50-year-old, about \$125,000.


- __Unsupervised learning__. (_BAIT 509 will not focus on this_). Sometimes we can't see the output of the black box. _Unsupervised learning_ attempts to find structure in the data without any output. 

For example, consider the following two gene expression measurements (actually two principal components). Are there groups that we can identify here?

```{r, echo=FALSE}
NCI60[[1]] %>% 
    prcomp() %>% 
    `[[`("x") %>% 
    as_tibble() %>% 
    select(PC1, PC2) %>% 
    ggplot(aes(PC1, PC2)) +
    geom_point(alpha=0.5) +
    ggtitle("Gene expression data") +
    theme_bw()
```

You've seen methods for doing this in BABS 507/508, such as k-means. 



## Types of Supervised Learning

There are two main types of supervised learning methods -- determined entirely by the type of response variable.

- __Regression__ is supervised learning when the response is numeric.
- __Classification__ is supervised learning when the response is categorical. 

We'll examine both equally in this course. 

Note: Don't confuse classification with _clustering_! The latter is an unsupervised learning method.


## Local Regression

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

(BAIT 509 Class Meeting 03)

```{r setup, include=FALSE}
suppressPackageStartupMessages(library(tidyverse))
rotate_y <- theme(axis.title.y=element_text(angle=0, vjust=0.5))
```

Let's turn our attention to the first "new" machine learning methods of the course: $k$ __Nearest Neighbours__ (aka kNN or $k$-NN) and __loess__ (aka "LOcal regrESSion").

The fundamental idea behind these methods is to _base your prediction on what happened in similar cases in the past_.

### kNN

Pick a positive integer $k$. 

To make a prediction of the response at a particular observation of the predictors (I'll call this the __query point__) -- that is, when $X_1=x_1$, ..., $X_p=x_p$:

1. Subset your data to $k$ observations (rows) whose values of the predictors $(X_1, \ldots, X_p)$ are closest to $(x_1,\ldots,x_p)$.
2. For kNN classificiation, use the "most popular vote" (i.e., the modal category) of the subsetted observations. For kNN regression, use the average $Y$ of the remaining subsetted observations.

Recall how to calculate distance between two vectors $(a_1, \ldots, a_p)$ and $(b_1, \ldots, b_p)$:
$$ \text{distance} = \sqrt{(a_1-b_1)^2 + \cdots + (a_p-b_p)^2}. $$
It's even easier when there's one predictor: it's just the absolute value of the difference. 

### loess

(This is actually the simplest version of loess, sometimes called a __moving window__ approach. We'll get to the "full" loess).

Pick a positive number $r$ (not necessarily integer). 

To make a prediction of the response at a query point (that is, a particular observation of the predictors, $X_1=x_1$, ..., $X_p=x_p$):

1. Subset your data to those observations (rows) having values of the predictors $(X_1,\ldots,X_p)$ within $r$ units of $(x_1,\ldots,x_p)$.
2. For kNN classificiation, use the "most popular vote" (i.e., the modal category) of the subsetted observations. For kNN regression, use the average $Y$ of the remaining subsetted observations.

Notice that Step 2 is the same as in kNN.

$k$ and $r$ are called __hyperparameters__, because we don't estimate them -- we choose them outright.

### In-Class Exercises

Consider the following data set, given by `dat`. Here's the top six rows of data:

```{r, echo=TRUE}
set.seed(87)
dat <- tibble(x = c(rnorm(100), rnorm(100)+5)-3,
              y = sin(x^2/5)/x + rnorm(200)/10 + exp(1))
```

Here's a scatterplot of the data:

```{r}
ggplot(dat, aes(x,y)) + 
    geom_point(colour="orange") +
    theme_bw() + 
    rotate_y
```

#### Exercise 1: Mean at $X=0$

Let's check your understanding of loess and kNN. Consider estimating the mean of $Y$ when $X=0$ by using data whose $X$ values are near 0. 

1. Eyeball the above scatterplot of the data. What would you say is a reasonable estimate of the mean of $Y$ at $X=0$? Why?

2. Estimate using loess and kNN (you choose the hyperparameters).
    1. Hints for kNN:
        - First, add a new column in the data that stores the _distance_ between $X=0$ and each observation. If that column is named `d`, you can do this with the following partial code: `dat$d <- YOUR_CALCULATION_HERE`. Recall that `dat$x` is a vector of the `x` column.
        - Then, arrange the data from smallest distance to largest with `arrange(dat)` (you'll need to load the `tidyverse` package first), and subset _that_ to the first $k$ rows. 
    2. Hints for loess:
        - Subset the data using the `filter` function. The condition to filter on: you want to keep rows whose distances (`d`) are ...
        
```
k <- 10
r <- 0.5
x0 <- 0
dat$dist <- FILL_THIS_IN
dat <- arrange(dat, dist)  # sort by distance
kNN_prediction <- FILL_THIS_IN
loess_prediction <- FILL_THIS_IN
```

3. What happens when you try to pick an $r$ that is way too small? Say, $r=0.01$? Why?

4. There's a tradeoff between choosing large and small values of either hyperparameter. What's good and what's bad about choosing a large value? What about small values?

#### Exercise 2: Regression Curve

Form the __regression curve__ / __model function__ by doing the estimation over a grid of x values, and connecting the dots:

```{r}
xgrid <- seq(-5, 4, length.out=1000)
k <- 10
r <- 0.5
kNN_estimates <- map_dbl(xgrid, function(x_){
    dat %>% 
        mutate(d = abs(x-x_)) %>% 
        arrange(d) %>% 
        summarize(yhat=mean(y[1:k])) %>% 
        `[[`("yhat")
})
loess_estimates <- map_dbl(xgrid, function(x_){
    dat %>% 
        mutate(d = abs(x-x_)) %>% 
        filter(d<r) %>% 
        summarize(yhat=mean(y)) %>% 
        `[[`("yhat")
})
est <- tibble(x=xgrid, kNN=kNN_estimates, loess=loess_estimates) %>% 
    gather(key="method", value="estimate", kNN, loess)
ggplot() +
    geom_point(data=dat, mapping=aes(x,y)) +
    geom_line(data=est, 
              mapping=aes(x,estimate, group=method, colour=method),
              size=1) +
    theme_bw()
```

__Exercises__:

- Play with different values of $k$ and $r$, and regenerate the plot each time. What effect does increasing these values have on the regression curve? What about decreasing? What would you say is a "good" choice of $k$ and $r$, and why?
- What happens when you choose $k=n=200$? What happens if you choose $r=10$ or bigger?

The phenomenon you see when $k$ and $r$ are very small is called __overfitting__. This means that your model displays patterns that are not actually present. __Underfitting__, on the other hand, is when your model misses patterns in the data that are actually present. 

### Hyperparameters and the bias/variance tradeoff 

Let's look at the bias and variance for different values of the hyperparameter in loess. 

```{r, fig.height=8}
set.seed(400)
N <- 100
xgrid <- seq(-7, 6.5, length.out=300)
true_mean <- Vectorize(function(x){
    if (x==0) return(exp(1)) else return(sin(x^2/5)/x + exp(1))
})
## Use "tibble %>% group_by %>% do" in place of `for` loop
expand.grid(iter=1:N, r=c(0.5, 1, 2, 4)) %>% group_by(iter, r) %>% do({
    this_r <- unique(.$r)
    dat <- tibble(x = c(rnorm(100), rnorm(100)+5)-3,
                  y = sin(x^2/5)/x + rnorm(200)/10 + exp(1))
    data.frame(
        .,
        x = xgrid,
        yhat = ksmooth(dat$x, dat$y, kernel="box", 
                         bandwidth=this_r,
                         x.points=xgrid)$y
    )
}) %>% 
    ungroup() %>% 
    mutate(r=paste0("bandwidth=", r)) %>% 
    ggplot(aes(x=x, y=yhat)) +
    facet_wrap(~ r, ncol=1) +
    geom_line(aes(group=iter, colour="Estimates"), alpha=0.1) +
    stat_function(fun=true_mean,
                  mapping=aes(colour="True mean")) +
    theme_bw() +
    scale_colour_brewer("", palette="Dark2") +
    ylab("y") + rotate_y
```

You can see the bias/variance tradeoff here:

- Notice that the estimates get _narrower_ as the bandwidth increases -- this means the variance reduces as the bandwidth increases.
- Notice that the estimates become biased as the bandwidth increases.

A similar phenomenon exists with kNN regression. 

Notice some other things about these plots:

- There's more variance whenever there's less data -- that's at the tails, and (by design) at around $X=0$. 
- Estimates don't exist sometimes, if no data fall in the "window". You can see that the tails are cut short when the bandwidth is small. 


### Extensions to kNN and loess

#### Kernel weighting

kNN and loess can be generalized by downweighing points that are further from the query point. In particular, we take a weighted average.

Suppose $y_1, \ldots, y_n$ are $n$ realizations of the response $Y$. If we assign (respective) weights $w_1, \ldots, w_n$ to these realizations, then the __weighted average__ is
$$ \frac{\sum_{i=1}^n w_i y_i}{\sum_{i=1}^n w_i}. $$

If the response is categorical, then each subsetted observation gives a "weighted vote". Sum up the weights corresponding to each category to obtain the total "number" of votes. 

We obtain these weights using a __kernel function__. A kernel function is any non-increasing function over the positive real numbers. Plug in the distance between the observed predictor(s) and the query point to obtain the weight. Some examples of kernel functions are plotted below.

#### Local polynomials

Another extension of loess is to consider __local polynomials__. The idea here is, after subsetting the data lying within $r$ units of the query point, add the following two steps:

1. Fit a linear (or quadratic) regression model to the subsetted data only.
    - This is the "local polynomial". You can think of this as like a "mini linear regression".
2. Obtain your prediction by evaluating the regression curve at the query point.
3. Throw away your fitted local polynomial.

OK, the 3rd step isn't really a true step, but I like to include it to emphasize that we only evaluate the local polynomial at the query point.

Note:

- We _could_ fit higher order polynomials, but that tends to overfit the data.
- We _could_ fit any other curve locally besides a polynomial, but polynomials are justified by the Taylor approximation.
- Local polynomials with degree=0 is the same as "not doing" local polynomials.

#### Combination

You can combine kernel weighting with local polynomials. When you fit the local polynomial to the subsetted data, you can run a _weighted_ regression. Instead of minimizing the sum of squared errors, we minimize the _weighted_ sum of squared errors. 

#### Other distances

We don't have to use the "usual" notion of distance. The formula I gave you earlier (above) is called the _Euclidean distance_, or L2 norms. There's also the L1 norm (also called the manhattan distance, which is distance by moving along the axes/rectangularly) and L0 norm (number of predictors having non-zero univariate distance).

#### Scaling

When you're using two or more predictors, your predictors might be on different scales. This means distances aren't weighed equally, depending on the direction. Instead of measuring distance on the original scale of the predictors, consider re-scaling the predictors by subtracting the mean and dividing by the standard deviation for each predictor. 

#### Demonstration

Let's look at the same example, but with kernel downweighting and local polynomials. 

Warning! The "bandwidth" hyperparameter in this plot is parameterized differently than in the previous plot, but carries the same interpretation.  

```{r, fig.height=8, fig.width=10}
set.seed(400)
N <- 100
xgrid <- seq(-7, 6.5, length.out=300)
true_mean <- Vectorize(function(x){
    if (x==0) return(exp(1)) else return(sin(x^2/5)/x + exp(1))
})
## Use "tibble %>% group_by %>% do" in place of `for` loop
expand.grid(iter=1:N, r=c(0.1, 0.5, 0.7), d=0:2) %>% 
    group_by(iter, r, d) %>% do({
    this_r <- unique(.$r)
    this_d <- unique(.$d)
    dat <- tibble(x = c(rnorm(100), rnorm(100)+5)-3,
                  y = sin(x^2/5)/x + rnorm(200)/10 + exp(1))
    data.frame(
        .,
        x = xgrid,
        yhat = predict(loess(y~x, data=dat, 
                             span=this_r, degree=this_d),
                       newdata=data.frame(x=xgrid))
    )
}) %>% 
    ungroup() %>% 
    mutate(r=paste0("bandwidth=", r),
           d=paste0("degree=", d)) %>% 
    ggplot(aes(x=x, y=yhat)) +
    facet_grid(r ~ d) +
    geom_line(aes(group=iter, colour="Estimates"), alpha=0.1) +
    stat_function(fun=true_mean,
                  mapping=aes(colour="True mean")) +
    theme_bw() +
    scale_colour_brewer("", palette="Dark2") +
    ylab("y") + rotate_y
```

Notice:

- For small bandwidth, increasing the degree of the poynomial just results in more variance -- degree=0 looks best for this bandwidth.
- But by increasing the degree (inc. variance, dec. bias) _and_ increasing the bandwidth (dec. variance, inc. bias), we end up getting an overall better fit: low bias, low variance. Bandwidth=0.5 and Degree=2 here seem to work best. 

### Model assumptions and the bias/variance tradeoff

Recall that we saw an incorrect model assumption leads to bias, such as fitting linear regression when the true mean is non-linear.

When you make model assumptions that are _close to the truth_, then this has the effect of _decreasing variance_. 

Adding good model assumptions is like adding more data -- after all data is information, and a model assumption is also information. 

Here's a demonstration:

Consider
$$ Y = X + \varepsilon, $$
where $X$ (predictor) is N(0,1), and $\varepsilon$ (error term) is also N(0,1) (both are independent). 

I'll generate a sample of size 100, 100 times. For each sample, I'll fit a linear regression model and a loess model. Here are the resulting 100 regression curves for each (the dashed line is the true mean):


```{r, fig.width=7, fig.height=3}
set.seed(474)
n <- 100
N <- 100
xgrid <- data.frame(x=seq(-4,4, length.out=100))
## Use "tibble %>% group_by %>% do" in place of `for` loop
tibble(iter=1:N) %>% group_by(iter) %>% do({
    dat <- tibble(x=rnorm(n), 
                  y=x+rnorm(n))
    data.frame(
        .,
        xgrid,
        Local  = predict(loess(y~x, data=dat),
                         newdata=xgrid),
        Linear = predict(lm(y~x, data=dat),
                         newdata=xgrid)
    )
}) %>% 
    gather(key="method", value="Prediction", Local, Linear) %>% 
    ggplot(aes(x=x, y=Prediction)) +
    facet_wrap(~ method) +
    geom_line(aes(group=iter), colour="orange", alpha=0.1) +
    geom_abline(intercept=0, slope=1, linetype="dashed") +
    theme_bw()
```

Notice that the local method has higher variance than the linear regression method. 


## Splines and Loess Regression

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

This tutorial describes spline and loess regression in R.

You can think of splines as regression between knots. We'll use the `splines` package to do this.

Here's some generated data:

```{r}
library(ggplot2)
x <- rnorm(1000)
y <- x^2 + rnorm(1000)
qplot(x, y, alpha=I(0.5)) + theme_bw()
```

First, we need to "set up" the regression by placing knots.

```{r}
library(splines)
x2 <- ns(x, knots=c(-2, 0, 2))
```

Now we can do regression between these knots, with the natural spline shape (as opposed to linear):

```{r}
fit <- lm(y ~ x2)
qplot(x, y, alpha=I(0.5)) +
    geom_line(data=data.frame(x=x, y=predict(fit)), 
              mapping=aes(x, y), colour="blue") + 
    theme_bw()
```

### Loess

Loess is "local regression", and is based on the idea of estimating the mean response based on similar observed data in the predictor space.

__Kernel smoothing__ is a method that estimates the mean using a weighted sample average, where observations that are further away in the predictor space get downweighted more, according to some kernel function.

__Local polynomials__ is a method that takes kernel smoothing one step further: instead of a weighted sample average, we use nearby data (in the predictor space) to fit a polynomial regression, and then fit a polynomial regression model.

There is a more basic version, too: a __"moving window"__, described next, before seeing how loess is done in R.

#### The "Moving Window"

The "moving window" approach is a special type of kernel smoother. For a given value of the predictor $X=x$, the mean response is estimated as the sample average of all response values whose predictor values are "near" $x$ -- within some distance $h$.

For example, if you want to estimate the mean number of "runs" of a baseball team when walks=100 and hits=1000, only look at cases where "walks" is approximately 100, "hits" is approximately 1000, and then average the response.

It's a special case of kernel regression, with a kernel function of 
$$ k\left(t\right) = I\left(|t-x| < h \right), $$ 
sometimes called the "boxcar" function. 

__Note the similarity to kNN!__ kNN regression uses the nearest $k$ points, resulting in a variable distance $h$, whereas the moving window regression uses a fixed distance $h$, resulting in a variable number of points $k$ used.

#### `ggplot2`

`ggplot2` comes with a fairly powerful tool for plotting a smoother, with `geom_smooth`.

```{r}
qplot(x, y) +
    geom_smooth(method="loess") + 
    theme_bw()
```

You can choose the bandwidth through the `span` argument:

```{r}
qplot(x, y) +
    geom_smooth(method="loess", span=0.1) +
    theme_bw()
```

Too wiggly. The default looks fine.

Note that `geom_smooth` can fit `lm` and `glm` fits too:

```{r}
qplot(x, y) +
    geom_smooth(method="lm", formula=y~x+I(x^2)) + 
    theme_bw()
```

#### Manual method

You can use the `loess` or `ksmooth` function in R to do kernel smoothing yourself. You get more flexibility here, since you can get things such as predictions. It works just like the other regression functions:

```{r}
(fit <- loess(y ~ x))
```

We can make predictions as usual:

```{r}
yhat <- predict(fit)
qplot(x, y) +
    geom_line(data=data.frame(x=x, y=yhat), mapping=aes(x, y), 
              colour="blue") + 
    theme_bw()
```

If you want the standard errors of the predictions, you can indicate this with `se=TRUE` in the `predict` function. 

Some key things that you might want to change in your kernel smoothing regression, through arguments in the `loess` function:

- Bandwidth/smoothing parameter. Indicate through the `span` argument. 
- Degree of the local polynomial fitted. Indicate through the `degree` argument.
- Kernel function.

The kernel function is not readily specified in `loess`. But you can use the `ksmooth`, where you're allowed to specify a "box" or "normal" kernel.

<!--chapter:end:090-Estimating_assumption_free.Rmd-->

# Overfitting: The problem with adding too many parameters

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

(Reducible Error)

(BAIT 509 Class Meeting 02)


```{r, include=FALSE}
suppressPackageStartupMessages(library(tidyverse))
rotate_y <- theme(axis.title.y=element_text(angle=0, vjust=0.5))
```

## Classification Exercise: Do Together

Let's use the `Default` data in the `ISLR` R package for classification: predict whether or not a new individual will default on their credit card debt.

Our task: make the best prediction model we can.

1. Code the null model.
	- What would you predict for a new person?
	- What's the error using the original data (aka training data)?
2. Plot all the data, and come up with a classifier by eye.
	- What would you predict for a non-student with income $40,000 and balance $1,011?
	- Classify the original (training) data. What's the overall error?
3. How might we optimize the classifier? Let's code it.

Discussion: does it make sense to classify a new iris plant species based on the `iris` dataset?

## Training Error vs. Generalization Error

Discussion: 

1. What's wrong with calculating error on the training data? 
2. What's an alternative way to compute error?

Activity: Get the generalization error for the above classifier.

## Model complexity

A big part of machine learning is choosing how complex to make a model. Examples:

- More partitions in the above credit card default classifier
- Adding higher order polynomials in linear regression

And lots more that we'll see when we explore more machine learning models.

__The difficulty__: adding more complexity will decrease training error. What's a way of dealing with this?

### Activity

Let's return to the `iris` example from lecture 1, predicting Sepal Width, this time using Petal Width only, using polynomial linear regression.

__Task__: Choose an optimal polynomial order.

Here's code for a graph to visualize the model fit:

```
p <- 4
ggplot(iris, aes(Petal.Width, Sepal.Width)) +
	geom_point(alpha = 0.2) +
	geom_smooth(method = "lm", formula = y ~ poly(x, p)) +
	theme_bw()
```


## Reducible Error

### What is it?

Last time, we saw what irreducible error is, and how to "beat" it. 

The other type of error is __reducible error__, which arises from not knowing the true distribution of the data (or some aspect of it, such as the mean or mode). We therefore have to _estimate_ this. Error in this estimation is known as the reducible error. 

### Example

- one numeric predictor
- one numeric response
- true (unknown) distribution of the data is $Y|X=x \sim N(5/x, 1)$ (and take $X \sim 1+Exp(1)$). 
- you only see the following 100 observations stored in `dat`, plotted below, and choose to use linear regression as a model:

```{r}
set.seed(400)
n <- 100
dat <- tibble(
	x = rexp(n) + 1,
	y = rnorm(n, mean = 5/x)
)
ggplot(dat, aes(x, y)) +
    stat_smooth(method="lm", se=FALSE, size=0.5,
                mapping=aes(colour="Estimate")) +
    stat_function(fun=function(x) 5/x,
                  mapping=aes(colour="True mean")) +
    scale_colour_brewer("", palette="Dark2") +
    theme_bw() +
    rotate_y
```

The difference between the true curve and the estimated curve is due to reducible error. 

In the classification setting, a misidentification of the mode is due to reducible error. 

(__Why the toy data set instead of real ones?__ Because I can embed characteristics into the data for pedagogical reasons. You'll see real data at least in the assignments and final project.)

### Bias and Variance

There are two key aspects to reducible error: __bias__ and __variance__. They only make sense in light of the hypothetical situation of building a model/forecaster over and over again as we generate a new data set over and over again.

- __Bias__ occurs when your estimates are systematically different from the truth. For regression, this means that the estimated mean is either usually bigger or usually smaller than the true mean. For a classifier, it's the systematic tendency to choosing an incorrect mode.
- __Variance__ refers to the variability of your estimates.

There is usually (always?) a tradeoff between bias and variance. It's referred to as the __bias/variance tradeoff__, and we'll see examples of this later.  

Let's look at the above linear regression example again. I'll generate 100 data sets, and fit a linear regression for each:

```{r}
set.seed(400)
n <- 100
N <- 100
xgrid <- data.frame(x=seq(0,6, length.out=100)) + 1
## Use "tibble %>% group_by %>% do" in place of `for` loop
bias_plot <- tibble(iter=1:N) %>% group_by(iter) %>% do({
    dat <- tibble(x=rexp(n)+1, 
                  y=5/x+rnorm(n))
    data.frame(
        .,
        xgrid,
        Linear = predict(lm(y~x, data=dat),
                         newdata=xgrid)
    )
}) %>% 
    ggplot(aes(x=x, y=Linear)) +
    geom_line(aes(group=iter, colour="Estimates"), alpha=0.1) +
    stat_function(fun=function(x) 5/x,
                  mapping=aes(colour="True mean")) +
    theme_bw() +
    scale_colour_brewer("", palette="Dark2") +
    ylab("y") + rotate_y
bias_plot
```

The _spread_ of the linear regression estimates is the variance; the difference between the _center of the regression lines_ and the true mean curve is the bias. 

### Reducing reducible error

As the name suggests, we can reduce reducible error. Exactly how depends on the machine learning method, but in general:

- We can reduce variance by increasing the sample size, and adding more model assumptions.
- We can reduce bias by being less strict with model assumptions, OR by specifying them to be closer to the truth (which we never know).

Consider the above regression example again. Notice how my estimates tighten up when they're based on a larger sample size (1000 here, instead of 100):

```{r}
set.seed(400)
n <- 1000
N <- 100
xgrid <- data.frame(x=seq(0,6, length.out=100)) + 1
## Use "tibble %>% group_by %>% do" in place of `for` loop
tibble(iter=1:N) %>% group_by(iter) %>% do({
    dat <- tibble(x=rexp(n)+1, 
                  y=5/x+rnorm(n))
    data.frame(
        .,
        xgrid,
        Linear = predict(lm(y~x, data=dat),
                         newdata=xgrid)
    )
}) %>% 
    ggplot(aes(x=x, y=Linear)) +
    geom_line(aes(group=iter, colour="Estimates"), alpha=0.1) +
    stat_function(fun=function(x) 5/x,
                  mapping=aes(colour="True mean")) +
    theme_bw() +
    scale_colour_brewer("", palette="Dark2") +
    ylab("y") + rotate_y
```

Notice how, after fitting the linear regression $E(Y|X=x)=\beta_0 + \beta_1 (1/x)$ (which is a _correct_ model assumption), the regression estimates are centered around the truth -- that is, they are unbiased:

```{r}
set.seed(400)
n <- 100
N <- 100
xgrid <- data.frame(xinv=(seq(0,6, length.out=100))) + 1
## Use "tibble %>% group_by %>% do" in place of `for` loop
tibble(iter=1:N) %>% group_by(iter) %>% do({
    dat <- tibble(x=rexp(n)+1, 
                  xinv=1/x,
                  y=5/x+rnorm(n))
    data.frame(
        .,
        xgrid,
        Linear = predict(lm(y~xinv, data=dat),
                         newdata=xgrid)
    )
}) %>% 
    ggplot(aes(x=1/xinv, y=Linear)) +
    geom_line(aes(group=iter, colour="Estimates"), alpha=0.1) +
    stat_function(fun=function(x) 5/x,
                  mapping=aes(colour="True mean")) +
    theme_bw() +
    scale_colour_brewer("", palette="Dark2") +
    ylab("y") + rotate_y +
    xlab("x")
```


### Error decomposition

We saw that we measure error using mean squared error (MSE) in the case of regression, and the error rate in the case of a classifier. These both contain all errors: irreducible error, bias, and variance:

MSE = bias^2 + variance + irreducible variance

A similar decomposition for error rate exists.

__Note__: If you look online, the MSE is often defined as the expected squared difference between a parameter and its estimate, in which case the "irreducible error" is not present. We're taking MSE to be the expected squared distance between a true "new" observation and our prediction (mean estimate). 

## Model Selection

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

(BAIT 509 Class Meeting 04)

```{r}
suppressPackageStartupMessages(library(tidyverse))
my_accent <- "#d95f02"
rotate_y <- theme(axis.title.y=element_text(angle=0, vjust=0.5))
```

### Exercise: CV 

k-fold cross validation with `caret::train()` in R. See [this resource](https://machinelearningmastery.com/how-to-estimate-model-accuracy-in-r-using-the-caret-package) 

In python, can use [`sklearn.model_selection.cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score).

### Out-of-sample Error

#### The fundamental problem

First, some terminology: The data that we use to fit a model is called __training data__, and the fitting procedure is called __training__. New data (or at least, data _not_ used in the training process) is called __test data__.

The goal of supervised learning is to build a model that has low error on _new_ (test) data.

\*\*\* A fundamental fact of supervised learning is that the error on the training data will (on average) be __better__ (lower) than the error on new data!

More terminology: __training error__ and __test error__ are errors computed on the respective data sets. Often, the test error is called __generalization error__. 

Let's check using loess on an artificial data set (from last time). Here's the training error (MSE):

```{r}
set.seed(87)
n <- 200
dat <- tibble(x = c(rnorm(n/2), rnorm(n/2)+5)-3,
              y = sin(x^2/5)/x + rnorm(n)/10 + exp(1))
fit <- loess(y ~ x, data=dat, span=0.3)
yhat <- predict(fit)
mean((yhat - dat$y)^2)
```

Here's the test error:

```{r}
n <- 1000
newdat <- tibble(x = c(rnorm(n/2), rnorm(n/2)+5)-3,
                 y = sin(x^2/5)/x + rnorm(n)/10 + exp(1))
yhat <- predict(fit, newdata = newdat)
mean((yhat - newdat$y)^2, na.rm = TRUE)
```

If you think this was due to luck, go ahead and try changing the seed -- more often than not, you'll see the test error > training error. 

This fundamental problem exists because, by definition, we build the model to be optimal based on the training data! For example, kNN and loess make a prediction that's _as close as possible_ to the training data. 

The more we try to make the model fit the training data -- i.e., the more we overfit the data -- the worse the problem gets. Let's reduce the loess bandwidth to emulate this effect. Here's the training error:

```{r}
set.seed(87)
n <- 200
dat <- tibble(x = c(rnorm(n/2), rnorm(n/2)+5)-3,
              y = sin(x^2/5)/x + rnorm(n)/10 + exp(1))
fit <- loess(y ~ x, data=dat, span=0.1)
yhat <- predict(fit)
mean((yhat - dat$y)^2)
```

Test error:

```{r}
n <- 1000
newdat <- tibble(x = c(rnorm(n/2), rnorm(n/2)+5)-3,
                 y = sin(x^2/5)/x + rnorm(n)/10 + exp(1))
yhat <- predict(fit, newdata = newdat)
mean((yhat - newdat$y)^2, na.rm = TRUE)
```

The effect gets even worse if we have less training data. 

For kNN and loess, we can play with the hyperparameter, weight function, and degree of local polynomial (in the case of regression) to try and avoid overfitting. Playing with these things is often called __tuning__. 

#### Solution 1: Use a hold-out set.

One solution is to split the data into two parts: __training__ and __validation__ data. The validation set is called a _hold-out set_, because we're holding it out in the model training. 

Then, we can tune the model (such as choosing the $k$ in kNN or $r$ in loess) to minimize error _on the validation set_.

```{r}
set.seed(87)
n <- 200
dat <- tibble(x = c(rnorm(n/2), rnorm(n/2)+5)-3,
              y = sin(x^2/5)/x + rnorm(n)/10 + exp(1))
n <- 1000
newdat <- tibble(x = c(rnorm(n/2), rnorm(n/2)+5)-3,
                 y = sin(x^2/5)/x + rnorm(n)/10 + exp(1))
tibble(r = seq(0.05, 0.7, length.out=100)) %>% 
    group_by(r) %>% 
    do({
        this_r <- .$r
        fit <- loess(y ~ x, data=dat, span=this_r)
        yhat_tr  <- predict(fit)
        yhat_val <- predict(fit, newdata = newdat)
        data.frame(
            r = this_r,
            training = mean((yhat_tr - dat$y)^2),
            validation = mean((yhat_val - newdat$y)^2, na.rm = TRUE)
        )
    }) %>% 
    gather(key="set", value="mse", training, validation) %>% 
    ggplot(aes(r, mse)) +
    geom_line(aes(group=set, colour=set)) +
    theme_bw()
```

We would choose a bandwidth ($r$) of approximately 0.35, because the error on the validation set is smallest. 

Notice from this plot:

- The training error is lower than the out-of-sample error.
- We can make the training error arbitrarily small by decreasing $r$. 
- The out-of-sample error decreases, and then starts to increase again.
    - NOTE: This doesn't _always_ happen, as you'll see in Assignment 1. But it usually does. 

After choosing the model that gives the smallest error on the validation set, then the _validation error_ is also going to be on average lower than in a test set -- that is, if we get even more data! The more tuning parameters we optimize using a validation set, the more pronounced this effect will be. Two things to note from this:

1. This is not as bad as the original problem (where the training error is less than the test error), because the tuning parameters are still chosen on an out-of-sample set.
2. If we want to use the validation error as an estimate of the out-of-sample error, we just have to be mindful of the fact that this is an optimistic estimate of the generalization error.

If you wanted an unbiased estimate of generalization error, you can start your procedure by splitting your data into three sets: training and validation as before, but also a test set that is __never touched until you've claimed a final model__! You only use the test set to get an unbiased estimate of generalization error. 

There's not really a standard choice for deciding _how much_ data to put aside for each set, but something like 60% training, 20% validation, and 20% test is generally acceptable.

#### Solution 2: Cross-validation

The problem with the training-validation-test set approach is that you're wasting a lot of data -- lots of data are not being used in training! Another problem is that it's not easy to choose how much data to put aside for each set.

A solution is to use ($c$-fold) __cross validation__ (CV), which can be used to estimate out-of-sample error, and to choose tuning parameters. (Note that usually people refer to this as $k$-fold cross validation, but I don't want to overload $k$ from kNN!) $c=10$ is generally accepted as the defacto standard. Taking $c$ equal to the sample size is a special case called leave-one-out cross validation. 

The general procedure is as follows:

1. Partition the data into $c$ (approximately equal) chunks.
2. Hold out chunk 1; train the model on the other $c-1$ chunks; calculate error on the held-out chunk.
3. Hold out chunk 2; train the model on the other $c-1$ chunks; calculate error on the held-out chunk.
4. Hold out chunk 3; train the model on the other $c-1$ chunks; calculate error on the held-out chunk.
5. etc., until you've held out each chunk exactly once.
6. Average the $c$ errors to get an estimate of the generalization error.

You can then repeat this procedure for different values of the tuning parameters, choosing values that give the lowest error. Once you choose this tuning parameter, go ahead and use _all_ the data as training data, with the selected tuning parameters. 

CV is generally preferred to the hold-out set method, because we can fit a model that has overall lower error, but it's computationally expensive. 



### Alternative measures of model goodness

The coefficient of determination ($R^2$) can be calculated whenever it makes sense to calculate MSE. It equals:
$$ R^2 = 1 - \frac{\text{MSE of your model}}{\text{MSE of the model that always predicts } \bar{y}}. $$ This number lies between 0 and 1, where a 1 represents perfect prediction on the set that you're computing $R^2$ with.

When we have a distributional assumption (such as Gaussian errors), we can calculate the likelihood -- or more often, the negative log likelihood ($\ell$). If the density/mass function of $y_i$ is $f_i$, and we have $n$ observations, then the negative log likelihood is
$$ \ell = -\sum_{i=1}^{n} \log(f_i(y_i)). $$

### Feature and model selection: setup

For supervised learning, we seek a model that gives us the lowest generalization error as possible. This involves two aspects:

1. Reduce the irreducible error.
    - This involves __feature engineering__ and __feature selection__: finding and choosing predictors that give us as much information about the response as we can get.
2. Reduce the reducible error (= bias & variance)
    - This involves __modelling__ and __tuning__, so that we can extract the information that the predictors hold about the response as best as we can. The better our model, the lower our reducible error is.
    - This has been the main focus of BAIT 509, via models such as loess, kNN, random forests, SVM, etc.

Recall for (2) that we avoid overfitting by tuning (choosing hyperparameters, such as $k$ in kNN) to optimize generalization error. We estimate generalization error either using the validation set approach, cross validation, or the out-of-bag approach for bagging. 

The same thing applies to choosing features/predictors and choosing models, although model selection has a few extra components that should be considered.

### Model selection

The question here is, what supervised learning method should you use? There are a few things you should consider.

1. Quantitative choice

Suppose you've gone ahead and fit your best random forest model, kNN model, linear regression model, etc. Which do you choose? You should have estimated the generalization error for each model (for example, on a validation set) -- choose the one that gives the lowest error.

You might find that some models have roughly the same error. In this case, feel free to use all of these to make predictions. You can either look at all predictions, or take an average of the model outputs (called __model averaging__). Considering all models may be quite informative, though -- for example, if all models are suggesting the same thing for a new case, then the decision is clearer than if they all say different things. 

2. Qualitative choice

Sometimes, after exploring the data, it makes sense to add model assumptions. For example, perhaps your response looks linear in your predictors. If so, it may be reasonable to assume linearity, and fit a linear regression model. 

Note that adding assumptions like this generally reduce the variance in your model fit -- but is prone to bias if the assumption is far from the truth. As usual, adding assumptions is about reducing the bias-variance tradeoff.

3. Human choice (interpretability)

Sometimes it's helpful for a model to be interpretable. For example, the slopes in linear regression hold meaning; odds ratios in logistic regression hold meaning; nodes in a decision tree have meaning. If this is the case, then interpretability should also be considered.

### Feature (predictor) selection

Recall that, when tuning a supervised learning method (such as choosing $k$ in kNN), we can make the training error arbitrarily small -- but this results in overfitting the training data. The same thing applies to the number of predictors you add. 

Here's an example. I'll generate 100 observations of 1 response and 99 predictor variables totally randomly, fit a linear regression model with all the predictors, and calculate MSE:

```{r}
set.seed(38)
dat <- as.data.frame(matrix(rnorm(100*100), ncol=100))
names(dat)[1] <- "y"
fit <- lm(y~., data=dat)
mean((dat$y - predict(fit))^2)
```

The MSE is 0 (up to computational precision) -- the response is perfectly predicted on the training set. 

If we consider the number of predictors as a tuning parameter, then we can optimize this by estimating generalization error, as usual. 

But there are approaches that we can use that's specific to feature selection, that we'll discuss next. You are not expected to apply these for your project! This is just for your information.

#### Specialized metrics for feature selection

_You are not required to use this method for your project._

Using these specialized metrics, we don't need to bother holding out data to estimate generalization error: they have a penalty built into them based on the number of predictors that the model uses.  

- The __adjusted $R^2$__ is a modified version of $R^2$.
- The __AIC__ and __BIC__ are modified versions of the negative log likelihood.

There are others, like Mallows' $C_p$. 

Optimize these on the training data -- they're designed to (try to) prevent overfitting. 

But, with $p$ predictors, we'd have $2^p$ models to calculate these statistics for! That's _a lot_ of models when $p$ is not even all that large. Here's how the number of models grows as $p$ increases:

```{r, echo=FALSE, fig.width=5, fig.height=3, fig.align="center"}
ggplot(data.frame(p=c(0,10)), aes(p)) +
    stat_function(fun=function(x) 2^x) +
    theme_bw() +
    ylab(expression(2^p)) +
    theme(axis.title.y = element_text(angle=0, vjust=0.5))
```

For 10 predictors, that's 1000 models. For 20 predictors, that's over 1,000,000.



#### Greedy Selection

_You are not required to use this method for your project._

Instead of fitting all models, we can take a "greedy approach". This may not result in the optimal model, but the hope is that we get close. One of three methods are typically used:

1. Forward Selection

The idea here is to start with the null model: no predictors. Then, add one predictor at a time, each time choosing the best one in terms of generalization error OR in terms of one of the specialized measures discussed above. Sometimes, a hypothesis test is used to determine whether the addition of the predictor is significant enough.

2. Backward Selection

The idea here is to start with the full model: all predictors. Then, gradually remove predictors that are either insignificant according to a hypothesis test, or that gives the greatest reduction in one of the specialized measures discussed above. 

3. Stepwise Selection

The idea here is to combine forward and backward selection. Instead of only adding or only removing predictors, we can consider either at each iteration: adding or removing. 

#### Regularization

_You are not required to use this method for your project._

When training a model, we can write the training procedure as the optimization of a __loss function__. For example, in regression, we want to minimize the sum of squared errors. 

__Regularization__ adds a penalty directly to this loss function, that grows as the number of predictors grows. This is in contrast to the specialized measures (like adjusted $R^2$) that adds the penalty to the error term _after_ the model is fit. There are different types of regularization, but typically those that involve an L1 regularizer are used in feature selection. 


<!--chapter:end:100-The_problem_with_adding_too_many_parameters_.Rmd-->

# Describing Relationships {-}

# There's meaning in parameters

## The types of parametric assumptions

A model can be _parametric_ (i.e., containing parameters) in roughly two ways:

### 1\. When defining a __model function__.

For example, in linear regression, we assume that the model function is linear. We might also make assumptions about the conditional variance.

This tends to be the meaning of "parametric" in Computer Science.

### 2\. When defining __probability distributions__. 

For example, we might assume that residuals are Gaussian, or perhaps some other distribution.

This tends to be the meaning of "parametric" in Statistics.

## The value of making parametric assumptions

There are arguably two reasons one might bother making a parametric assumption. They are:

1. Reduced error.
2. Interpretability.

### Value \#1: Reduced Error

One value of making parametric assumptions is that we _might_ achieve reduced error. As long as we don't introduce as many parameters as there are observations (or more), here's what generally happens when we make an assumption:

1. __The model variance decreases__. You can think of the reason behind this in two ways:
    - we're adding information to our data set; or
    - we don't need to estimate as many quantities. 
2. __The bias increases the "more incorrect" your assumption is__.
    - This is because we're identifying a framework that is almost surely not true.

Recall that mean squared error has both (squared) bias and model variance as components. The hope is that your model is "correct enough" so that the increase in bias is small in comparison to the decrease in variance, resulting in an overall decrease in error.

Challenge: run a simulation to convince yourself of this -- first in the univariate setting (where bias and variance are more interpretable), then in the regression setting.

For more information on the bias-variance tradeoff, check out Section 2.2.2 of the [ISLR book](http://www-bcf.usc.edu/~gareth/ISL/).

### Value \#2: Interpretation

Sometimes making a parametric assumption does not reduce the overall error by much, even when the assumption is true. This would not be appealing if all you care about is prediction performance. But if you want to gain some insight into relationships between your predictors and response, then introducing a parameter __so that it has meaning__ will help with this task.

For example, assuming the mean response is linear in the predictors gives us meaning behind the slope parameter, as it corresponds to the expected change in response associated with a difference of 1 unit of the corresponding predictor.  

The bonus here is that we don't always need to think of the parameters as being strictly correct. Your assumptions will never hold exactly, so as long as the assumption is not completely unreasonable, the parameters at least give you a sense of what's going on in your data. 



## ANOVA

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

(From 2018-2019 DSCI 561 lab1)

Remember that to make a hypothesis test, we need to first come up with some "distance metric" (called the test statistic) to measure discrepency from the null hypothesis. For example, to test whether two groups have different population means, a t-test bases its distance metric / test statistic on the difference between the two sample means -- the further this metric is from 0, the more evidence we have that the null hypothesis is not true.

There's another way to measure discrepency from the null hypothesis that all group means are equal -- a way that allows for any number of groups to be compared all at once (not just two). But, it operates under the assumption that the variance of the data in each group is the same. Here's the idea. If the population means of each group truly are the same, then we can estimate that common variance in two ways: using the "overall" variance (ignoring the groups altogether), OR by averaging the variances of each group. The distance metric / test statistic is based off of the ratio of these two values (which is more meaningful than looking at the difference between the two).

This is ANOVA -- ANalysis Of VAriance.

For the pregnancy (two-group) dataset, calculate:

1. the overall variance of the response, and store it in var_tot;
2. the average of the variances for each group, and store it in var_grp; and
3. how many times larger is (1) compared to (2)? i.e., calculate (1)/(2). Store it in my_ratio.

Take a moment to reflect as to whether you think this is a big difference or not (no need to write anything).

The actual test statistic of ANOVA has minor adjustments to the ratio you just calculated, although is based on the same concepts.

- Since (1) >= (2), the ratio will always be >1. So instead, the numerator is based on the difference between (1) and (2) (this results in a variance sometimes called the "treatment variance").
- The variance estimates don't always use n-1 -- they adjust for the number of groups, based on the concept of "degrees of freedom".

Still, the larger the ratio (test statistic), the less evidence we have to support the null hypothesis.

Your task: run the ANOVA in R using the aov() function for the pregnancy data. Quality code uses broom::tidy(). Then, store the p-value in preg_aov_p, the test statistic in preg_aov_F, and the "Species" and "Residuals" degrees of freedom (df) as a length-2 vector in the variable preg_aov_df.

If the null hypothesis is true, then the sampling distribution of this test statistic is (to a good approximation) a specific F-distribution. This "good approximation" is thanks to the CLT, and is true as long as the sample size isn't small (and it's always true if the data are Gaussian).

<!--chapter:end:110-Theres_meaning_in_parameters.Rmd-->

# The meaning of interaction

Interaction terms, and when relationships change given a variable.

<!--chapter:end:120-The_meaning_of_interaction.Rmd-->

# Scales and the restricted range problem

**Caution: in a highly developmental stage! See Section  \@ref(caution).**


link functions and alternative parameter interpretations (categorical data too)

In Regression I, the response was allowed to take on any real number. But what if the range is restricted?

## Problems

Here are some common examples.

1. Positive values: river flow. 
    - Lower limit: 0
2. Percent/proportion data: proportion of income spent on housing in Vancouver. 
    - Lower limit: 0
    - Upper limit: 1. 
3. Binary data: success/failure data.
    - Only take values of 0 and 1.
4. Count data: number of male crabs nearby a nesting female
    - Only take count values (0, 1, 2, ...)

Here is an example of the fat content of a cow's milk, which was recorded over time. Data are from the paper ["Transform or Link?"](https://core.ac.uk/download/pdf/79036775.pdf). Let's consider data as of week 10:

```{r}
(plot_cow <- cow %>% 
    filter(week >= 10) %>% 
    ggplot(aes(week, fat*100)) +
    geom_point() +
    theme_bw() +
    labs(y = "Fat Content (%)") +
    ggtitle("Fat content of cow milk"))
```

Let's try fitting a linear regression model. 

```{r}
plot_cow +
    geom_smooth(method = "lm", se = FALSE)
```

Notice the problem here -- __the regression lines extend beyond the possible range of the response__. This is _mathematically incorrect_, since the expected value cannot extend outside of the range of Y. But what are the _practical_ consequences of this?

In practice, when fitting a linear regression model when the range of the response is restricted, we lose hope for extrapolation, as we obtain logical fallacies if we do. In this example, a cow is expected to produce _negative_ fat content after week 35!

Despite this, a linear regression model might still be useful in these settings. After all, the linear trend looks good for the range of the data. 


## Solutions

How can we fit a regression curve to stay within the bounds of the data, while still retaining the interpretability that we have with a linear model function? Remember, non-parametric methods like random forests or loess will not give us interpretation. Here are some options:

1. Transform the data. 
2. Transform the linear model function: link functions
3. Use a scientifically-backed parametric function.

### Solution 1: Transformations

One solution that _might_ be possible is to transform the response so that its range is no longer restricted. 
The most typical example is for positive data, like river flow. If we log-transform the response, then the new response can be any real number. All we have to do is fit a linear regression model to this transformed data.

One downfall is that we lose interpretability, since we are estimating the mean of $\log(Y)$ (or some other transformation) given the predictors, not $Y$ itself! Transforming the model function by exponentiating will not fix this problem, either, since the exponential of an expectation is not the expectation of an exponential. Though, this is a mathematical technicality, and might still be a decent approximation in practice.

Also, transforming the response might not be fruitful. For example, consider a binary response. No transformation can spread the two values to be non-binary!

### Solution 2: Link Functions

Instead of transforming the data, why not transform the model function? For example, instead of taking the logarithm of the response, perhaps fit the model $$ E(Y|X=x) = \exp(\beta_0 + \beta x) = \alpha \exp(\beta x) $$. Or, in general, $$ g(E(Y|X=x)) = X^T \beta $$ for some increasing function $g$ called the _link function_. 

This has the added advantage that we do not need to be able to transform the response.

Two common examples of link functions:

- $\log$, for positive response values.
    - Parameter interpretation: an increase of one unit in the predictor is associated with an $\exp(\beta)$ times increase in the mean response, where $\beta$ is the slope parameter.
- $\text{logit}(x)=\log(x/(1-x))$, for binary response values.
    - Parameter interpretation: an increase of one unit in the predictor is associated with an $\exp(\beta)$ times increase in the odds of "success", where $\beta$ is the slope parameter, and odds is the ratio of success to failure probabilities.

### Solution 3: Scientifically-backed functions

Sometimes there are theoretically derived formulas for the relationship between response and predictors, which have parameters that carry some meaning to them.

## GLM's in R

This document introduces the `glm()` function in R for fitting a Generlized Linear Model (GLM). We'll work with the `titanic_train` dataset in the `titanic` package.

```{r}
library(tidyverse)
library(broom)
library(titanic)
dat <- na.omit(titanic_train)
str(dat)
```


Consider the regression of `Survived` on `Age`. Let's take a look at the data with jitter:

```{r, fig.height=2, warning=FALSE}
ggplot(dat, aes(Age, Survived)) +
    geom_jitter(height=0.1, alpha=0.25) +
    scale_y_continuous(breaks=0:1, labels=c("Perished", "Survived")) +
    theme_bw()
```

Recall that the linear regression can be done with the `lm` function:

```{r}
res_lm <- lm(Survived ~ Age, data=dat)
summary(res_lm)
```

In this case, the regression line is ```r res_lm$coefficients[1]``` + ```r res_lm$coefficients[2]``` `Age`.

A GLM can be fit in a similar way, using the `glm` function -- we just need to indicate what type of regression we're doing (binomial? poission?) and the link function. We are doing bernoulli (binomial) regression, since the response is binary (0 or 1); lets choose a `probit` link function.

```{r}
res_glm <- glm(factor(Survived) ~ Age, data=dat, family=binomial(link="probit"))
```

The `family` argument takes a __function__, indicating the type of regression. See `?family` for the various types of regression allowed by `glm()`. 

Let's see a summary of the GLM regression:

```{r}
summary(res_glm)
```

We can make predictions too, but this is not as straight-forward as in `lm()` -- here are the "predictions" using the `predict()` generic function:

```{r}
pred <- predict(res_glm)
qplot(dat$Age, pred) + labs(x="Age", y="Default Predictions")
```

Why the negative predictions? It turns out this is just the linear predictor, ```r res_glm$coefficients[1]``` + ```r res_glm$coefficients[2]``` `Age`.

The documentation for the `predict()` generic function on `glm` objects can be found by typing `?predict.glm`. Notice that the `predict()` generic function allows you to specify the *type* of predictions to be made. To make predictions on the mean (probability of `Survived=1`), indicate `type="response"`, which is the equivalent of applying the inverse link function to the linear predictor.

Here are those predictions again, this time indicating `type="response"`:

```{r}
pred <- predict(res_glm, type="response")
qplot(dat$Age, pred) + labs(x="Age", y="Mean Estimates")
```

Look closely -- these predictions don't actually fall on a straight line. They follow an inverse probit function (i.e., a Gaussian cdf):

```{r}
mu <- function(x) pnorm(res_glm$coefficients[1] + res_glm$coefficients[2] * x)
qplot(dat$Age, pred) + 
    labs(x="Age", y="Mean Estimates") +
    stat_function(fun=mu, colour="blue") +
    scale_x_continuous(limits=c(-200, 200))
```

### `broom::augment()`

We can use the `broom` package on `glm` objects, too. But, just like we had to specify `type="response"` when using the `predict()` function in order to evaluate the model function, so to do we have to specify something in the `broom::augment()` function. Here, the `type.predict` argument gets passed to the `predict()` generic function (actually, the `predict.glm()` method). This means that indicating `type.predict="response"` will evaluate the model function:

```{r}
res_glm %>% 
  augment(type.predict = "response") %>% 
  head()
```


## Options for Logistic Regression


Some popular
interpretable quantities (IQ's) which compare exposure risk $\pi_{E}$ to
baseline (unexposed) risk $\pi_{B}$ are

1.  the *risk difference*, $\pi_{E}-\pi_{B}$,

2.  the reciprocal risk difference, or *number needed to treat* (NNT)
    (or sometimes *number needed to harm*),

3.  the *relative risk*, $\pi_{E}/\pi_{B}$, and

4.  the *odds ratio*,

These IQ's consider all other factors to be equal.


### Models

A first inclination may be to model the mean as one would in the case of
multiple linear regression that is, as a linear combination of the
covariates. The link function $g$ is the identity, and the model becomes (EQUATION).
Kovalchik and others (2013) refer to this as the "Binomial Linear
Model", or BLM, though it is more commonly known as the "Linear
Probability Model", or LPM (see, for example, Aldrich and Nelson, 1984;
Amemiya, 1977; Horrace and Oaxaca, 2006). In this paper, the model is
referred to as the LPM.

Before proceeding with any further discussion, the validity of this
model must be enforced. The Bernoulli distribution requires
$0\leq\pi(X)\leq1$ for all $x$
$\boldsymbol{x}\in XX$ to be a valid distribution. Validity
can be ensured by restricting the parameter space of
$\left(\beta_{0},\boldsymbol{\beta}\right)$ to .
However, the parameter space can be severely restricted depending on the
covariate space. For example, if predictor $k$ of
is unbounded, then the only allowable value for
$\beta_{k}$ is zero. In other words, any covariate in the LPM that has
an unbounded range cannot technically be included in the LPM. Further,
even if component $k$ is bounded, if it has a large
range, then the slope is restricted to be small.

One reason why the LPM is used, despite the above restrictions, is for
access to *constant interpretable quantities* that is, IQ's discussed in
section 1 which do not depend on other covariates. In an LPM, the risk
difference by increasing $X_{k}$ by one unit is simply given by
$\beta_{k}$, and the NNT is $1/\beta_{k}$. However, the relative risk
and odds ratio are non-constant, as they are functions of the other
covariates.

Since the LPM is just a multiple linear regression model, the regression
parameters can be estimated without bias by ordinary least squares
(OLS). However, we do not necessarily have homoskedastic errors, since $Var(Y|X=x)$
differs with the covariates. As such, the efficiency of the OLS
estimator can be improved by the weighted least squares estimator with
weights $1/\sigma\left(\boldsymbol{x}\right)$. Since these weights are
unknown, an iterative algorithm is used, which calculates weights using
the fitted probabilities
from parameter estimates of the previous step to compute a new
"re-weighted" estimator. Iterating this beginning with the OLS estimator
converges to the *iteratively re-weighted least squares* (IRLS)
estimator. Amemiya (1977) shows that the IRLS is identical to the
maximum likelihood estimator (MLE).

An alternative model which is sometimes confused for the LPM (for
example, see Horrace and Oaxaca, 2006) is to allow for an arbitrary
parameter space by taking
$\pi(x)$ to be zero when $\eta$ is less than
zero, and unity otherwise. This model, which I call the "truncated LPM"
(TLPM), is (EQUATION)
where the inverse-link function $T$ is the ramp function (actually, $T$ is not
quite an inverse-link function because it is non-invertible, but this is
unimportant). However, as one can see by the differing link function,
this is not the LPM, although it is often mistaken for the LPM. Horrace
and Oaxaca (2006) mistake the TLPM for the LPM, and in doing so, show
that estimation of the model parameters through OLS or IRLS provide
biased and inconsistent estimators. This is a good reason why the TLPM
should not be used unless a different method of estimation is
considered.

To rid the parameter space of restrictions, one may
consider link functions similar to the ramp function (preferably smooth) to ensure
$0\leq\pi(x)\leq1$. Popular choices are logit, probit, the inverse Gumbel distribution function, or the angular function (Cox and Snell, 1989). Each of these
link functions ensures a valid probability for an arbitrary parameter
space. The logit
link function is a popular choice because it has the best
interpretability. It models the log-odds as a linear function of the
covariates that is, (EQUATION).
This model is known as the logistic regression model, and can be written
equivalently as ,
where 
is the inverse logit function. The logistic model
stands out over models with other link functions because a constant IQ
can be obtained from it the odds ratio by increasing $X_{k}$ by one unit
is simply $\exp\left(\beta_{k}\right)$. However, of the interpretable
quantities discussed in Section 1, the odds ratio is the most difficult
to interpret. Though, if both risks are smallthe "rare disease
assumption" with risks under $0.1$ then the odds ratio is a good
approximation to the relative risk, which is easier to interpret. The
lack of an easy interpretable constant IQ is why some researchers will
opt for the LPM instead of the logistic model when the rare disease
assumption is invalid. Indeed, this is one major reason behind the study
done by Kovalchik and others (2013). One other method to decide is
through Goodness of Fit criteria, which was the other deciding factor of
Kovalchik and others.

Conveniently, the log odds appears in the likelihood of the logistic
model, which simplifies some computations. This leads to the MLE which
solves the equation .
However, occasionally it is possible that no MLE exists when there is a
$\left(b_{0},\boldsymbol{b}\right)\in\mathcal{F}$ such that
$b_{0}+\boldsymbol{x}_{i}^{T}\boldsymbol{b}>0$ has $Y_{i}=1$ and
$b_{0}+\boldsymbol{x}_{i}^{T}\boldsymbol{b}<0$ has $Y_{i}=0$ for each
$i=1,\ldots,n$ (Albert and Anderson, 1984). This is called the case of
"complete separation", and the likelihood has no maximum, so a "perfect
fit" is made by infinitely pushing the covariate data to the tails of
the expit curve. This is not an issue with the LPM
model.


<!--chapter:end:130-Scales_and_the_restricted_range_problem.Rmd-->

# Improving estimation through distributional assumptions

<!--chapter:end:140-Improving_estimation_through_distributional_assumptions.Rmd-->

# When we only want interpretation on some predictors

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

## Non-identifiability in GAMS

Here is some help on Lab 4 Exercise 1(b). Exercise 1(b) is intended to get you to think about what the $h$ functions in a Generalized Additive Model (GAM) are.

An interpretation of the $h$ functions can only make sense in light of the *non-identifiability* issue of GAM's, so that's discussed first. Then, hints are given for the first two questions in Exercise 1(b). 

### Non-identifiability

What is "non-identifiability", exactly? It can happen for any model that's not carefully specified (not just GAM's). Let's look at an example first.

In simple linear regression, why not write the model
$$ Y = \beta_0 + \alpha_0 + \beta_1 X + \varepsilon, $$
where $\mathbb{E}(\varepsilon)=0$? It's because three parameters are too many to describe a line. In other words, \textbf{more than one parameter selection can give you the same regression line}. For example, the model $Y=1+X+\varepsilon$ can be written with
$$ \beta_0 = 0, \alpha_0 = 1, \beta_1 = 1, $$
or
$$ \beta_0 = -1, \alpha_0 = 2, \beta_1 = 1, $$
etc. In fact, as long as $\alpha_0 = 1 - \beta_0$, and $\beta_1=1$, we get the same regression line. 

In general, and roughly speaking, when more than one parameter selection gives you the same model, there's a non-identifiability issue. It leads to problems in estimation and estimator properties. It also leads to an \emph{interpretation} problem: the parameters don't have a meaning, since they can represent more than one thing in the model.

This is even true in non-parametric cases, such as the GAM. Let's look at a two-predictor GAM:
$$ Y = \beta_0 + h_1\left(X_1\right) + h_2\left(X_2\right) + \varepsilon, $$
where $\beta_0$ is any real number, $h_1$ and $h_2$ are any smooth functions, and $\mathbb{E}(\varepsilon)=0$. As it is, this model is non-identifiable: if you pick a $\beta_0$, $h_1$, and $h_2$, I can find another set of $\beta_0$, $h_1$, and $h_2$ that gives the same regression surface. How? I can just add a constant $c$ to your $\beta_0$, and subtract that constant from your, say, $h_1$ (i.e., "vertically shift" your $h_1$ function downwards by $c$). 

So, the "parameters" (which includes the $h$ functions) in a GAM are non-identifiable -- the $h$ functions can be vertically shifted, and $\beta_0$ can just compensate for these shifts to give the same regression surface.

To make the model identifiable, we force the $h$ functions to be vertically centered at zero. Here's how: we ensure that after transforming the $j$'th predictor to $h_j\left(X_j\right)$, the resulting data are centered at 0. Mathematically, we ensure that
$$ \frac{1}{n}\sum_{i=1}^{n}h_j\left(x_{ij}\right) = 0 $$
for each predictor $j$, where $x_{ij}$ for $i=1,\ldots,n$ are the observations. 

### Question 1b

__Notation__: Let's call $\hat{\beta}_0$ the estimate of $\beta_0$, and the the functions $\hat{h}_1$ and $\hat{h}_2$ the estimates of $h_1$ and $h_2$, respectively.

The prediction on observation $i$, denoted $\hat{Y}_i$, is
$$ \hat{Y}_i = \hat{\beta}_0 + \hat{h}_1\left(x_{i1}\right) + \hat{h}_1\left(x_{i2}\right). $$
This will help with the first question:

> Suppose the `gam` fit is called `fit`. Why is `mean(predict(fit))` the same as the estimate of the intercept?


Here's a hint: `predict(fit)` gives you the vector $\hat{Y}_1, \ldots, \hat{Y}_n$. Then, `mean` averages them. The question is asking you to indicate why we have
$$ \frac{1}{n}\sum_{i=1}^{n}\hat{Y}_i = \hat{\beta}_0. $$
The answer uses Equation

The next question asks you to think about how you'd recover an $h$ function. It asks:

> For each $h$ function, write an R function that evaluates the $h$ function over a grid of values, without calling the `plot` function on the fit. Show that the function works by evaluating it over a small grid of values.

Suppose you want to evaluate function $\hat{h}_1$ at some generic point $x_0$. You can do this using the `predict` function, and somehow specifying $x_0$ in the `newdata` argument (in place of "predictor 1"). But `predict` will give you all three components of the model, added together: the $\hat{\beta}_0$ part, plus the $\hat{h}_1$ part (evaluated at whatever is in the "predictor 1" column), plus the $\hat{h}_2$ part (evaluated at whatever is in the "predictor 2" column). Your job is to "isolate" the $\hat{h}_1$ part, evaluated at $x_0$. We can subtract out $\hat{\beta}_0$, which is specified in the model output. But you can't just subtract out the $\hat{h}_2$ part, because we don't know it. Your job is to use a property of $\hat{h}_2$ (hint: Equation ) to remove it. 

You can also think of it this way: if `mean(predict(fit))` "zeroes-out" both $h$ functions, how can you modify the prediction data so that one of the $h$ functions *doesn't* zero-out, but instead evaluates at some desired point?

<!--chapter:end:150-When_we_only_want_interpretation_on_some_predictors.Rmd-->

# Special cases {-}



# Regression when data are censored: survival analysis

<!--chapter:end:160-Regression_when_data_are_censored.Rmd-->

# Regression in the presence of outliers: robust regression

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

## Robust Regression in R

DSCI 562 Lab 4 Tutorial

There are many R packages out there to assist with robust estimation. It depends on the task at hand. We'll go over a few.

For robust linear regression, there are a few options. 

There's the `rlm` function in the `MASS` package. It works in a similar way as the `lm` function. Can also use the functions `predict`, `residuals`, `coefficients`, etc. on the output. I like this option because it allows for different psi functions besides the Huber.

```
library(MASS)
(fit6 <- rlm(mpg ~ disp + wt, data=mtcars))
(fit7 <- rlm(mpg ~ disp + wt, data=mtcars, psi=psi.bisquare))
```

The package `robustbase` has the function `lmrob` for linear models, but also has `glmrob` for GLM's. Similarly, the `robust` package has similar functions `lmRob` and `glmRob`. 

A robust version of GAM's can be obtained with the `robustgam` function in the `robustgam` package. 

A robust version of LME's can be obtained with the `rlmer` function in the `robustlmm` package. 

### Heavy Tailed Regression

For a heavy tailed extension of `lm`, one can use the `tlm` function in the `hett` package. The package `heavy` has some regression techniques using heavy tailed distributions. 

<!--chapter:end:170-Regression_in_the_presence_of_outliers.Rmd-->

# Regression in the presence of extremes: extreme value regression

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

The problem with estimating extreme quantiles in the "usual" sense: 

Here is a histogram of 100 observations generated from a Student's _t_(1) distribution (it's heavy-tailed):

```{r}
set.seed(4)
y <- rt(100, df=1)
qplot(y) + theme_bw()
```

Here are estimates of high and low quantiles, compared to the actual. You can see the discrepency grows quickly. __Extreme-low quantiles are too high__, whereas __extreme-high quantiles are too low__. 


```{r, fig.width=8, echo=FALSE}
p1 <- ggplot(data.frame(x=c(0,0.05)), aes(x)) + 
    stat_function(aes(colour="Estimated"),
                  fun=function(x) quantile(y, probs=x, type=1)) +
    stat_function(aes(colour="Actual"),
                  fun=function(x) qt(x, df=1)) +
    scale_colour_discrete(guide=FALSE) +
    labs(x=expression(paste("Quantile level (", tau, ")")),
         y="Quantile") +
    theme_bw()
p2 <- ggplot(data.frame(x=c(0.95,1)), aes(x)) + 
    stat_function(aes(colour="Estimated"),
                  fun=function(x) quantile(y, probs=x, type=1)) +
    stat_function(aes(colour="Actual"),
                  fun=function(x) qt(x, df=1)) +
    scale_colour_discrete("Type") +
    labs(x=expression(paste("Quantile level (", tau, ")")),
         y="Quantile") +
    theme_bw()
cowplot::plot_grid(p1, p2, ncol=2)
```


As a rule of thumb, it's best to stay below $\tau=0.95$ or above $\tau=0.05$. If you really want estimates of these extreme quantiles, you'll need to turn to __Extreme Value Theory__ to make an assumption on the tail of the distribution of the data. One common approach is to fit a generalized Pareto distribution to the upper portion of the data, after which you can extract high quantiles. 

<!--chapter:end:180-Regression_in_the_presence_of_extremes.Rmd-->

# Regression when data are ordinal

<!--chapter:end:190-Regression_when_data_are_ordinal.Rmd-->

# Regression when data are missing: multiple imputation

<!--chapter:end:200-Regression_when_data_are_missing.Rmd-->

# Regression under many groups: mixed effects models

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

## Motivation for LME

Let's take a look at the `esoph` data set, to see how the number of controls `ncontrols` affects the number of cases `ncases` of cancer for each age group `agegp`. Here's what the data look like (with a tad bit of vertical jitter):

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
suppressMessages(library(tidyverse))
library(plyr)
dat <- as_tibble(esoph) %>% 
    mutate(agegp = as.character(agegp))
(p <- ggplot(dat, aes(ncontrols, ncases, group=agegp, colour=agegp)) +
    geom_jitter(height=0.25) +
    scale_colour_discrete("Age Group") +
    ylab("Number of Cases") + xlab("Number of Controls"))
```

It seems each age group has a different relationship. Should we then fit regression lines for each group separately? Here's what we get, if we do:

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
p + geom_smooth(method="lm", se=FALSE, size=0.5)
```

But, each group has so few observations, making the regression less powerful:

```{r, echo=FALSE}
dat %>% 
    group_by(agegp) %>% 
    summarise(n=length(ncases))
```

__Question__: can we borrow information across groups to strengthen regression, while still allowing each group to have its own regression line?

Here's another scenario: suppose we want to know the effect of `ncontrols` on the average person. Then, we would only include one common slope parameter for all individuals. Even if each individual "has their own unique slope", this model is still sensible because the common slope can be interpreted as the _average effect_. The problem with this model is that the typical estimates of standard error on our regression coefficients will be artificially small due to correlation in the data induced by the grouping.

Here is a simulation that compares the "actual" SE (or at least an approximation of it) and the SE reported by `lm`:

```{r, cache=TRUE}
# library(tidyverse)
# library(broom)
# set.seed(1000)
# ## Number of groups
# g <- 10
# ## Number of observations per group
# ng <- 10
# ## Initiate slope and SE estimates
# beta1hat <- numeric(0)
# se <- numeric(0)
# for (i in 1:1000) {
#   ## Generate intercept and slope from a joint Normal distribution
#   beta0 <- rnorm(g)
#   beta1 <- 1 + beta0 + rnorm(g)
#   ## Generate iid data from within each group
#   dat <- tibble(group=LETTERS[1:g], beta0, beta1) %>%
#     mutate(x = map(beta0, ~ rnorm(ng))) %>%
#     unnest() %>%
#     group_by(group) %>%
#     mutate(eps = rnorm(length(x)),
#            y = beta0 + beta1 * x + eps)
#   ## Fit a linear regression, forcing a common slope
#   fit <- lm(y ~ x + group, data=dat) %>%
#     tidy()
#   beta1hat[i] <- fit$estimate[2]
#   se[i] <- fit$std.error[2]
# }
# ## Actual SE:
# sd(beta1hat)
# ## SE given from the lm fit:
# mean(se)
# 
# ## Here's a plot of the last sample generated:
# ggplot(dat, aes(x, y)) +
#   geom_point(aes(colour=group), alpha=0.5) +
#   theme_bw()
```

__Question__: How can we account for the dependence in the data?

Both questions can be addressed using a _Linear Mixed Effects_ (LME) model. An LME model is just a linear regression model for each group, with different slopes and intercepts, but the collection of slopes and intercepts _is assumed to come from some normal distribution_.

### Definition

With one predictor ($X$), we can write an LME as follows:
$$ Y = \left(\beta_0 + b_0\right) + \left(\beta_1 + b_1\right) X + \varepsilon,  $$
where the error term $\varepsilon$ has mean zero, and the $b_0$ and $b_1$ terms are normally distributed having a mean of zero, and some unknown variances and correlation. The $\beta$ terms are called the _fixed effects_, and the $b$ terms are called the _random effects_. Since the model has both types of effects, it's said to be a _mixed_ model -- hence the name of "LME". 

Note that we don't have to make _both_ the slope and intercept random. For example, we can remove the $b_0$ term, which would mean that each group is forced to have the same (fixed) intercept $\beta_0$. Also, we can add more predictors ($X$ variables).

### R Tools for Fitting

Two R packages exist for working with mixed effects models: `lme4` and `nlme`. We'll be using the `lme4` package (check out [this](http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models) discussion on Cross Validated for a comparison of the two packages).

Let's fit the model. We need to indicate a formula first in the `lmer` function, and indicate the data set we're using.

```{r}
library(lme4)
fit <- lmer(ncases ~ ncontrols + (ncontrols | agegp), 
            data=dat)
```

Let's take a closer look at the _formula_, which in this case is `ncases ~ ncontrols + (ncontrols | agegp)`. 

On the left of the `~` is the response variable, as usual (just like for `lm`). On the right, we need to specify both the fixed and random effects. The fixed effects part is the same as usual: `ncontrols` indicates the explanatory variables that get a fixed effect. Then, we need to indicate which explanatory variables get a random effect. The random effects can be indicated in parentheses, separated by `+`, followed by a `|`, after which the variable(s) that you wish to group by are indicated. So `|` can be interpreted as "grouped by".

Now let's look at the model output:

```{r}
summary(fit)
```

The random and fixed effects are indicated here.

- Under the "Random effects:" section, we have the variance of each random effect, and the lower part of the correlation matrix of these random effects.
- Under the "Fixed effects:" section, we have the estimates of the fixed effects, as well as the uncertainty in the estimate (indicated by the Std. Error).

We can extract the collection of slopes and intercepts for each group using the `coef` function:

```{r}
(par_coll <- coef(fit)[[1]])
```

Let's put these regression lines on the plot:

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
## Put the slopes and intercepts with the data frame:
par_coll %>% 
    rownames_to_column("agegp") %>% 
    left_join(dat, by="agegp")
par_coll <- rownames_to_column(par_coll)
dat <- ddply(dat, ~ agegp, function(df){
    pars <- subset(par_coll, rowname==unique(df$agegp))
    int <- pars$`(Intercept)`
    slp <- pars$ncontrols
    cbind(df, intercept=int, slope=slp)
})

## Plot
ggplot(dat, aes(ncontrols, ncases, group=agegp, colour=agegp)) +
    geom_jitter(height=0.25) +
    geom_abline(aes(intercept=intercept, slope=slope, colour=agegp)) +
    scale_colour_discrete("Age Group") +
    ylab("Number of Cases") + xlab("Number of Controls")
```

So, each group still gets its own regression line, but tying the parameters together with a normal distribution gives us a more powerful regression.

## Mixed Effects Models in R: tutorial

**Caution: in a highly developmental stage! See Section  \@ref(caution).**

Two R packages exist for working with mixed effects models: `lme4` and `nlme`. We'll be using the `lme4` package (check out [this](http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models) discussion on Cross Validated for a comparison of the two packages).

```{r}
library(lme4)
```

In Lab 1, we compared linear regression (function `lm`) with GLM's (function `glm`). In Lab 2, we consider adding a random effect to either of these:

- A linear model with random effects is a _Linear Mixed-Effects Model_, and is fit using the `lmer` function.
- A generalized linear model with random effects is a _Generalized Linear Mixed-Effects Model_, and is fit using the `glmer` function.

We'll work with the `esoph` data set, to see how the number of controls `ncontrols` affects the number of cases `ncases` based on age group `agegp`. Here's what the data look like (with a tad bit of vertical jitter):

```{r}
library(ggplot2)
library(dplyr)
dat <- esoph
p <- ggplot(dat, aes(ncontrols, ncases)) +
    geom_jitter(aes(colour=agegp), height=0.25)
p
```

Since the response is a count variable, we'll go ahead with a Poisson regression -- a Generalized Linear Mixed-Effects Model. The model is
$$ Y_{ij} \mid X_{ij} = x_{ij} \sim \text{Poisson}\left(\lambda_{ij}\right) $$
for each observation $i$ on the $j$'th age group, where $Y_{ij}$ is the number of cases, $X_{ij}$ is the number of controls, and $\lambda_{ij}$ is the conditional mean of $Y_{ij}.$ We model $\lambda_{ij}$ as
$$ \log\left(\lambda_{ij}\right) = \left(\beta_0 + b_{0j}\right) + \left(\beta_1 + b_{1j}\right) x_{ij}, $$
where $b_{0j}$ and $b_{1j}$ are joint (bivariate) normally distributed with zero mean. 

What does this model mean? First, it means that the mean is exponential in the explanatory variable, since we chose a $\log$ link function. Second, each age group ($j$) gets its own mean curve, via its own linear predictor. But we're saying that these linear predictors are related: the collection of slopes and intercepts across age groups are centered around $\beta_0$ and $\beta_1$ (respectively, called the _fixed effects_), and the slope and intercept of each age group departs from this center according to some Gaussian random noise (the $b$ terms, called the _random effects_).

Let's fit the model. Then we'll go through the formula, and the output.

```{r}
fit <- glmer(ncases ~ ncontrols + (1 + ncontrols | agegp), 
             data=dat, 
             family=poisson)
summary(fit)
```

To specify the formula, the fixed effects part is the same as usual: `ncases ~ ncontrols` gives you `ncases = beta0 + beta1 * ncontrols`. Note that the intercept is put in there by default. Then, we need to indicate which explanatory variables are getting the random effects -- including the intercept this time (with a 1), if you want it (in this case, we do). The random effects can be indicated in parentheses, separated by `+`, followed by a `|`, after which the variable(s) that you wish to group by are indicated. So `|` can be interpreted as "grouped by".

The output of the model fit is similar to what you've seen before (in `glm` for example), but the "random effects" part is new. That gives us the estimates of the joint normal distribution of the random effects -- through the variances, and correlation matrix to the right (only the lower-diagonal of the correlation matrix is given, because that matrix is symmetric anyway).

Let's see what the intercepts and slopes for each age group are, and let's plot the estimated mean curves:

```{r}
(coef_fit <- coef(fit)$agegp)
## Colours with stat_function are not nice to deal with. Do manually.
p + stat_function(aes(colour="25-34"), fun = function(x) exp(coef_fit[1,1] + coef_fit[1,2]*x)) +
    stat_function(aes(colour="35-44"), fun = function(x) exp(coef_fit[2,1] + coef_fit[2,2]*x)) +
    stat_function(aes(colour="45-54"), fun = function(x) exp(coef_fit[3,1] + coef_fit[3,2]*x)) +
    stat_function(aes(colour="55-64"), fun = function(x) exp(coef_fit[4,1] + coef_fit[4,2]*x)) +
    stat_function(aes(colour="65-74"), fun = function(x) exp(coef_fit[5,1] + coef_fit[5,2]*x)) +
    stat_function(aes(colour="75+"),   fun = function(x) exp(coef_fit[6,1] + coef_fit[6,2]*x))
```

A (response-) residual plot is somewhat sensible to look at here:

```{r}
plot(fit)
```

Looks fairly centered at zero, so the shape of the mean curves are satisfactory.


<!--chapter:end:210-Regression_under_many_groups.Rmd-->

# Regression on an entire distribution: Probabilistic Forecasting

**Caution: in a highly developmental stage! See Section  \@ref(caution).**


Up until now, we've only seen different ways of using a predictor to give us more information the __mean__ and __mode__ of the response. The world holds a huge emphasis on the mean and mode, but these are not always what's important. Two alternatives are:

1. __Probabilistic forecasting__
2. __Quantile Regression__ (numeric response only)

## Probabilistic Forecasting: What it is

The idea here is to put forth an _entire probability distribution_ as a prediction. 

Let's look at an example. Suppose there are two baseball teams, one that gets 1000 total hits in a year, and another that gets 1500. Using "total hits in a year" as a predictor, we set out to predict the total number of runs of both teams. Here's the top snippet of the data:

```{r}
baseball
```

Let's not concern ourselves with the _methods_ yet. Using a standard regression technique, here are our predictions:

```{r}
r <- 20
datsub <- filter(baseball,
                 (hits>=1000-r & hits<=1000+r) |
                     (hits>=1500-r & hits<=1500+r)) %>% 
    mutate(approx_hits = if_else(hits>=1000-r & hits<=1000+r, 1000, 1500))
datsub %>% 
    group_by(approx_hits) %>%
    summarize(expected_runs=round(mean(runs))) %>% 
    select(hits=approx_hits, expected_runs)
```

Using a probabilistic forecast, here are our predictions:

```{r, echo=FALSE}
ggplot(datsub, aes(runs)) + 
    geom_density(aes(fill=factor(approx_hits)), 
                 colour=NA, alpha=0.5) +
    scale_fill_discrete("Number of Hits (X)") +
    theme_bw() +
    xlab("Number of Runs (Y)")
```

Don't you think this is far more informative than the mean estimates in the above table?

The probabilistic forecast/prediction contains the most amount of information about the response as possible (based on a set of predictors), because it communicates the entire belief of what $Y$ values are most plausible, given values of the predictor.

Predictions/forecasts here are called __predictive distributions__.

From @gneiting_raftery:

> Indeed, over the past two decades, probabilistic forecasting has 
> become routine in such applications as weather and climate prediction
> (Palmer 2002; Gneiting and Raftery 2005), computational finance 
> (Duffle and Pan 1997), and macroeconomic forecasting
> (Garratt, Lee, Pesaran, and Shin 2003; Granger 2006).


## Review: Univariate distribution estimates

Let's review how to estimate a univariate probability density function or probability mass function.

### Continuous response

Here's a random sample of 10 continuous variables, ordered from smallest to largest, stored in the variable `x`:

```{r, echo=FALSE}
set.seed(43340)
x <- rnorm(10, sd=10) %>% round(1) %>% sort
```


Recall that we can use __histograms__ to estimate the density of the data. The idea is:

1. Cut the range of the data into "bins" of a certain width.
    - For these data, the range is 40. Let's set up four bins of width 10: -19.8 to -9.8, -9.8 to 0.2, etc.
2. Count the number of observations that fall into each bin.
    - For our setup, the number of observations falling into the four bins, in order, are: 3,2,2,3.
3. Make a bar plot (with no space between the bars), where the bar width corresponds to the bins, and the bar height corresponds to the number of observations in that bin.
    - For our setup, we have:
    
```{r}
ggplot(data.frame(x=x), aes(x)) +
    geom_histogram(binwidth=10, center=min(x)+5,
                   fill="orange", colour="black") +
    theme_bw()
```

(Note: this is not a true density, since the area under the curve is not 1, but the shape is what matters)

You'd have to play with the binwidth to get a histogram that looks about right (not too jagged, not too coarse). For the above example, there are too few data to make a good estimate. Let's now generate 1000 observations, and make a histogram using `qplot` from R's `ggplot2` package, with a variety of binwidths -- too small, too large, and just right.

```{r}
x <- rnorm(1000, sd=10)
qplot(x, binwidth=1)  # too small
qplot(x, binwidth=10)  # too big
qplot(x, binwidth=3.5)  # just right
```

__Advanced method__: There's a technique called the _kernel density estimate_ that works as an alernative to the histogram. The idea is to put a "little mound" (a kernel) on top of each observation, and add them up. Instead of playing with the binwidth, you can play with the "bandwidth" of the kernels. Use `geom="density"` in `qplot`, and use `bw` to play with the bandwidth:

```{r}
qplot(x, geom="density", bw=2.5)
```

### Discrete Response

When the response is discrete (this includes categorical), the approach is simpler:

1. Calculate the proportion of observations that fall into each category.
2. Make a bar chart, placing a bar over each category, and using the proportions as the bar heights.

Here are ten observations, stored in `x`:

```{r, echo=FALSE}
set.seed(4)
x <- rpois(10, lambda=1)
```

```{r}
x
```

The proportions are as follows:

```{r}
props <- tibble(Value=x) %>% 
    group_by(Value) %>% 
    summarize(Proportion=length(Value)/length(x))
```

You can plot these proportions with `qplot`, specifying `geom="col"`:

```{r}
qplot(x=Value, y=Proportion, data=props, geom="col")
```

You can use `ggplot2` to calculate the proportions, but it's more complex. It's easier to plot the raw counts, instead of proportions -- and that's fine, you'll still get the same shape. Using `qplot` again, let's make a plot for 1000 observations (note that I indicate that my data are discrete by using the `factor` function):

```{r}
set.seed(2)
x <- rpois(1000, lambda=1)
qplot(factor(x))
```

Here's the code to get proportions instead of counts:

```
qplot(factor(x), mapping=aes(y=..prop..), group=1)
```


## Probabilistic Forecasts: subset-based learning methods

### The techniques

The local methods and classification/regression trees that we've seen so far can be used to produce probabilistic forecasts. For local methods, let's ignore the complications of kernel weighting and local polynomials. These methods result in a _subset_ of the data, for which we're used to taking the mean or mode. Instead, _use the subsetted data to plot a distribution_.

- For kNN, form a histogram/density plot/bar plot using the $k$ nearest neighbours.
- For the moving window (loess), form a histogram/density plot/bar plot using the observations that fall in the window.
- For tree-based methods, use the observations within a leaf to form a histogram/density plot/bar plot for that leaf.

The above baseball example used a moving window with a radius of ```r r``` hits. Visually, you can see the data that I subsetted within these two narrow windows, for hits of 1000 and 1500:

```{r}
ggplot(baseball, aes(hits, runs)) +
    geom_point(colour="orange", alpha=0.1) +
    geom_vline(xintercept=c(1000+c(-r,r), 1500+c(-r,r)),
               linetype="dashed") +
    theme_bw() +
    labs(x="Number of Hits (X)",
         y="Number of Runs (Y)")
```

### Exercise

1. Install the `Lahman` package, which contains the `Teams` dataset.
2. Build a null model probabilistic forecast of "number of runs" (`R` column).
3. Build a probabilistic forecast, using kNN, of "number of runs" for a team that has 1500 hits (`H` column) and 70 wins (`W` column). Don't forget to scale the predictors!
4. Do the same thing, but using linear regression. What additional assumption(s) is/are needed here?

### Bias-variance tradeoff

Let's examine the bias-variance / overfitting-underfitting tradeoff with kNN-based probabilistic forecasts. I'll run a simulation like so:

- Generate data from a bivariate Normal distribution, so that $X \sim N(0, 100)$, and $Y = X + N(0, 100)$.
- Training data will contain 500 observations, for which a kNN probabilistic forecast will be built when $X=25$.
- Try both a small (k=15) and large (k=100) value of $k$.
- For each value of $k$, we'll generate 20 training data sets.

Here are the 20 estimates for the values of $k$. The overall mean of the distributions are indicated by a vertical dashed line.

```{r, echo=FALSE}
set.seed(38)
N <- 20
n <- 500
sd <- 10
k <- c(15, 100)
x0 <- 25
estimates <- expand.grid(iter=1:N, k=k) %>% 
    group_by(iter, k) %>% do({
        this_k <- .$k
        tibble(iter = .$iter,
               k = paste("k =", this_k), 
               x = rnorm(n, sd=sd),
               y = x + rnorm(n, sd=sd),
               d = abs(x-x0)) %>% 
            arrange(d) %>% 
            select(-x, -d) %>% 
            `[`(1:this_k, 1:ncol(.))
    }) %>% 
    bind_rows(
        tibble(iter = NA,
               k = "Actual",
               y = rnorm(20000, mean=x0, sd=sd),
               type = "Actual")
    )
means <- estimates %>% 
    group_by(k) %>% 
    summarize(mean=mean(y))
ggplot(estimates, aes(x=y)) +
    facet_wrap(~k) +
    geom_density(aes(group=iter)) +
    geom_vline(data=means, mapping=aes(xintercept=mean),
               linetype="dashed", colour="orange") +
    theme_bw()
```

Notice that:

- When $k$ is large, our estimates are biased, because the distributions are not centered correctly. But, the estimates are more consistent.
- When $k$ is small, our estimates are less biased, because the distributions overall have a mean that is close to the true mean. But the variance is high -- we get all sorts of distribution shapes here.

A similar thing happens with a moving window, with the window width parameter. For tree-based methods, the amount that you partition the predictor space controls the bias-variance tradeoff.

### Evaluating Model Goodness

To choose a balance between bias and variance, we need a measure of prediction goodness. When predicting the mean, the MSE works. When predicting the mode, the classification error works. But what works for probabilistic forecasts?

This is an active area of research. The idea is to use a _proper scoring rule_ -- a way of assigning a score based on the forecast distribution and the outcome only, that _also encourages honesty_. We won't go into details -- see [@gneiting_raftery] for details.

_At the very least_, one should check that the forecast distributions are "calibrated" -- that is, the actual outcomes are spread evenly amongst the forecasts. You can check this by applying the forecast cdf to the corresponding outcome -- the resulting sample should be Uniform(0,1). Note that this is built-in to at least some proper scoring rules.

For this course, we won't be picky about how you choose your tuning parameters. Just look for a subset that you think has "enough" observations in it so that the distribution starts to take some shape, but not so much that it starts to shift.

## Discussion Points

For (1) and (2) below, you're choosing between two candidates to hire. Discuss the pros and cons of choosing one candidate over the other in the following situations.

1. Both are predicted to have the same productivity score of 75, but have the following probabilistic forecasts.

```{r, echo=FALSE}
suppressPackageStartupMessages(library(viridis))
mu <- 75
ggplot(data.frame(x=c(-10,10)+mu), aes(x)) +
    stat_function(fun=function(x) dnorm(x, mean=mu), 
                  mapping=aes(colour="A")) +
    stat_function(fun=function(x) dnorm(x, mean=mu, sd=3),
                  mapping=aes(colour="B")) +
    theme_bw() +
    scale_colour_viridis("", discrete=TRUE) +
    xlab("Productivity") +
    ylab("Density")
```

> It's hard to make a decision here. On the one hand, we can be fairly certain that the _actual_ productivity of candidate A will be about 75, but there's more of a gamble with candidate B. There's a very real chance that B's productivity is actually quite a bit higher than A -- for example, a productivity of 80 is plausible for B, but not for A. On the other hand, there's also a very real chance that B's productivity is actually quite a bit _lower_ than A, for the same reason. Your decision would depend on whether you would want to take a risk or not.

> On the other hand, in reality, this is only one tool out of many other aspects of the candidate that you would consider. It might be a good idea to chat with B to get a better sense of what their productivity might actually be.

2. Two "non-overlapping" forecasts:

```{r, echo=FALSE}
ggplot(data.frame(x=c(50,90)), aes(x)) +
    stat_function(fun=function(x) dnorm(x, mean=60), n=500,
                  mapping=aes(colour="A")) +
    stat_function(fun=function(x) dnorm(x, mean=80, sd=3), n=500,
                  mapping=aes(colour="B")) +
    theme_bw() +
    scale_colour_viridis("", discrete=TRUE) +
    xlab("Productivity") +
    ylab("Density")
```

> In this case, B is very very likely to have higher productivity than A, because all "plausible" productivity values for B are higher than all "plausible" productivity values of A.

> Again, this is just one tool you might use to make a decision. 

3. You've formed a probabilistic forecast for a particular value of the predictors, displayed below as a density. You then collect test data for that same value of the predictor, indicated as the points below the density. What is the problem with the probabilistic forecast?

```{r, echo=FALSE}
set.seed(473)
ggplot(data.frame(x=c(-3,5)), aes(x)) +
    stat_function(fun=dnorm, n=500) +
    geom_point(data=data.frame(x=rnorm(20, mean=2), y=0),
               mapping=aes(x,y)) +
    theme_bw() +
    xlab("Response") +
    ylab("Density")
```

> The forecast is biased, because the actual values are occuring near the upper tail of the distribution -- they _should_ be scattered about the middle, with a higher density of points occuring near 0. If using local methods, we'd have to reduce $k$ or the window width to decrease bias (to remove "further" data that are less relevant); if using a tree-based method, you could grow the tree deeper to lower the bias. 

## When are they not useful?

Probabilistic forecasts are useful if you're making a small amount of decisions at a time. For example:

- Predicting which hockey team will win the Stanley Cup
- Looking at the 2-day-ahead prediction of river flow every day to decide whether to take flood mitigation measures.

But they are not appropriate when making decisions en-masse. For example:

- A bus company wants to know how long it takes a bus to travel between stops, for all stops and all busses.
- You want to predict future behaviour of customers.

<!--chapter:end:220-Regression_on_an_entire_distribution.Rmd-->

