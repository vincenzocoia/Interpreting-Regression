<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 25 Regression on an entire distribution: Probabilistic Forecasting | Interpreting Regression</title>
  <meta name="description" content="My tutorials for regression analysis, in the form of a bookdown book.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 25 Regression on an entire distribution: Probabilistic Forecasting | Interpreting Regression" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="My tutorials for regression analysis, in the form of a bookdown book." />
  <meta name="github-repo" content="vincenzocoia/Interpreting-Regression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 25 Regression on an entire distribution: Probabilistic Forecasting | Interpreting Regression" />
  
  <meta name="twitter:description" content="My tutorials for regression analysis, in the form of a bookdown book." />
  

<meta name="author" content="Vincenzo Coia">


<meta name="date" content="2019-05-29">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regression-under-many-groups-mixed-effects-models.html">
<link rel="next" href="regression-when-order-matters-time-series-and-spatial-analysis.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpreting Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#caution"><i class="fa fa-check"></i><b>1.1</b> Caution</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#purpose-of-the-book"><i class="fa fa-check"></i><b>1.2</b> Purpose of the book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#tasks-that-motivate-regression"><i class="fa fa-check"></i><b>1.3</b> Tasks that motivate Regression</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html"><i class="fa fa-check"></i><b>2</b> Regression in the context of problem solving</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#communicating-distillation-3-4"><i class="fa fa-check"></i><b>2.1</b> Communicating (Distillation 3-4)</a></li>
<li class="chapter" data-level="2.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#modelling-distillation-2-3"><i class="fa fa-check"></i><b>2.2</b> Modelling (Distillation 2-3)</a></li>
<li class="chapter" data-level="2.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#asking-useful-statistical-questions-distillation-1-2"><i class="fa fa-check"></i><b>2.3</b> Asking useful statistical questions (Distillation 1-2)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#business-objectives-examples"><i class="fa fa-check"></i><b>2.3.1</b> Business objectives: examples</a></li>
<li class="chapter" data-level="2.3.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-refining"><i class="fa fa-check"></i><b>2.3.2</b> Statistical objectives: refining</a></li>
<li class="chapter" data-level="2.3.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-examples"><i class="fa fa-check"></i><b>2.3.3</b> Statistical objectives: examples</a></li>
<li class="chapter" data-level="2.3.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-questions-are-not-the-full-picture"><i class="fa fa-check"></i><b>2.3.4</b> Statistical questions are not the full picture</a></li>
<li class="chapter" data-level="2.3.5" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-unrelated-to-supervised-learning"><i class="fa fa-check"></i><b>2.3.5</b> Statistical objectives unrelated to supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#prerequisites-to-an-analysis"><i class="fa fa-check"></i><b>2.4</b> Prerequisites to an analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="an-outcome-on-its-own.html"><a href="an-outcome-on-its-own.html"><i class="fa fa-check"></i>An outcome on its own</a></li>
<li class="chapter" data-level="3" data-path="distributions-uncertainty-is-worth-explaining.html"><a href="distributions-uncertainty-is-worth-explaining.html"><i class="fa fa-check"></i><b>3</b> Distributions: Uncertainty is worth explaining</a></li>
<li class="chapter" data-level="4" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html"><i class="fa fa-check"></i><b>4</b> Explaining an uncertain outcome: interpretable quantities</a><ul>
<li class="chapter" data-level="4.1" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#probabilistic-quantities"><i class="fa fa-check"></i><b>4.1</b> Probabilistic Quantities</a></li>
<li class="chapter" data-level="4.2" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#what-is-the-mean-anyway"><i class="fa fa-check"></i><b>4.2</b> What is the mean, anyway?</a></li>
<li class="chapter" data-level="4.3" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#quantiles"><i class="fa fa-check"></i><b>4.3</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-versions-of-interpretable-quantities.html"><a href="data-versions-of-interpretable-quantities.html"><i class="fa fa-check"></i><b>5</b> Data versions of interpretable quantities</a><ul>
<li class="chapter" data-level="5.1" data-path="data-versions-of-interpretable-quantities.html"><a href="data-versions-of-interpretable-quantities.html#estimation-of-probabilistic-quantities"><i class="fa fa-check"></i><b>5.1</b> Estimation of Probabilistic Quantities</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sampling-distributions-another-layer-of-uncertainty-added-from-estimation.html"><a href="sampling-distributions-another-layer-of-uncertainty-added-from-estimation.html"><i class="fa fa-check"></i><b>6</b> Sampling distributions: Another layer of uncertainty added from estimation</a></li>
<li class="chapter" data-level="7" data-path="improving-estimator-quality-by-parametric-distributional-assumptions-and-mle.html"><a href="improving-estimator-quality-by-parametric-distributional-assumptions-and-mle.html"><i class="fa fa-check"></i><b>7</b> Improving estimator quality by parametric distributional assumptions and MLE</a></li>
<li class="chapter" data-level="" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html"><i class="fa fa-check"></i>Prediction: harnessing the signal</a></li>
<li class="chapter" data-level="8" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html"><i class="fa fa-check"></i><b>8</b> Reducing uncertainty of the outcome: including predictors</a><ul>
<li class="chapter" data-level="8.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-terminology"><i class="fa fa-check"></i><b>8.1</b> Variable terminology</a><ul>
<li class="chapter" data-level="8.1.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-types"><i class="fa fa-check"></i><b>8.1.1</b> Variable types</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#irreducible-error"><i class="fa fa-check"></i><b>8.2</b> Irreducible Error</a></li>
<li class="chapter" data-level="8.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#in-class-exercises-irreducible-error"><i class="fa fa-check"></i><b>8.3</b> In-class Exercises: Irreducible Error</a><ul>
<li class="chapter" data-level="8.3.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-regression"><i class="fa fa-check"></i><b>8.3.1</b> Oracle regression</a></li>
<li class="chapter" data-level="8.3.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-classification"><i class="fa fa-check"></i><b>8.3.2</b> Oracle classification</a></li>
<li class="chapter" data-level="8.3.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-random-prediction"><i class="fa fa-check"></i><b>8.3.3</b> (BONUS) Random prediction</a></li>
<li class="chapter" data-level="8.3.4" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-a-more-non-standard-regression"><i class="fa fa-check"></i><b>8.3.4</b> (BONUS) A more non-standard regression</a></li>
<li class="chapter" data-level="8.3.5" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-oracle-mse"><i class="fa fa-check"></i><b>8.3.5</b> (BONUS) Oracle MSE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html"><i class="fa fa-check"></i><b>9</b> The signal: model functions</a><ul>
<li class="chapter" data-level="9.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#linear-quantile-regression"><i class="fa fa-check"></i><b>9.1</b> Linear Quantile Regression</a><ul>
<li class="chapter" data-level="9.1.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#exercise"><i class="fa fa-check"></i><b>9.1.1</b> Exercise</a></li>
<li class="chapter" data-level="9.1.2" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-crossing-quantiles"><i class="fa fa-check"></i><b>9.1.2</b> Problem: Crossing quantiles</a></li>
<li class="chapter" data-level="9.1.3" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-upper-quantiles"><i class="fa fa-check"></i><b>9.1.3</b> Problem: Upper quantiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html"><i class="fa fa-check"></i><b>10</b> The Model-Fitting Paradigm in R</a><ul>
<li class="chapter" data-level="10.1" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html#broom-package"><i class="fa fa-check"></i><b>10.1</b> <code>broom</code> package</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html"><i class="fa fa-check"></i><b>11</b> Estimating parametric model functions</a><ul>
<li class="chapter" data-level="11.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#writing-the-sample-mean-as-an-optimization-problem"><i class="fa fa-check"></i><b>11.1</b> Writing the sample mean as an optimization problem</a></li>
<li class="chapter" data-level="11.2" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#evaluating-model-goodness-quantiles"><i class="fa fa-check"></i><b>11.2</b> Evaluating Model Goodness: Quantiles</a></li>
<li class="chapter" data-level="11.3" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#simple-linear-regression"><i class="fa fa-check"></i><b>11.3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="11.3.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#model-specification"><i class="fa fa-check"></i><b>11.3.1</b> Model Specification</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#linear-models-in-general"><i class="fa fa-check"></i><b>11.4</b> Linear models in general</a></li>
<li class="chapter" data-level="11.5" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#reference-treatment-parameterization"><i class="fa fa-check"></i><b>11.5</b> reference-treatment parameterization</a><ul>
<li class="chapter" data-level="11.5.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#more-than-one-category-lab-2"><i class="fa fa-check"></i><b>11.5.1</b> More than one category (Lab 2)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><i class="fa fa-check"></i><b>12</b> Estimating assumption-free: the world of supervised learning techniques</a><ul>
<li class="chapter" data-level="12.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#what-machine-learning-is"><i class="fa fa-check"></i><b>12.1</b> What machine learning is</a></li>
<li class="chapter" data-level="12.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#types-of-supervised-learning"><i class="fa fa-check"></i><b>12.2</b> Types of Supervised Learning</a></li>
<li class="chapter" data-level="12.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#local-regression"><i class="fa fa-check"></i><b>12.3</b> Local Regression</a><ul>
<li class="chapter" data-level="12.3.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#knn"><i class="fa fa-check"></i><b>12.3.1</b> kNN</a></li>
<li class="chapter" data-level="12.3.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess"><i class="fa fa-check"></i><b>12.3.2</b> loess</a></li>
<li class="chapter" data-level="12.3.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#in-class-exercises"><i class="fa fa-check"></i><b>12.3.3</b> In-Class Exercises</a></li>
<li class="chapter" data-level="12.3.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#hyperparameters-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>12.3.4</b> Hyperparameters and the bias/variance tradeoff</a></li>
<li class="chapter" data-level="12.3.5" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#extensions-to-knn-and-loess"><i class="fa fa-check"></i><b>12.3.5</b> Extensions to kNN and loess</a></li>
<li class="chapter" data-level="12.3.6" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#model-assumptions-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>12.3.6</b> Model assumptions and the bias/variance tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#splines-and-loess-regression"><i class="fa fa-check"></i><b>12.4</b> Splines and Loess Regression</a><ul>
<li class="chapter" data-level="12.4.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess-1"><i class="fa fa-check"></i><b>12.4.1</b> Loess</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html"><i class="fa fa-check"></i><b>13</b> Overfitting: The problem with adding too many parameters</a><ul>
<li class="chapter" data-level="13.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#classification-exercise-do-together"><i class="fa fa-check"></i><b>13.1</b> Classification Exercise: Do Together</a></li>
<li class="chapter" data-level="13.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#training-error-vs.generalization-error"><i class="fa fa-check"></i><b>13.2</b> Training Error vs. Generalization Error</a></li>
<li class="chapter" data-level="13.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-complexity"><i class="fa fa-check"></i><b>13.3</b> Model complexity</a><ul>
<li class="chapter" data-level="13.3.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#activity"><i class="fa fa-check"></i><b>13.3.1</b> Activity</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducible-error"><i class="fa fa-check"></i><b>13.4</b> Reducible Error</a><ul>
<li class="chapter" data-level="13.4.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#what-is-it"><i class="fa fa-check"></i><b>13.4.1</b> What is it?</a></li>
<li class="chapter" data-level="13.4.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#example"><i class="fa fa-check"></i><b>13.4.2</b> Example</a></li>
<li class="chapter" data-level="13.4.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#bias-and-variance"><i class="fa fa-check"></i><b>13.4.3</b> Bias and Variance</a></li>
<li class="chapter" data-level="13.4.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducing-reducible-error"><i class="fa fa-check"></i><b>13.4.4</b> Reducing reducible error</a></li>
<li class="chapter" data-level="13.4.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#error-decomposition"><i class="fa fa-check"></i><b>13.4.5</b> Error decomposition</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection"><i class="fa fa-check"></i><b>13.5</b> Model Selection</a><ul>
<li class="chapter" data-level="13.5.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#exercise-cv"><i class="fa fa-check"></i><b>13.5.1</b> Exercise: CV</a></li>
<li class="chapter" data-level="13.5.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#out-of-sample-error"><i class="fa fa-check"></i><b>13.5.2</b> Out-of-sample Error</a></li>
<li class="chapter" data-level="13.5.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#alternative-measures-of-model-goodness"><i class="fa fa-check"></i><b>13.5.3</b> Alternative measures of model goodness</a></li>
<li class="chapter" data-level="13.5.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-and-model-selection-setup"><i class="fa fa-check"></i><b>13.5.4</b> Feature and model selection: setup</a></li>
<li class="chapter" data-level="13.5.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection-1"><i class="fa fa-check"></i><b>13.5.5</b> Model selection</a></li>
<li class="chapter" data-level="13.5.6" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-predictor-selection"><i class="fa fa-check"></i><b>13.5.6</b> Feature (predictor) selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="describing-relationships.html"><a href="describing-relationships.html"><i class="fa fa-check"></i>Describing Relationships</a></li>
<li class="chapter" data-level="14" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html"><i class="fa fa-check"></i><b>14</b> There’s meaning in parameters</a><ul>
<li class="chapter" data-level="14.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#the-types-of-parametric-assumptions"><i class="fa fa-check"></i><b>14.1</b> The types of parametric assumptions</a><ul>
<li class="chapter" data-level="14.1.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#when-defining-a-model-function."><i class="fa fa-check"></i><b>14.1.1</b> 1. When defining a <strong>model function</strong>.</a></li>
<li class="chapter" data-level="14.1.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#when-defining-probability-distributions."><i class="fa fa-check"></i><b>14.1.2</b> 2. When defining <strong>probability distributions</strong>.</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#the-value-of-making-parametric-assumptions"><i class="fa fa-check"></i><b>14.2</b> The value of making parametric assumptions</a><ul>
<li class="chapter" data-level="14.2.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#value-1-reduced-error"><i class="fa fa-check"></i><b>14.2.1</b> Value #1: Reduced Error</a></li>
<li class="chapter" data-level="14.2.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#value-2-interpretation"><i class="fa fa-check"></i><b>14.2.2</b> Value #2: Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#anova"><i class="fa fa-check"></i><b>14.3</b> ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="the-meaning-of-interaction.html"><a href="the-meaning-of-interaction.html"><i class="fa fa-check"></i><b>15</b> The meaning of interaction</a></li>
<li class="chapter" data-level="16" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html"><i class="fa fa-check"></i><b>16</b> Scales and the restricted range problem</a><ul>
<li class="chapter" data-level="16.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#problems"><i class="fa fa-check"></i><b>16.1</b> Problems</a></li>
<li class="chapter" data-level="16.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solutions"><i class="fa fa-check"></i><b>16.2</b> Solutions</a><ul>
<li class="chapter" data-level="16.2.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-1-transformations"><i class="fa fa-check"></i><b>16.2.1</b> Solution 1: Transformations</a></li>
<li class="chapter" data-level="16.2.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-2-link-functions"><i class="fa fa-check"></i><b>16.2.2</b> Solution 2: Link Functions</a></li>
<li class="chapter" data-level="16.2.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-3-scientifically-backed-functions"><i class="fa fa-check"></i><b>16.2.3</b> Solution 3: Scientifically-backed functions</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#glms-in-r"><i class="fa fa-check"></i><b>16.3</b> GLM’s in R</a><ul>
<li class="chapter" data-level="16.3.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#broomaugment"><i class="fa fa-check"></i><b>16.3.1</b> <code>broom::augment()</code></a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#options-for-logistic-regression"><i class="fa fa-check"></i><b>16.4</b> Options for Logistic Regression</a><ul>
<li class="chapter" data-level="16.4.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#models"><i class="fa fa-check"></i><b>16.4.1</b> Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="improving-estimation-through-distributional-assumptions.html"><a href="improving-estimation-through-distributional-assumptions.html"><i class="fa fa-check"></i><b>17</b> Improving estimation through distributional assumptions</a></li>
<li class="chapter" data-level="18" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html"><i class="fa fa-check"></i><b>18</b> When we only want interpretation on some predictors</a><ul>
<li class="chapter" data-level="18.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability-in-gams"><i class="fa fa-check"></i><b>18.1</b> Non-identifiability in GAMS</a><ul>
<li class="chapter" data-level="18.1.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability"><i class="fa fa-check"></i><b>18.1.1</b> Non-identifiability</a></li>
<li class="chapter" data-level="18.1.2" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#question-1b"><i class="fa fa-check"></i><b>18.1.2</b> Question 1b</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="special-cases.html"><a href="special-cases.html"><i class="fa fa-check"></i>Special cases</a></li>
<li class="chapter" data-level="19" data-path="regression-when-data-are-censored-survival-analysis.html"><a href="regression-when-data-are-censored-survival-analysis.html"><i class="fa fa-check"></i><b>19</b> Regression when data are censored: survival analysis</a></li>
<li class="chapter" data-level="20" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html"><i class="fa fa-check"></i><b>20</b> Regression in the presence of outliers: robust regression</a><ul>
<li class="chapter" data-level="20.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#robust-regression-in-r"><i class="fa fa-check"></i><b>20.1</b> Robust Regression in R</a><ul>
<li class="chapter" data-level="20.1.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#heavy-tailed-regression"><i class="fa fa-check"></i><b>20.1.1</b> Heavy Tailed Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="regression-in-the-presence-of-extremes-extreme-value-regression.html"><a href="regression-in-the-presence-of-extremes-extreme-value-regression.html"><i class="fa fa-check"></i><b>21</b> Regression in the presence of extremes: extreme value regression</a></li>
<li class="chapter" data-level="22" data-path="regression-when-data-are-ordinal.html"><a href="regression-when-data-are-ordinal.html"><i class="fa fa-check"></i><b>22</b> Regression when data are ordinal</a></li>
<li class="chapter" data-level="23" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html"><i class="fa fa-check"></i><b>23</b> Regression when data are missing: multiple imputation</a><ul>
<li class="chapter" data-level="23.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#mean-imputation"><i class="fa fa-check"></i><b>23.1</b> Mean Imputation</a></li>
<li class="chapter" data-level="23.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation"><i class="fa fa-check"></i><b>23.2</b> Multiple Imputation</a><ul>
<li class="chapter" data-level="23.2.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#patterns"><i class="fa fa-check"></i><b>23.2.1</b> Patterns</a></li>
<li class="chapter" data-level="23.2.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation-1"><i class="fa fa-check"></i><b>23.2.2</b> Multiple Imputation</a></li>
<li class="chapter" data-level="23.2.3" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#pooling"><i class="fa fa-check"></i><b>23.2.3</b> Pooling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html"><i class="fa fa-check"></i><b>24</b> Regression under many groups: mixed effects models</a><ul>
<li class="chapter" data-level="24.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#motivation-for-lme"><i class="fa fa-check"></i><b>24.1</b> Motivation for LME</a><ul>
<li class="chapter" data-level="24.1.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#definition"><i class="fa fa-check"></i><b>24.1.1</b> Definition</a></li>
<li class="chapter" data-level="24.1.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#r-tools-for-fitting"><i class="fa fa-check"></i><b>24.1.2</b> R Tools for Fitting</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#mixed-effects-models-in-r-tutorial"><i class="fa fa-check"></i><b>24.2</b> Mixed Effects Models in R: tutorial</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html"><i class="fa fa-check"></i><b>25</b> Regression on an entire distribution: Probabilistic Forecasting</a><ul>
<li class="chapter" data-level="25.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasting-what-it-is"><i class="fa fa-check"></i><b>25.1</b> Probabilistic Forecasting: What it is</a></li>
<li class="chapter" data-level="25.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#review-univariate-distribution-estimates"><i class="fa fa-check"></i><b>25.2</b> Review: Univariate distribution estimates</a><ul>
<li class="chapter" data-level="25.2.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#continuous-response"><i class="fa fa-check"></i><b>25.2.1</b> Continuous response</a></li>
<li class="chapter" data-level="25.2.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discrete-response"><i class="fa fa-check"></i><b>25.2.2</b> Discrete Response</a></li>
</ul></li>
<li class="chapter" data-level="25.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasts-subset-based-learning-methods"><i class="fa fa-check"></i><b>25.3</b> Probabilistic Forecasts: subset-based learning methods</a><ul>
<li class="chapter" data-level="25.3.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#the-techniques"><i class="fa fa-check"></i><b>25.3.1</b> The techniques</a></li>
<li class="chapter" data-level="25.3.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#exercise-1"><i class="fa fa-check"></i><b>25.3.2</b> Exercise</a></li>
<li class="chapter" data-level="25.3.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>25.3.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="25.3.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#evaluating-model-goodness"><i class="fa fa-check"></i><b>25.3.4</b> Evaluating Model Goodness</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discussion-points"><i class="fa fa-check"></i><b>25.4</b> Discussion Points</a></li>
<li class="chapter" data-level="25.5" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#when-are-they-not-useful"><i class="fa fa-check"></i><b>25.5</b> When are they not useful?</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html"><i class="fa fa-check"></i><b>26</b> Regression when order matters: time series and spatial analysis</a><ul>
<li class="chapter" data-level="26.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#timeseries-in-base-r"><i class="fa fa-check"></i><b>26.1</b> Timeseries in (base) R</a></li>
<li class="chapter" data-level="26.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#spatial-example"><i class="fa fa-check"></i><b>26.2</b> Spatial Example</a></li>
<li class="chapter" data-level="26.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#a-model-for-river-rock-size"><i class="fa fa-check"></i><b>26.3</b> A Model for River Rock Size</a><ul>
<li class="chapter" data-level="26.3.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#average-rock-size"><i class="fa fa-check"></i><b>26.3.1</b> 1. Average rock size:</a></li>
<li class="chapter" data-level="26.3.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-rock-size"><i class="fa fa-check"></i><b>26.3.2</b> 2. Mean rock size:</a></li>
<li class="chapter" data-level="26.3.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#downstream-fining-curve"><i class="fa fa-check"></i><b>26.3.3</b> 3. Downstream fining curve:</a></li>
</ul></li>
<li class="chapter" data-level="26.4" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objectives"><i class="fa fa-check"></i><b>26.4</b> Statistical Objectives</a><ul>
<li class="chapter" data-level="26.4.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#preliminaries-variance-and-correlation"><i class="fa fa-check"></i><b>26.4.1</b> Preliminaries: Variance and Correlation</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#three-concepts"><i class="fa fa-check"></i><b>26.5</b> Three Concepts</a><ul>
<li class="chapter" data-level="26.5.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#error-variance-sigma_e2leftxright"><i class="fa fa-check"></i><b>26.5.1</b> Error Variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span></a></li>
<li class="chapter" data-level="26.5.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-variance-sigma_m2"><i class="fa fa-check"></i><b>26.5.2</b> Mean Variance <span class="math inline">\(\sigma_{M}^{2}\)</span></a></li>
<li class="chapter" data-level="26.5.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-correlation-rholeftdright"><i class="fa fa-check"></i><b>26.5.3</b> Mean Correlation <span class="math inline">\(\rho\left(d\right)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="26.6" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#estimation"><i class="fa fa-check"></i><b>26.6</b> Estimation</a><ul>
<li class="chapter" data-level="26.6.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#constant-error-variance"><i class="fa fa-check"></i><b>26.6.1</b> Constant Error Variance</a></li>
<li class="chapter" data-level="26.6.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#non-constant-error-variance"><i class="fa fa-check"></i><b>26.6.2</b> Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="26.7" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-1-downstream-fining-curve"><i class="fa fa-check"></i><b>26.7</b> Statistical Objective 1: Downstream Fining Curve</a><ul>
<li class="chapter" data-level="26.7.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#regression-form"><i class="fa fa-check"></i><b>26.7.1</b> Regression Form</a></li>
</ul></li>
<li class="chapter" data-level="26.8" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-2-river-profile"><i class="fa fa-check"></i><b>26.8</b> Statistical Objective 2: River Profile</a><ul>
<li class="chapter" data-level="26.8.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#simple-kriging"><i class="fa fa-check"></i><b>26.8.1</b> Simple Kriging</a></li>
<li class="chapter" data-level="26.8.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#universal-kriging"><i class="fa fa-check"></i><b>26.8.2</b> Universal Kriging</a></li>
<li class="chapter" data-level="26.8.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#kriging-under-non-constant-error-variance"><i class="fa fa-check"></i><b>26.8.3</b> Kriging under Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="26.9" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#confidence-intervals-of-the-river-profile"><i class="fa fa-check"></i><b>26.9</b> Confidence Intervals of the River Profile</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/vincenzocoia/Interpreting-Regression" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpreting Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-on-an-entire-distribution-probabilistic-forecasting" class="section level1">
<h1><span class="header-section-number">Chapter 25</span> Regression on an entire distribution: Probabilistic Forecasting</h1>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<p>Up until now, we’ve only seen different ways of using a predictor to give us more information the <strong>mean</strong> and <strong>mode</strong> of the response. The world holds a huge emphasis on the mean and mode, but these are not always what’s important. Two alternatives are:</p>
<ol style="list-style-type: decimal">
<li><strong>Probabilistic forecasting</strong></li>
<li><strong>Quantile Regression</strong> (numeric response only)</li>
</ol>
<div id="probabilistic-forecasting-what-it-is" class="section level2">
<h2><span class="header-section-number">25.1</span> Probabilistic Forecasting: What it is</h2>
<p>The idea here is to put forth an <em>entire probability distribution</em> as a prediction.</p>
<p>Let’s look at an example. Suppose there are two baseball teams, one that gets 1000 total hits in a year, and another that gets 1500. Using “total hits in a year” as a predictor, we set out to predict the total number of runs of both teams. Here’s the top snippet of the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">baseball</code></pre></div>
<pre><code>## # A tibble: 2,835 x 2
##     runs  hits
##    &lt;int&gt; &lt;int&gt;
##  1   401   426
##  2   302   323
##  3   249   328
##  4   137   178
##  5   302   403
##  6   376   410
##  7   231   274
##  8   351   384
##  9   310   375
## 10   617   747
## # … with 2,825 more rows</code></pre>
<p>Let’s not concern ourselves with the <em>methods</em> yet. Using a standard regression technique, here are our predictions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">r &lt;-<span class="st"> </span><span class="dv">20</span>
datsub &lt;-<span class="st"> </span><span class="kw">filter</span>(baseball,
                 (hits<span class="op">&gt;=</span><span class="dv">1000</span><span class="op">-</span>r <span class="op">&amp;</span><span class="st"> </span>hits<span class="op">&lt;=</span><span class="dv">1000</span><span class="op">+</span>r) <span class="op">|</span>
<span class="st">                     </span>(hits<span class="op">&gt;=</span><span class="dv">1500</span><span class="op">-</span>r <span class="op">&amp;</span><span class="st"> </span>hits<span class="op">&lt;=</span><span class="dv">1500</span><span class="op">+</span>r)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">approx_hits =</span> <span class="kw">if_else</span>(hits<span class="op">&gt;=</span><span class="dv">1000</span><span class="op">-</span>r <span class="op">&amp;</span><span class="st"> </span>hits<span class="op">&lt;=</span><span class="dv">1000</span><span class="op">+</span>r, <span class="dv">1000</span>, <span class="dv">1500</span>))
datsub <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">group_by</span>(approx_hits) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">expected_runs=</span><span class="kw">round</span>(<span class="kw">mean</span>(runs))) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">rename</span>(<span class="dt">hits=</span>approx_hits) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">select</span>(hits, expected_runs)</code></pre></div>
<pre><code>## # A tibble: 2 x 2
##    hits expected_runs
##   &lt;dbl&gt;         &lt;dbl&gt;
## 1  1000           558
## 2  1500           768</code></pre>
<p>Using a probabilistic forecast, here are our predictions:</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-87-1.png" width="672" /></p>
<p>Don’t you think this is far more informative than the mean estimates in the above table?</p>
<p>The probabilistic forecast/prediction contains the most amount of information about the response as possible (based on a set of predictors), because it communicates the entire belief of what <span class="math inline">\(Y\)</span> values are most plausible, given values of the predictor.</p>
<p>Predictions/forecasts here are called <strong>predictive distributions</strong>.</p>
<p>From <span class="citation">@gneiting_raftery</span>:</p>
<blockquote>
<p>Indeed, over the past two decades, probabilistic forecasting has become routine in such applications as weather and climate prediction (Palmer 2002; Gneiting and Raftery 2005), computational finance (Duffle and Pan 1997), and macroeconomic forecasting (Garratt, Lee, Pesaran, and Shin 2003; Granger 2006).</p>
</blockquote>
</div>
<div id="review-univariate-distribution-estimates" class="section level2">
<h2><span class="header-section-number">25.2</span> Review: Univariate distribution estimates</h2>
<p>Let’s review how to estimate a univariate probability density function or probability mass function.</p>
<div id="continuous-response" class="section level3">
<h3><span class="header-section-number">25.2.1</span> Continuous response</h3>
<p>Here’s a random sample of 10 continuous variables, ordered from smallest to largest, stored in the variable <code>x</code>:</p>
<p>Recall that we can use <strong>histograms</strong> to estimate the density of the data. The idea is:</p>
<ol style="list-style-type: decimal">
<li>Cut the range of the data into “bins” of a certain width.
<ul>
<li>For these data, the range is 40. Let’s set up four bins of width 10: -19.8 to -9.8, -9.8 to 0.2, etc.</li>
</ul></li>
<li>Count the number of observations that fall into each bin.
<ul>
<li>For our setup, the number of observations falling into the four bins, in order, are: 3,2,2,3.</li>
</ul></li>
<li>Make a bar plot (with no space between the bars), where the bar width corresponds to the bins, and the bar height corresponds to the number of observations in that bin.
<ul>
<li>For our setup, we have:</li>
</ul></li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">data.frame</span>(<span class="dt">x=</span>x), <span class="kw">aes</span>(x)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth=</span><span class="dv">10</span>, <span class="dt">center=</span><span class="kw">min</span>(x)<span class="op">+</span><span class="dv">5</span>,
                   <span class="dt">fill=</span><span class="st">&quot;orange&quot;</span>, <span class="dt">colour=</span><span class="st">&quot;black&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-89-1.png" width="672" /></p>
<p>(Note: this is not a true density, since the area under the curve is not 1, but the shape is what matters)</p>
<p>You’d have to play with the binwidth to get a histogram that looks about right (not too jagged, not too coarse). For the above example, there are too few data to make a good estimate. Let’s now generate 1000 observations, and make a histogram using <code>qplot</code> from R’s <code>ggplot2</code> package, with a variety of binwidths – too small, too large, and just right.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>, <span class="dt">sd=</span><span class="dv">10</span>)
<span class="kw">qplot</span>(x, <span class="dt">binwidth=</span><span class="dv">1</span>)  <span class="co"># too small</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-90-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(x, <span class="dt">binwidth=</span><span class="dv">10</span>)  <span class="co"># too big</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-90-2.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(x, <span class="dt">binwidth=</span><span class="fl">3.5</span>)  <span class="co"># just right</span></code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-90-3.png" width="672" /></p>
<p><strong>Advanced method</strong>: There’s a technique called the <em>kernel density estimate</em> that works as an alernative to the histogram. The idea is to put a “little mound” (a kernel) on top of each observation, and add them up. Instead of playing with the binwidth, you can play with the “bandwidth” of the kernels. Use <code>geom=&quot;density&quot;</code> in <code>qplot</code>, and use <code>bw</code> to play with the bandwidth:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(x, <span class="dt">geom=</span><span class="st">&quot;density&quot;</span>, <span class="dt">bw=</span><span class="fl">2.5</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-91-1.png" width="672" /></p>
</div>
<div id="discrete-response" class="section level3">
<h3><span class="header-section-number">25.2.2</span> Discrete Response</h3>
<p>When the response is discrete (this includes categorical), the approach is simpler:</p>
<ol style="list-style-type: decimal">
<li>Calculate the proportion of observations that fall into each category.</li>
<li>Make a bar chart, placing a bar over each category, and using the proportions as the bar heights.</li>
</ol>
<p>Here are ten observations, stored in <code>x</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x</code></pre></div>
<pre><code>##  [1] 1 0 0 0 2 0 1 2 3 0</code></pre>
<p>The proportions are as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">props &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">Value=</span>x) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">group_by</span>(Value) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">Proportion=</span><span class="kw">length</span>(Value)<span class="op">/</span><span class="kw">length</span>(x))</code></pre></div>
<p>You can plot these proportions with <code>qplot</code>, specifying <code>geom=&quot;col&quot;</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(<span class="dt">x=</span>Value, <span class="dt">y=</span>Proportion, <span class="dt">data=</span>props, <span class="dt">geom=</span><span class="st">&quot;col&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-95-1.png" width="672" /></p>
<p>You can use <code>ggplot2</code> to calculate the proportions, but it’s more complex. It’s easier to plot the raw counts, instead of proportions – and that’s fine, you’ll still get the same shape. Using <code>qplot</code> again, let’s make a plot for 1000 observations (note that I indicate that my data are discrete by using the <code>factor</code> function):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">2</span>)
x &lt;-<span class="st"> </span><span class="kw">rpois</span>(<span class="dv">1000</span>, <span class="dt">lambda=</span><span class="dv">1</span>)
<span class="kw">qplot</span>(<span class="kw">factor</span>(x))</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-96-1.png" width="672" /></p>
<p>Here’s the code to get proportions instead of counts:</p>
<pre><code>qplot(factor(x), mapping=aes(y=..prop..), group=1)</code></pre>
</div>
</div>
<div id="probabilistic-forecasts-subset-based-learning-methods" class="section level2">
<h2><span class="header-section-number">25.3</span> Probabilistic Forecasts: subset-based learning methods</h2>
<div id="the-techniques" class="section level3">
<h3><span class="header-section-number">25.3.1</span> The techniques</h3>
<p>The local methods and classification/regression trees that we’ve seen so far can be used to produce probabilistic forecasts. For local methods, let’s ignore the complications of kernel weighting and local polynomials. These methods result in a <em>subset</em> of the data, for which we’re used to taking the mean or mode. Instead, <em>use the subsetted data to plot a distribution</em>.</p>
<ul>
<li>For kNN, form a histogram/density plot/bar plot using the <span class="math inline">\(k\)</span> nearest neighbours.</li>
<li>For the moving window (loess), form a histogram/density plot/bar plot using the observations that fall in the window.</li>
<li>For tree-based methods, use the observations within a leaf to form a histogram/density plot/bar plot for that leaf.</li>
</ul>
<p>The above baseball example used a moving window with a radius of <code>20</code> hits. Visually, you can see the data that I subsetted within these two narrow windows, for hits of 1000 and 1500:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(baseball, <span class="kw">aes</span>(hits, runs)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">colour=</span><span class="st">&quot;orange&quot;</span>, <span class="dt">alpha=</span><span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_vline</span>(<span class="dt">xintercept=</span><span class="kw">c</span>(<span class="dv">1000</span><span class="op">+</span><span class="kw">c</span>(<span class="op">-</span>r,r), <span class="dv">1500</span><span class="op">+</span><span class="kw">c</span>(<span class="op">-</span>r,r)),
               <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&quot;Number of Hits (X)&quot;</span>,
         <span class="dt">y=</span><span class="st">&quot;Number of Runs (Y)&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-97-1.png" width="672" /></p>
</div>
<div id="exercise-1" class="section level3">
<h3><span class="header-section-number">25.3.2</span> Exercise</h3>
<ol style="list-style-type: decimal">
<li>Install the <code>Lahman</code> package, which contains the <code>Teams</code> dataset.</li>
<li>Build a null model probabilistic forecast of “number of runs” (<code>R</code> column).</li>
<li>Build a probabilistic forecast, using kNN, of “number of runs” for a team that has 1500 hits (<code>H</code> column) and 70 wins (<code>W</code> column). Don’t forget to scale the predictors!</li>
<li>Do the same thing, but using linear regression. What additional assumption(s) is/are needed here?</li>
</ol>
</div>
<div id="bias-variance-tradeoff" class="section level3">
<h3><span class="header-section-number">25.3.3</span> Bias-variance tradeoff</h3>
<p>Let’s examine the bias-variance / overfitting-underfitting tradeoff with kNN-based probabilistic forecasts. I’ll run a simulation like so:</p>
<ul>
<li>Generate data from a bivariate Normal distribution, so that <span class="math inline">\(X \sim N(0, 100)\)</span>, and <span class="math inline">\(Y = X + N(0, 100)\)</span>.</li>
<li>Training data will contain 500 observations, for which a kNN probabilistic forecast will be built when <span class="math inline">\(X=25\)</span>.</li>
<li>Try both a small (k=15) and large (k=100) value of <span class="math inline">\(k\)</span>.</li>
<li>For each value of <span class="math inline">\(k\)</span>, we’ll generate 20 training data sets.</li>
</ul>
<p>Here are the 20 estimates for the values of <span class="math inline">\(k\)</span>. The overall mean of the distributions are indicated by a vertical dashed line.</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-98-1.png" width="672" /></p>
<p>Notice that:</p>
<ul>
<li>When <span class="math inline">\(k\)</span> is large, our estimates are biased, because the distributions are not centered correctly. But, the estimates are more consistent.</li>
<li>When <span class="math inline">\(k\)</span> is small, our estimates are less biased, because the distributions overall have a mean that is close to the true mean. But the variance is high – we get all sorts of distribution shapes here.</li>
</ul>
<p>A similar thing happens with a moving window, with the window width parameter. For tree-based methods, the amount that you partition the predictor space controls the bias-variance tradeoff.</p>
</div>
<div id="evaluating-model-goodness" class="section level3">
<h3><span class="header-section-number">25.3.4</span> Evaluating Model Goodness</h3>
<p>To choose a balance between bias and variance, we need a measure of prediction goodness. When predicting the mean, the MSE works. When predicting the mode, the classification error works. But what works for probabilistic forecasts?</p>
<p>This is an active area of research. The idea is to use a <em>proper scoring rule</em> – a way of assigning a score based on the forecast distribution and the outcome only, that <em>also encourages honesty</em>. We won’t go into details – see <span class="citation">[@gneiting_raftery]</span> for details.</p>
<p><em>At the very least</em>, one should check that the forecast distributions are “calibrated” – that is, the actual outcomes are spread evenly amongst the forecasts. You can check this by applying the forecast cdf to the corresponding outcome – the resulting sample should be Uniform(0,1). Note that this is built-in to at least some proper scoring rules.</p>
<p>For this course, we won’t be picky about how you choose your tuning parameters. Just look for a subset that you think has “enough” observations in it so that the distribution starts to take some shape, but not so much that it starts to shift.</p>
</div>
</div>
<div id="discussion-points" class="section level2">
<h2><span class="header-section-number">25.4</span> Discussion Points</h2>
<p>For (1) and (2) below, you’re choosing between two candidates to hire. Discuss the pros and cons of choosing one candidate over the other in the following situations.</p>
<ol style="list-style-type: decimal">
<li>Both are predicted to have the same productivity score of 75, but have the following probabilistic forecasts.</li>
</ol>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-99-1.png" width="672" /></p>
<blockquote>
<p>It’s hard to make a decision here. On the one hand, we can be fairly certain that the <em>actual</em> productivity of candidate A will be about 75, but there’s more of a gamble with candidate B. There’s a very real chance that B’s productivity is actually quite a bit higher than A – for example, a productivity of 80 is plausible for B, but not for A. On the other hand, there’s also a very real chance that B’s productivity is actually quite a bit <em>lower</em> than A, for the same reason. Your decision would depend on whether you would want to take a risk or not.</p>
</blockquote>
<blockquote>
<p>On the other hand, in reality, this is only one tool out of many other aspects of the candidate that you would consider. It might be a good idea to chat with B to get a better sense of what their productivity might actually be.</p>
</blockquote>
<ol start="2" style="list-style-type: decimal">
<li>Two “non-overlapping” forecasts:</li>
</ol>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-100-1.png" width="672" /></p>
<blockquote>
<p>In this case, B is very very likely to have higher productivity than A, because all “plausible” productivity values for B are higher than all “plausible” productivity values of A.</p>
</blockquote>
<blockquote>
<p>Again, this is just one tool you might use to make a decision.</p>
</blockquote>
<ol start="3" style="list-style-type: decimal">
<li>You’ve formed a probabilistic forecast for a particular value of the predictors, displayed below as a density. You then collect test data for that same value of the predictor, indicated as the points below the density. What is the problem with the probabilistic forecast?</li>
</ol>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-101-1.png" width="672" /></p>
<blockquote>
<p>The forecast is biased, because the actual values are occuring near the upper tail of the distribution – they <em>should</em> be scattered about the middle, with a higher density of points occuring near 0. If using local methods, we’d have to reduce <span class="math inline">\(k\)</span> or the window width to decrease bias (to remove “further” data that are less relevant); if using a tree-based method, you could grow the tree deeper to lower the bias.</p>
</blockquote>
</div>
<div id="when-are-they-not-useful" class="section level2">
<h2><span class="header-section-number">25.5</span> When are they not useful?</h2>
<p>Probabilistic forecasts are useful if you’re making a small amount of decisions at a time. For example:</p>
<ul>
<li>Predicting which hockey team will win the Stanley Cup</li>
<li>Looking at the 2-day-ahead prediction of river flow every day to decide whether to take flood mitigation measures.</li>
</ul>
<p>But they are not appropriate when making decisions en-masse. For example:</p>
<ul>
<li>A bus company wants to know how long it takes a bus to travel between stops, for all stops and all busses.</li>
<li>You want to predict future behaviour of customers.</li>
</ul>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-under-many-groups-mixed-effects-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression-when-order-matters-time-series-and-spatial-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/vincenzocoia/Interpreting-Regression/edit/master/220-Regression_on_an_entire_distribution.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
