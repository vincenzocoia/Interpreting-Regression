<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 9 Local Regression | Interpreting Regression</title>
  <meta name="description" content="My tutorials for regression analysis, in the form of a bookdown book.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 9 Local Regression | Interpreting Regression" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="My tutorials for regression analysis, in the form of a bookdown book." />
  <meta name="github-repo" content="vincenzocoia/Interpreting-Regression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Local Regression | Interpreting Regression" />
  
  <meta name="twitter:description" content="My tutorials for regression analysis, in the form of a bookdown book." />
  

<meta name="author" content="Vincenzo Coia">


<meta name="date" content="2019-03-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regression-in-the-context-of-problem-solving.html">
<link rel="next" href="reducible-error.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpreting Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#caution"><i class="fa fa-check"></i><b>1.1</b> Caution</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#purpose-of-the-book"><i class="fa fa-check"></i><b>1.2</b> Purpose of the book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#tasks-that-motivate-regression"><i class="fa fa-check"></i><b>1.3</b> Tasks that motivate Regression</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>2</b> Distributions</a><ul>
<li class="chapter" data-level="2.1" data-path="distributions.html"><a href="distributions.html#probabilistic-quantities"><i class="fa fa-check"></i><b>2.1</b> Probabilistic Quantities</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>3</b> Estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="estimation.html"><a href="estimation.html#estimation-of-probabilistic-quantities"><i class="fa fa-check"></i><b>3.1</b> Estimation of Probabilistic Quantities</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="writing-the-sample-mean-as-an-optimization-problem.html"><a href="writing-the-sample-mean-as-an-optimization-problem.html"><i class="fa fa-check"></i><b>4</b> Writing the sample mean as an optimization problem</a></li>
<li class="chapter" data-level="5" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html"><i class="fa fa-check"></i><b>5</b> Probabilistic Forecasting</a><ul>
<li class="chapter" data-level="5.1" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#probabilistic-forecasting-what-it-is"><i class="fa fa-check"></i><b>5.1</b> Probabilistic Forecasting: What it is</a></li>
<li class="chapter" data-level="5.2" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#review-univariate-distribution-estimates"><i class="fa fa-check"></i><b>5.2</b> Review: Univariate distribution estimates</a><ul>
<li class="chapter" data-level="5.2.1" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#continuous-response"><i class="fa fa-check"></i><b>5.2.1</b> Continuous response</a></li>
<li class="chapter" data-level="5.2.2" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#discrete-response"><i class="fa fa-check"></i><b>5.2.2</b> Discrete Response</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#probabilistic-forecasts-subset-based-learning-methods"><i class="fa fa-check"></i><b>5.3</b> Probabilistic Forecasts: subset-based learning methods</a><ul>
<li class="chapter" data-level="5.3.1" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#the-techniques"><i class="fa fa-check"></i><b>5.3.1</b> The techniques</a></li>
<li class="chapter" data-level="5.3.2" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#exercise"><i class="fa fa-check"></i><b>5.3.2</b> Exercise</a></li>
<li class="chapter" data-level="5.3.3" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>5.3.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="5.3.4" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#evaluating-model-goodness"><i class="fa fa-check"></i><b>5.3.4</b> Evaluating Model Goodness</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#discussion-points"><i class="fa fa-check"></i><b>5.4</b> Discussion Points</a></li>
<li class="chapter" data-level="5.5" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#when-are-they-not-useful"><i class="fa fa-check"></i><b>5.5</b> When are they not useful?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="quantile-regression.html"><a href="quantile-regression.html"><i class="fa fa-check"></i><b>6</b> Quantile Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="quantile-regression.html"><a href="quantile-regression.html#what-is-the-mean-anyway"><i class="fa fa-check"></i><b>6.1</b> What is the mean, anyway?</a></li>
<li class="chapter" data-level="6.2" data-path="quantile-regression.html"><a href="quantile-regression.html#linear-quantile-regression"><i class="fa fa-check"></i><b>6.2</b> Linear Quantile Regression</a></li>
<li class="chapter" data-level="6.3" data-path="quantile-regression.html"><a href="quantile-regression.html#exercise-1"><i class="fa fa-check"></i><b>6.3</b> Exercise</a></li>
<li class="chapter" data-level="6.4" data-path="quantile-regression.html"><a href="quantile-regression.html#problem-crossing-quantiles"><i class="fa fa-check"></i><b>6.4</b> Problem: Crossing quantiles</a></li>
<li class="chapter" data-level="6.5" data-path="quantile-regression.html"><a href="quantile-regression.html#problem-upper-quantiles"><i class="fa fa-check"></i><b>6.5</b> Problem: Upper quantiles</a></li>
<li class="chapter" data-level="6.6" data-path="quantile-regression.html"><a href="quantile-regression.html#evaluating-model-goodness-1"><i class="fa fa-check"></i><b>6.6</b> Evaluating Model Goodness</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html"><i class="fa fa-check"></i><b>7</b> Introduction to Machine Learning</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#what-machine-learning-is"><i class="fa fa-check"></i><b>7.1</b> What machine learning is</a></li>
<li class="chapter" data-level="7.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#variable-terminology"><i class="fa fa-check"></i><b>7.2</b> Variable terminology</a><ul>
<li class="chapter" data-level="7.2.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#variable-types"><i class="fa fa-check"></i><b>7.2.1</b> Variable types</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#types-of-supervised-learning"><i class="fa fa-check"></i><b>7.3</b> Types of Supervised Learning</a></li>
<li class="chapter" data-level="7.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#together-linear-regression-example"><i class="fa fa-check"></i><b>7.4</b> Together: Linear Regression Example</a></li>
<li class="chapter" data-level="7.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#irreducible-error"><i class="fa fa-check"></i><b>7.5</b> Irreducible Error</a></li>
<li class="chapter" data-level="7.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#in-class-exercises-irreducible-error"><i class="fa fa-check"></i><b>7.6</b> In-class Exercises: Irreducible Error</a><ul>
<li class="chapter" data-level="7.6.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#oracle-regression"><i class="fa fa-check"></i><b>7.6.1</b> Oracle regression</a></li>
<li class="chapter" data-level="7.6.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#oracle-classification"><i class="fa fa-check"></i><b>7.6.2</b> Oracle classification</a></li>
<li class="chapter" data-level="7.6.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#bonus-random-prediction"><i class="fa fa-check"></i><b>7.6.3</b> (BONUS) Random prediction</a></li>
<li class="chapter" data-level="7.6.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#bonus-a-more-non-standard-regression"><i class="fa fa-check"></i><b>7.6.4</b> (BONUS) A more non-standard regression</a></li>
<li class="chapter" data-level="7.6.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#bonus-oracle-mse"><i class="fa fa-check"></i><b>7.6.5</b> (BONUS) Oracle MSE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html"><i class="fa fa-check"></i><b>8</b> Regression in the context of problem solving</a><ul>
<li class="chapter" data-level="8.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#communicating-distillation-3-4"><i class="fa fa-check"></i><b>8.1</b> Communicating (Distillation 3-4)</a></li>
<li class="chapter" data-level="8.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#modelling-distillation-2-3"><i class="fa fa-check"></i><b>8.2</b> Modelling (Distillation 2-3)</a></li>
<li class="chapter" data-level="8.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#asking-useful-statistical-questions-distillation-1-2"><i class="fa fa-check"></i><b>8.3</b> Asking useful statistical questions (Distillation 1-2)</a><ul>
<li class="chapter" data-level="8.3.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#business-objectives-examples"><i class="fa fa-check"></i><b>8.3.1</b> Business objectives: examples</a></li>
<li class="chapter" data-level="8.3.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-refining"><i class="fa fa-check"></i><b>8.3.2</b> Statistical objectives: refining</a></li>
<li class="chapter" data-level="8.3.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-examples"><i class="fa fa-check"></i><b>8.3.3</b> Statistical objectives: examples</a></li>
<li class="chapter" data-level="8.3.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-questions-are-not-the-full-picture"><i class="fa fa-check"></i><b>8.3.4</b> Statistical questions are not the full picture</a></li>
<li class="chapter" data-level="8.3.5" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-unrelated-to-supervised-learning"><i class="fa fa-check"></i><b>8.3.5</b> Statistical objectives unrelated to supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#prerequisites-to-an-analysis"><i class="fa fa-check"></i><b>8.4</b> Prerequisites to an analysis</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="local-regression.html"><a href="local-regression.html"><i class="fa fa-check"></i><b>9</b> Local Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="local-regression.html"><a href="local-regression.html#knn"><i class="fa fa-check"></i><b>9.1</b> kNN</a></li>
<li class="chapter" data-level="9.2" data-path="local-regression.html"><a href="local-regression.html#loess"><i class="fa fa-check"></i><b>9.2</b> loess</a></li>
<li class="chapter" data-level="9.3" data-path="local-regression.html"><a href="local-regression.html#in-class-exercises"><i class="fa fa-check"></i><b>9.3</b> In-Class Exercises</a><ul>
<li class="chapter" data-level="9.3.1" data-path="local-regression.html"><a href="local-regression.html#exercise-1-mean-at-x0"><i class="fa fa-check"></i><b>9.3.1</b> Exercise 1: Mean at <span class="math inline">\(X=0\)</span></a></li>
<li class="chapter" data-level="9.3.2" data-path="local-regression.html"><a href="local-regression.html#exercise-2-regression-curve"><i class="fa fa-check"></i><b>9.3.2</b> Exercise 2: Regression Curve</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="local-regression.html"><a href="local-regression.html#hyperparameters-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>9.4</b> Hyperparameters and the bias/variance tradeoff</a></li>
<li class="chapter" data-level="9.5" data-path="local-regression.html"><a href="local-regression.html#extensions-to-knn-and-loess"><i class="fa fa-check"></i><b>9.5</b> Extensions to kNN and loess</a><ul>
<li class="chapter" data-level="9.5.1" data-path="local-regression.html"><a href="local-regression.html#kernel-weighting"><i class="fa fa-check"></i><b>9.5.1</b> Kernel weighting</a></li>
<li class="chapter" data-level="9.5.2" data-path="local-regression.html"><a href="local-regression.html#local-polynomials"><i class="fa fa-check"></i><b>9.5.2</b> Local polynomials</a></li>
<li class="chapter" data-level="9.5.3" data-path="local-regression.html"><a href="local-regression.html#combination"><i class="fa fa-check"></i><b>9.5.3</b> Combination</a></li>
<li class="chapter" data-level="9.5.4" data-path="local-regression.html"><a href="local-regression.html#other-distances"><i class="fa fa-check"></i><b>9.5.4</b> Other distances</a></li>
<li class="chapter" data-level="9.5.5" data-path="local-regression.html"><a href="local-regression.html#scaling"><i class="fa fa-check"></i><b>9.5.5</b> Scaling</a></li>
<li class="chapter" data-level="9.5.6" data-path="local-regression.html"><a href="local-regression.html#demonstration"><i class="fa fa-check"></i><b>9.5.6</b> Demonstration</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="local-regression.html"><a href="local-regression.html#model-assumptions-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>9.6</b> Model assumptions and the bias/variance tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="reducible-error.html"><a href="reducible-error.html"><i class="fa fa-check"></i><b>10</b> Reducible Error</a><ul>
<li class="chapter" data-level="10.1" data-path="reducible-error.html"><a href="reducible-error.html#classification-exercise-do-together"><i class="fa fa-check"></i><b>10.1</b> Classification Exercise: Do Together</a></li>
<li class="chapter" data-level="10.2" data-path="reducible-error.html"><a href="reducible-error.html#training-error-vs.generalization-error"><i class="fa fa-check"></i><b>10.2</b> Training Error vs. Generalization Error</a></li>
<li class="chapter" data-level="10.3" data-path="reducible-error.html"><a href="reducible-error.html#model-complexity"><i class="fa fa-check"></i><b>10.3</b> Model complexity</a><ul>
<li class="chapter" data-level="10.3.1" data-path="reducible-error.html"><a href="reducible-error.html#activity"><i class="fa fa-check"></i><b>10.3.1</b> Activity</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="reducible-error.html"><a href="reducible-error.html#reducible-error-1"><i class="fa fa-check"></i><b>10.4</b> Reducible Error</a><ul>
<li class="chapter" data-level="10.4.1" data-path="reducible-error.html"><a href="reducible-error.html#what-is-it"><i class="fa fa-check"></i><b>10.4.1</b> What is it?</a></li>
<li class="chapter" data-level="10.4.2" data-path="reducible-error.html"><a href="reducible-error.html#example"><i class="fa fa-check"></i><b>10.4.2</b> Example</a></li>
<li class="chapter" data-level="10.4.3" data-path="reducible-error.html"><a href="reducible-error.html#bias-and-variance"><i class="fa fa-check"></i><b>10.4.3</b> Bias and Variance</a></li>
<li class="chapter" data-level="10.4.4" data-path="reducible-error.html"><a href="reducible-error.html#reducing-reducible-error"><i class="fa fa-check"></i><b>10.4.4</b> Reducing reducible error</a></li>
<li class="chapter" data-level="10.4.5" data-path="reducible-error.html"><a href="reducible-error.html#error-decomposition"><i class="fa fa-check"></i><b>10.4.5</b> Error decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>11</b> Model Selection</a><ul>
<li class="chapter" data-level="11.1" data-path="model-selection.html"><a href="model-selection.html#exercise-cv"><i class="fa fa-check"></i><b>11.1</b> Exercise: CV</a></li>
<li class="chapter" data-level="11.2" data-path="model-selection.html"><a href="model-selection.html#out-of-sample-error"><i class="fa fa-check"></i><b>11.2</b> Out-of-sample Error</a><ul>
<li class="chapter" data-level="11.2.1" data-path="model-selection.html"><a href="model-selection.html#the-fundamental-problem"><i class="fa fa-check"></i><b>11.2.1</b> The fundamental problem</a></li>
<li class="chapter" data-level="11.2.2" data-path="model-selection.html"><a href="model-selection.html#solution-1-use-a-hold-out-set."><i class="fa fa-check"></i><b>11.2.2</b> Solution 1: Use a hold-out set.</a></li>
<li class="chapter" data-level="11.2.3" data-path="model-selection.html"><a href="model-selection.html#solution-2-cross-validation"><i class="fa fa-check"></i><b>11.2.3</b> Solution 2: Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="model-selection.html"><a href="model-selection.html#alternative-measures-of-model-goodness"><i class="fa fa-check"></i><b>11.3</b> Alternative measures of model goodness</a></li>
<li class="chapter" data-level="11.4" data-path="model-selection.html"><a href="model-selection.html#feature-and-model-selection-setup"><i class="fa fa-check"></i><b>11.4</b> Feature and model selection: setup</a></li>
<li class="chapter" data-level="11.5" data-path="model-selection.html"><a href="model-selection.html#model-selection-1"><i class="fa fa-check"></i><b>11.5</b> Model selection</a></li>
<li class="chapter" data-level="11.6" data-path="model-selection.html"><a href="model-selection.html#feature-predictor-selection"><i class="fa fa-check"></i><b>11.6</b> Feature (predictor) selection</a><ul>
<li class="chapter" data-level="11.6.1" data-path="model-selection.html"><a href="model-selection.html#specialized-metrics-for-feature-selection"><i class="fa fa-check"></i><b>11.6.1</b> Specialized metrics for feature selection</a></li>
<li class="chapter" data-level="11.6.2" data-path="model-selection.html"><a href="model-selection.html#greedy-selection"><i class="fa fa-check"></i><b>11.6.2</b> Greedy Selection</a></li>
<li class="chapter" data-level="11.6.3" data-path="model-selection.html"><a href="model-selection.html#regularization"><i class="fa fa-check"></i><b>11.6.3</b> Regularization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="splines-and-loess-regression.html"><a href="splines-and-loess-regression.html"><i class="fa fa-check"></i><b>12</b> Splines and Loess Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="splines-and-loess-regression.html"><a href="splines-and-loess-regression.html#loess-1"><i class="fa fa-check"></i><b>12.1</b> Loess</a><ul>
<li class="chapter" data-level="12.1.1" data-path="splines-and-loess-regression.html"><a href="splines-and-loess-regression.html#the-moving-window"><i class="fa fa-check"></i><b>12.1.1</b> The “Moving Window”</a></li>
<li class="chapter" data-level="12.1.2" data-path="splines-and-loess-regression.html"><a href="splines-and-loess-regression.html#ggplot2"><i class="fa fa-check"></i><b>12.1.2</b> <code>ggplot2</code></a></li>
<li class="chapter" data-level="12.1.3" data-path="splines-and-loess-regression.html"><a href="splines-and-loess-regression.html#manual-method"><i class="fa fa-check"></i><b>12.1.3</b> Manual method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="model-fitting-in-r.html"><a href="model-fitting-in-r.html"><i class="fa fa-check"></i><b>13</b> Model fitting in R</a><ul>
<li class="chapter" data-level="13.1" data-path="model-fitting-in-r.html"><a href="model-fitting-in-r.html#broom-package"><i class="fa fa-check"></i><b>13.1</b> <code>broom</code> package</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>14</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-specification"><i class="fa fa-check"></i><b>14.1</b> Model Specification</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="linear-models-in-general.html"><a href="linear-models-in-general.html"><i class="fa fa-check"></i><b>15</b> Linear models in general</a></li>
<li class="chapter" data-level="16" data-path="reference-treatment-parameterization.html"><a href="reference-treatment-parameterization.html"><i class="fa fa-check"></i><b>16</b> reference-treatment parameterization</a><ul>
<li class="chapter" data-level="16.1" data-path="reference-treatment-parameterization.html"><a href="reference-treatment-parameterization.html#more-than-one-category-lab-2"><i class="fa fa-check"></i><b>16.1</b> More than one category (Lab 2)</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="non-identifiability-in-gams.html"><a href="non-identifiability-in-gams.html"><i class="fa fa-check"></i><b>17</b> Non-identifiability in GAMS</a><ul>
<li class="chapter" data-level="17.1" data-path="non-identifiability-in-gams.html"><a href="non-identifiability-in-gams.html#non-identifiability"><i class="fa fa-check"></i><b>17.1</b> Non-identifiability</a></li>
<li class="chapter" data-level="17.2" data-path="non-identifiability-in-gams.html"><a href="non-identifiability-in-gams.html#question-1b"><i class="fa fa-check"></i><b>17.2</b> Question 1b</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>18</b> ANOVA</a><ul>
<li class="chapter" data-level="18.1" data-path="anova.html"><a href="anova.html#the-types-of-parametric-assumptions"><i class="fa fa-check"></i><b>18.1</b> The types of parametric assumptions</a><ul>
<li class="chapter" data-level="18.1.1" data-path="anova.html"><a href="anova.html#when-defining-a-model-function."><i class="fa fa-check"></i><b>18.1.1</b> 1. When defining a <strong>model function</strong>.</a></li>
<li class="chapter" data-level="18.1.2" data-path="anova.html"><a href="anova.html#when-defining-probability-distributions."><i class="fa fa-check"></i><b>18.1.2</b> 2. When defining <strong>probability distributions</strong>.</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="anova.html"><a href="anova.html#the-value-of-making-parametric-assumptions"><i class="fa fa-check"></i><b>18.2</b> The value of making parametric assumptions</a><ul>
<li class="chapter" data-level="18.2.1" data-path="anova.html"><a href="anova.html#value-1-reduced-error"><i class="fa fa-check"></i><b>18.2.1</b> Value #1: Reduced Error</a></li>
<li class="chapter" data-level="18.2.2" data-path="anova.html"><a href="anova.html#value-2-interpretation"><i class="fa fa-check"></i><b>18.2.2</b> Value #2: Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="anova.html"><a href="anova.html#problems"><i class="fa fa-check"></i><b>18.3</b> Problems</a></li>
<li class="chapter" data-level="18.4" data-path="anova.html"><a href="anova.html#solutions"><i class="fa fa-check"></i><b>18.4</b> Solutions</a><ul>
<li class="chapter" data-level="18.4.1" data-path="anova.html"><a href="anova.html#solution-1-transformations"><i class="fa fa-check"></i><b>18.4.1</b> Solution 1: Transformations</a></li>
<li class="chapter" data-level="18.4.2" data-path="anova.html"><a href="anova.html#solution-2-link-functions"><i class="fa fa-check"></i><b>18.4.2</b> Solution 2: Link Functions</a></li>
<li class="chapter" data-level="18.4.3" data-path="anova.html"><a href="anova.html#solution-3-scientifically-backed-functions"><i class="fa fa-check"></i><b>18.4.3</b> Solution 3: Scientifically-backed functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="glms-in-r.html"><a href="glms-in-r.html"><i class="fa fa-check"></i><b>19</b> GLM’s in R</a><ul>
<li class="chapter" data-level="19.0.1" data-path="glms-in-r.html"><a href="glms-in-r.html#broomaugment"><i class="fa fa-check"></i><b>19.0.1</b> <code>broom::augment()</code></a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="logistic-regression-paper-with-paul.html"><a href="logistic-regression-paper-with-paul.html"><i class="fa fa-check"></i><b>20</b> Logistic Regression paper with Paul</a><ul>
<li class="chapter" data-level="20.1" data-path="logistic-regression-paper-with-paul.html"><a href="logistic-regression-paper-with-paul.html#the-traditional-approach"><i class="fa fa-check"></i><b>20.1</b> The Traditional Approach</a><ul>
<li class="chapter" data-level="20.1.1" data-path="logistic-regression-paper-with-paul.html"><a href="logistic-regression-paper-with-paul.html#the-linear-probability-model"><i class="fa fa-check"></i><b>20.1.1</b> The Linear Probability Model</a></li>
<li class="chapter" data-level="20.1.2" data-path="logistic-regression-paper-with-paul.html"><a href="logistic-regression-paper-with-paul.html#the-logistic-model"><i class="fa fa-check"></i><b>20.1.2</b> The Logistic Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="robust-regression-in-r.html"><a href="robust-regression-in-r.html"><i class="fa fa-check"></i><b>21</b> Robust Regression in R</a><ul>
<li class="chapter" data-level="21.0.1" data-path="robust-regression-in-r.html"><a href="robust-regression-in-r.html#heavy-tailed-regression"><i class="fa fa-check"></i><b>21.0.1</b> Heavy Tailed Regression</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="from-linear-regression-to-mixed-effects-models.html"><a href="from-linear-regression-to-mixed-effects-models.html"><i class="fa fa-check"></i><b>22</b> From Linear Regression to Mixed Effects Models</a><ul>
<li class="chapter" data-level="22.1" data-path="from-linear-regression-to-mixed-effects-models.html"><a href="from-linear-regression-to-mixed-effects-models.html#motivation-for-lme"><i class="fa fa-check"></i><b>22.1</b> Motivation for LME</a></li>
<li class="chapter" data-level="22.2" data-path="from-linear-regression-to-mixed-effects-models.html"><a href="from-linear-regression-to-mixed-effects-models.html#definition"><i class="fa fa-check"></i><b>22.2</b> Definition</a></li>
<li class="chapter" data-level="22.3" data-path="from-linear-regression-to-mixed-effects-models.html"><a href="from-linear-regression-to-mixed-effects-models.html#r-tools-for-fitting"><i class="fa fa-check"></i><b>22.3</b> R Tools for Fitting</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="mixed-effects-models-in-r-tutorial.html"><a href="mixed-effects-models-in-r-tutorial.html"><i class="fa fa-check"></i><b>23</b> Mixed Effects Models in R: tutorial</a></li>
<li class="chapter" data-level="24" data-path="dsci-562-tutorial-missing-data.html"><a href="dsci-562-tutorial-missing-data.html"><i class="fa fa-check"></i><b>24</b> DSCI 562 Tutorial: Missing Data</a><ul>
<li class="chapter" data-level="24.1" data-path="dsci-562-tutorial-missing-data.html"><a href="dsci-562-tutorial-missing-data.html#mean-imputation"><i class="fa fa-check"></i><b>24.1</b> Mean Imputation</a></li>
<li class="chapter" data-level="24.2" data-path="dsci-562-tutorial-missing-data.html"><a href="dsci-562-tutorial-missing-data.html#multiple-imputation"><i class="fa fa-check"></i><b>24.2</b> Multiple Imputation</a><ul>
<li class="chapter" data-level="24.2.1" data-path="dsci-562-tutorial-missing-data.html"><a href="dsci-562-tutorial-missing-data.html#patterns"><i class="fa fa-check"></i><b>24.2.1</b> Patterns</a></li>
<li class="chapter" data-level="24.2.2" data-path="dsci-562-tutorial-missing-data.html"><a href="dsci-562-tutorial-missing-data.html#multiple-imputation-1"><i class="fa fa-check"></i><b>24.2.2</b> Multiple Imputation</a></li>
<li class="chapter" data-level="24.2.3" data-path="dsci-562-tutorial-missing-data.html"><a href="dsci-562-tutorial-missing-data.html#pooling"><i class="fa fa-check"></i><b>24.2.3</b> Pooling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="25" data-path="spatial.html"><a href="spatial.html"><i class="fa fa-check"></i><b>25</b> Spatial</a><ul>
<li class="chapter" data-level="25.1" data-path="spatial.html"><a href="spatial.html#a-model-for-river-rock-size"><i class="fa fa-check"></i><b>25.1</b> A Model for River Rock Size</a></li>
<li class="chapter" data-level="25.2" data-path="spatial.html"><a href="spatial.html#statistical-objectives"><i class="fa fa-check"></i><b>25.2</b> Statistical Objectives</a><ul>
<li class="chapter" data-level="25.2.1" data-path="spatial.html"><a href="spatial.html#preliminaries-variance-and-correlation"><i class="fa fa-check"></i><b>25.2.1</b> Preliminaries: Variance and Correlation</a></li>
</ul></li>
<li class="chapter" data-level="25.3" data-path="spatial.html"><a href="spatial.html#three-concepts"><i class="fa fa-check"></i><b>25.3</b> Three Concepts</a><ul>
<li class="chapter" data-level="25.3.1" data-path="spatial.html"><a href="spatial.html#error-variance-sigma_e2leftxright"><i class="fa fa-check"></i><b>25.3.1</b> Error Variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span></a></li>
<li class="chapter" data-level="25.3.2" data-path="spatial.html"><a href="spatial.html#mean-variance-sigma_m2"><i class="fa fa-check"></i><b>25.3.2</b> Mean Variance <span class="math inline">\(\sigma_{M}^{2}\)</span></a></li>
<li class="chapter" data-level="25.3.3" data-path="spatial.html"><a href="spatial.html#mean-correlation-rholeftdright"><i class="fa fa-check"></i><b>25.3.3</b> Mean Correlation <span class="math inline">\(\rho\left(d\right)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="spatial.html"><a href="spatial.html#estimation-1"><i class="fa fa-check"></i><b>25.4</b> Estimation</a><ul>
<li class="chapter" data-level="25.4.1" data-path="spatial.html"><a href="spatial.html#constant-error-variance"><i class="fa fa-check"></i><b>25.4.1</b> Constant Error Variance</a></li>
<li class="chapter" data-level="25.4.2" data-path="spatial.html"><a href="spatial.html#non-constant-error-variance"><i class="fa fa-check"></i><b>25.4.2</b> Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="25.5" data-path="spatial.html"><a href="spatial.html#statistical-objective-1-downstream-fining-curve"><i class="fa fa-check"></i><b>25.5</b> Statistical Objective 1: Downstream Fining Curve</a><ul>
<li class="chapter" data-level="25.5.1" data-path="spatial.html"><a href="spatial.html#regression-form"><i class="fa fa-check"></i><b>25.5.1</b> Regression Form</a></li>
</ul></li>
<li class="chapter" data-level="25.6" data-path="spatial.html"><a href="spatial.html#statistical-objective-2-river-profile"><i class="fa fa-check"></i><b>25.6</b> Statistical Objective 2: River Profile</a><ul>
<li class="chapter" data-level="25.6.1" data-path="spatial.html"><a href="spatial.html#simple-kriging"><i class="fa fa-check"></i><b>25.6.1</b> Simple Kriging</a></li>
<li class="chapter" data-level="25.6.2" data-path="spatial.html"><a href="spatial.html#universal-kriging"><i class="fa fa-check"></i><b>25.6.2</b> Universal Kriging</a></li>
<li class="chapter" data-level="25.6.3" data-path="spatial.html"><a href="spatial.html#kriging-under-non-constant-error-variance"><i class="fa fa-check"></i><b>25.6.3</b> Kriging under Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="25.7" data-path="spatial.html"><a href="spatial.html#confidence-intervals-of-the-river-profile"><i class="fa fa-check"></i><b>25.7</b> Confidence Intervals of the River Profile</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="timeseries-in-base-r.html"><a href="timeseries-in-base-r.html"><i class="fa fa-check"></i><b>26</b> Timeseries in (base) R</a></li>
<li class="divider"></li>
<li><a href="https://github.com/vincenzocoia/Interpreting-Regression" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpreting Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="local-regression" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Local Regression</h1>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<p>(BAIT 509 Class Meeting 03)</p>
<p>Let’s turn our attention to the first “new” machine learning methods of the course: <span class="math inline">\(k\)</span> <strong>Nearest Neighbours</strong> (aka kNN or <span class="math inline">\(k\)</span>-NN) and <strong>loess</strong> (aka “LOcal regrESSion”).</p>
<p>The fundamental idea behind these methods is to <em>base your prediction on what happened in similar cases in the past</em>.</p>
<div id="knn" class="section level2">
<h2><span class="header-section-number">9.1</span> kNN</h2>
<p>Pick a positive integer <span class="math inline">\(k\)</span>.</p>
<p>To make a prediction of the response at a particular observation of the predictors (I’ll call this the <strong>query point</strong>) – that is, when <span class="math inline">\(X_1=x_1\)</span>, …, <span class="math inline">\(X_p=x_p\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Subset your data to <span class="math inline">\(k\)</span> observations (rows) whose values of the predictors <span class="math inline">\((X_1, \ldots, X_p)\)</span> are closest to <span class="math inline">\((x_1,\ldots,x_p)\)</span>.</li>
<li>For kNN classificiation, use the “most popular vote” (i.e., the modal category) of the subsetted observations. For kNN regression, use the average <span class="math inline">\(Y\)</span> of the remaining subsetted observations.</li>
</ol>
<p>Recall how to calculate distance between two vectors <span class="math inline">\((a_1, \ldots, a_p)\)</span> and <span class="math inline">\((b_1, \ldots, b_p)\)</span>: <span class="math display">\[ \text{distance} = \sqrt{(a_1-b_1)^2 + \cdots + (a_p-b_p)^2}. \]</span> It’s even easier when there’s one predictor: it’s just the absolute value of the difference.</p>
</div>
<div id="loess" class="section level2">
<h2><span class="header-section-number">9.2</span> loess</h2>
<p>(This is actually the simplest version of loess, sometimes called a <strong>moving window</strong> approach. We’ll get to the “full” loess).</p>
<p>Pick a positive number <span class="math inline">\(r\)</span> (not necessarily integer).</p>
<p>To make a prediction of the response at a query point (that is, a particular observation of the predictors, <span class="math inline">\(X_1=x_1\)</span>, …, <span class="math inline">\(X_p=x_p\)</span>):</p>
<ol style="list-style-type: decimal">
<li>Subset your data to those observations (rows) having values of the predictors <span class="math inline">\((X_1,\ldots,X_p)\)</span> within <span class="math inline">\(r\)</span> units of <span class="math inline">\((x_1,\ldots,x_p)\)</span>.</li>
<li>For kNN classificiation, use the “most popular vote” (i.e., the modal category) of the subsetted observations. For kNN regression, use the average <span class="math inline">\(Y\)</span> of the remaining subsetted observations.</li>
</ol>
<p>Notice that Step 2 is the same as in kNN.</p>
<p><span class="math inline">\(k\)</span> and <span class="math inline">\(r\)</span> are called <strong>hyperparameters</strong>, because we don’t estimate them – we choose them outright.</p>
</div>
<div id="in-class-exercises" class="section level2">
<h2><span class="header-section-number">9.3</span> In-Class Exercises</h2>
<p>Consider the following data set, given by <code>dat</code>. Here’s the top six rows of data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">87</span>)
dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>(<span class="dv">100</span>), <span class="kw">rnorm</span>(<span class="dv">100</span>)<span class="op">+</span><span class="dv">5</span>)<span class="op">-</span><span class="dv">3</span>,
              <span class="dt">y =</span> <span class="kw">sin</span>(x<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">5</span>)<span class="op">/</span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>)<span class="op">/</span><span class="dv">10</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))</code></pre></div>
<p>Here’s a scatterplot of the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(x,y)) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">colour=</span>my_accent) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>() <span class="op">+</span><span class="st"> </span>
<span class="st">    </span>rotate_y</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<div id="exercise-1-mean-at-x0" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Exercise 1: Mean at <span class="math inline">\(X=0\)</span></h3>
<p>Let’s check your understanding of loess and kNN. Consider estimating the mean of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X=0\)</span> by using data whose <span class="math inline">\(X\)</span> values are near 0.</p>
<ol style="list-style-type: decimal">
<li><p>Eyeball the above scatterplot of the data. What would you say is a reasonable estimate of the mean of <span class="math inline">\(Y\)</span> at <span class="math inline">\(X=0\)</span>? Why?</p></li>
<li>Estimate using loess and kNN (you choose the hyperparameters).
<ol style="list-style-type: decimal">
<li>Hints for kNN:
<ul>
<li>First, add a new column in the data that stores the <em>distance</em> between <span class="math inline">\(X=0\)</span> and each observation. If that column is named <code>d</code>, you can do this with the following partial code: <code>dat$d &lt;- YOUR_CALCULATION_HERE</code>. Recall that <code>dat$x</code> is a vector of the <code>x</code> column.</li>
<li>Then, arrange the data from smallest distance to largest with <code>arrange(dat)</code> (you’ll need to load the <code>tidyverse</code> package first), and subset <em>that</em> to the first <span class="math inline">\(k\)</span> rows.</li>
</ul></li>
<li>Hints for loess:
<ul>
<li>Subset the data using the <code>filter</code> function. The condition to filter on: you want to keep rows whose distances (<code>d</code>) are …</li>
</ul></li>
</ol></li>
</ol>
<pre><code>k &lt;- 10
r &lt;- 0.5
x0 &lt;- 0
dat$dist &lt;- FILL_THIS_IN
dat &lt;- arrange(dat, dist)  # sort by distance
kNN_prediction &lt;- FILL_THIS_IN
loess_prediction &lt;- FILL_THIS_IN</code></pre>
<ol start="3" style="list-style-type: decimal">
<li><p>What happens when you try to pick an <span class="math inline">\(r\)</span> that is way too small? Say, <span class="math inline">\(r=0.01\)</span>? Why?</p></li>
<li><p>There’s a tradeoff between choosing large and small values of either hyperparameter. What’s good and what’s bad about choosing a large value? What about small values?</p></li>
</ol>
</div>
<div id="exercise-2-regression-curve" class="section level3">
<h3><span class="header-section-number">9.3.2</span> Exercise 2: Regression Curve</h3>
<p>Form the <strong>regression curve</strong> / <strong>model function</strong> by doing the estimation over a grid of x values, and connecting the dots:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xgrid &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">4</span>, <span class="dt">length.out=</span><span class="dv">1000</span>)
k &lt;-<span class="st"> </span><span class="dv">10</span>
r &lt;-<span class="st"> </span><span class="fl">0.5</span>
kNN_estimates &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(xgrid, <span class="cf">function</span>(x_){
    dat <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">        </span><span class="kw">mutate</span>(<span class="dt">d =</span> <span class="kw">abs</span>(x<span class="op">-</span>x_)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">        </span><span class="kw">arrange</span>(d) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">        </span><span class="kw">summarize</span>(<span class="dt">yhat=</span><span class="kw">mean</span>(y[<span class="dv">1</span><span class="op">:</span>k])) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">        `</span><span class="dt">[[</span><span class="st">`</span>(<span class="st">&quot;yhat&quot;</span>)
})
loess_estimates &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(xgrid, <span class="cf">function</span>(x_){
    dat <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">        </span><span class="kw">mutate</span>(<span class="dt">d =</span> <span class="kw">abs</span>(x<span class="op">-</span>x_)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">        </span><span class="kw">filter</span>(d<span class="op">&lt;</span>r) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">        </span><span class="kw">summarize</span>(<span class="dt">yhat=</span><span class="kw">mean</span>(y)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">        `</span><span class="dt">[[</span><span class="st">`</span>(<span class="st">&quot;yhat&quot;</span>)
})
est &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x=</span>xgrid, <span class="dt">kNN=</span>kNN_estimates, <span class="dt">loess=</span>loess_estimates) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">gather</span>(<span class="dt">key=</span><span class="st">&quot;method&quot;</span>, <span class="dt">value=</span><span class="st">&quot;estimate&quot;</span>, kNN, loess)
<span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">data=</span>dat, <span class="dt">mapping=</span><span class="kw">aes</span>(x,y)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data=</span>est, 
              <span class="dt">mapping=</span><span class="kw">aes</span>(x,estimate, <span class="dt">group=</span>method, <span class="dt">colour=</span>method),
              <span class="dt">size=</span><span class="dv">1</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p><strong>Exercises</strong>:</p>
<ul>
<li>Play with different values of <span class="math inline">\(k\)</span> and <span class="math inline">\(r\)</span>, and regenerate the plot each time. What effect does increasing these values have on the regression curve? What about decreasing? What would you say is a “good” choice of <span class="math inline">\(k\)</span> and <span class="math inline">\(r\)</span>, and why?</li>
<li>What happens when you choose <span class="math inline">\(k=n=200\)</span>? What happens if you choose <span class="math inline">\(r=10\)</span> or bigger?</li>
</ul>
<p>The phenomenon you see when <span class="math inline">\(k\)</span> and <span class="math inline">\(r\)</span> are very small is called <strong>overfitting</strong>. This means that your model displays patterns that are not actually present. <strong>Underfitting</strong>, on the other hand, is when your model misses patterns in the data that are actually present.</p>
</div>
</div>
<div id="hyperparameters-and-the-biasvariance-tradeoff" class="section level2">
<h2><span class="header-section-number">9.4</span> Hyperparameters and the bias/variance tradeoff</h2>
<p>Let’s look at the bias and variance for different values of the hyperparameter in loess.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">400</span>)
N &lt;-<span class="st"> </span><span class="dv">100</span>
xgrid &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">7</span>, <span class="fl">6.5</span>, <span class="dt">length.out=</span><span class="dv">300</span>)
true_mean &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(<span class="cf">function</span>(x){
    <span class="cf">if</span> (x<span class="op">==</span><span class="dv">0</span>) <span class="kw">return</span>(<span class="kw">exp</span>(<span class="dv">1</span>)) <span class="cf">else</span> <span class="kw">return</span>(<span class="kw">sin</span>(x<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">5</span>)<span class="op">/</span>x <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
})
## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop
<span class="kw">expand.grid</span>(<span class="dt">iter=</span><span class="dv">1</span><span class="op">:</span>N, <span class="dt">r=</span><span class="kw">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(iter, r) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">do</span>({
    this_r &lt;-<span class="st"> </span><span class="kw">unique</span>(.<span class="op">$</span>r)
    dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>(<span class="dv">100</span>), <span class="kw">rnorm</span>(<span class="dv">100</span>)<span class="op">+</span><span class="dv">5</span>)<span class="op">-</span><span class="dv">3</span>,
                  <span class="dt">y =</span> <span class="kw">sin</span>(x<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">5</span>)<span class="op">/</span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>)<span class="op">/</span><span class="dv">10</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
    <span class="kw">data.frame</span>(
        .,
        <span class="dt">x =</span> xgrid,
        <span class="dt">yhat =</span> <span class="kw">ksmooth</span>(dat<span class="op">$</span>x, dat<span class="op">$</span>y, <span class="dt">kernel=</span><span class="st">&quot;box&quot;</span>, 
                         <span class="dt">bandwidth=</span>this_r,
                         <span class="dt">x.points=</span>xgrid)<span class="op">$</span>y
    )
}) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">r=</span><span class="kw">paste0</span>(<span class="st">&quot;bandwidth=&quot;</span>, r)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>yhat)) <span class="op">+</span>
<span class="st">    </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>r, <span class="dt">ncol=</span><span class="dv">1</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group=</span>iter, <span class="dt">colour=</span><span class="st">&quot;Estimates&quot;</span>), <span class="dt">alpha=</span><span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun=</span>true_mean,
                  <span class="dt">mapping=</span><span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;True mean&quot;</span>)) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">scale_colour_brewer</span>(<span class="st">&quot;&quot;</span>, <span class="dt">palette=</span><span class="st">&quot;Dark2&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;y&quot;</span>) <span class="op">+</span><span class="st"> </span>rotate_y</code></pre></div>
<pre><code>## Warning: Removed 3240 rows containing missing values (geom_path).</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
<p>You can see the bias/variance tradeoff here:</p>
<ul>
<li>Notice that the estimates get <em>narrower</em> as the bandwidth increases – this means the variance reduces as the bandwidth increases.</li>
<li>Notice that the estimates become biased as the bandwidth increases.</li>
</ul>
<p>A similar phenomenon exists with kNN regression.</p>
<p>Notice some other things about these plots:</p>
<ul>
<li>There’s more variance whenever there’s less data – that’s at the tails, and (by design) at around <span class="math inline">\(X=0\)</span>.</li>
<li>Estimates don’t exist sometimes, if no data fall in the “window”. You can see that the tails are cut short when the bandwidth is small.</li>
</ul>
</div>
<div id="extensions-to-knn-and-loess" class="section level2">
<h2><span class="header-section-number">9.5</span> Extensions to kNN and loess</h2>
<div id="kernel-weighting" class="section level3">
<h3><span class="header-section-number">9.5.1</span> Kernel weighting</h3>
<p>kNN and loess can be generalized by downweighing points that are further from the query point. In particular, we take a weighted average.</p>
<p>Suppose <span class="math inline">\(y_1, \ldots, y_n\)</span> are <span class="math inline">\(n\)</span> realizations of the response <span class="math inline">\(Y\)</span>. If we assign (respective) weights <span class="math inline">\(w_1, \ldots, w_n\)</span> to these realizations, then the <strong>weighted average</strong> is <span class="math display">\[ \frac{\sum_{i=1}^n w_i y_i}{\sum_{i=1}^n w_i}. \]</span></p>
<p>If the response is categorical, then each subsetted observation gives a “weighted vote”. Sum up the weights corresponding to each category to obtain the total “number” of votes.</p>
<p>We obtain these weights using a <strong>kernel function</strong>. A kernel function is any non-increasing function over the positive real numbers. Plug in the distance between the observed predictor(s) and the query point to obtain the weight. Some examples of kernel functions are plotted below.</p>
</div>
<div id="local-polynomials" class="section level3">
<h3><span class="header-section-number">9.5.2</span> Local polynomials</h3>
<p>Another extension of loess is to consider <strong>local polynomials</strong>. The idea here is, after subsetting the data lying within <span class="math inline">\(r\)</span> units of the query point, add the following two steps:</p>
<ol style="list-style-type: decimal">
<li>Fit a linear (or quadratic) regression model to the subsetted data only.
<ul>
<li>This is the “local polynomial”. You can think of this as like a “mini linear regression”.</li>
</ul></li>
<li>Obtain your prediction by evaluating the regression curve at the query point.</li>
<li>Throw away your fitted local polynomial.</li>
</ol>
<p>OK, the 3rd step isn’t really a true step, but I like to include it to emphasize that we only evaluate the local polynomial at the query point.</p>
<p>Note:</p>
<ul>
<li>We <em>could</em> fit higher order polynomials, but that tends to overfit the data.</li>
<li>We <em>could</em> fit any other curve locally besides a polynomial, but polynomials are justified by the Taylor approximation.</li>
<li>Local polynomials with degree=0 is the same as “not doing” local polynomials.</li>
</ul>
</div>
<div id="combination" class="section level3">
<h3><span class="header-section-number">9.5.3</span> Combination</h3>
<p>You can combine kernel weighting with local polynomials. When you fit the local polynomial to the subsetted data, you can run a <em>weighted</em> regression. Instead of minimizing the sum of squared errors, we minimize the <em>weighted</em> sum of squared errors.</p>
</div>
<div id="other-distances" class="section level3">
<h3><span class="header-section-number">9.5.4</span> Other distances</h3>
<p>We don’t have to use the “usual” notion of distance. The formula I gave you earlier (above) is called the <em>Euclidean distance</em>, or L2 norms. There’s also the L1 norm (also called the manhattan distance, which is distance by moving along the axes/rectangularly) and L0 norm (number of predictors having non-zero univariate distance).</p>
</div>
<div id="scaling" class="section level3">
<h3><span class="header-section-number">9.5.5</span> Scaling</h3>
<p>When you’re using two or more predictors, your predictors might be on different scales. This means distances aren’t weighed equally, depending on the direction. Instead of measuring distance on the original scale of the predictors, consider re-scaling the predictors by subtracting the mean and dividing by the standard deviation for each predictor.</p>
</div>
<div id="demonstration" class="section level3">
<h3><span class="header-section-number">9.5.6</span> Demonstration</h3>
<p>Let’s look at the same example, but with kernel downweighting and local polynomials.</p>
<p>Warning! The “bandwidth” hyperparameter in this plot is parameterized differently than in the previous plot, but carries the same interpretation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">400</span>)
N &lt;-<span class="st"> </span><span class="dv">100</span>
xgrid &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">7</span>, <span class="fl">6.5</span>, <span class="dt">length.out=</span><span class="dv">300</span>)
true_mean &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(<span class="cf">function</span>(x){
    <span class="cf">if</span> (x<span class="op">==</span><span class="dv">0</span>) <span class="kw">return</span>(<span class="kw">exp</span>(<span class="dv">1</span>)) <span class="cf">else</span> <span class="kw">return</span>(<span class="kw">sin</span>(x<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">5</span>)<span class="op">/</span>x <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
})
## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop
<span class="kw">expand.grid</span>(<span class="dt">iter=</span><span class="dv">1</span><span class="op">:</span>N, <span class="dt">r=</span><span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>), <span class="dt">d=</span><span class="dv">0</span><span class="op">:</span><span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">group_by</span>(iter, r, d) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">do</span>({
    this_r &lt;-<span class="st"> </span><span class="kw">unique</span>(.<span class="op">$</span>r)
    this_d &lt;-<span class="st"> </span><span class="kw">unique</span>(.<span class="op">$</span>d)
    dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>(<span class="dv">100</span>), <span class="kw">rnorm</span>(<span class="dv">100</span>)<span class="op">+</span><span class="dv">5</span>)<span class="op">-</span><span class="dv">3</span>,
                  <span class="dt">y =</span> <span class="kw">sin</span>(x<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="dv">5</span>)<span class="op">/</span>x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>)<span class="op">/</span><span class="dv">10</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
    <span class="kw">data.frame</span>(
        .,
        <span class="dt">x =</span> xgrid,
        <span class="dt">yhat =</span> <span class="kw">predict</span>(<span class="kw">loess</span>(y<span class="op">~</span>x, <span class="dt">data=</span>dat, 
                             <span class="dt">span=</span>this_r, <span class="dt">degree=</span>this_d),
                       <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">x=</span>xgrid))
    )
}) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">r=</span><span class="kw">paste0</span>(<span class="st">&quot;bandwidth=&quot;</span>, r),
           <span class="dt">d=</span><span class="kw">paste0</span>(<span class="st">&quot;degree=&quot;</span>, d)) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>yhat)) <span class="op">+</span>
<span class="st">    </span><span class="kw">facet_grid</span>(r <span class="op">~</span><span class="st"> </span>d) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group=</span>iter, <span class="dt">colour=</span><span class="st">&quot;Estimates&quot;</span>), <span class="dt">alpha=</span><span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun=</span>true_mean,
                  <span class="dt">mapping=</span><span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;True mean&quot;</span>)) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">scale_colour_brewer</span>(<span class="st">&quot;&quot;</span>, <span class="dt">palette=</span><span class="st">&quot;Dark2&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;y&quot;</span>) <span class="op">+</span><span class="st"> </span>rotate_y</code></pre></div>
<pre><code>## Warning: Removed 8034 rows containing missing values (geom_path).</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-38-1.png" width="960" /></p>
<p>Notice:</p>
<ul>
<li>For small bandwidth, increasing the degree of the poynomial just results in more variance – degree=0 looks best for this bandwidth.</li>
<li>But by increasing the degree (inc. variance, dec. bias) <em>and</em> increasing the bandwidth (dec. variance, inc. bias), we end up getting an overall better fit: low bias, low variance. Bandwidth=0.5 and Degree=2 here seem to work best.</li>
</ul>
</div>
</div>
<div id="model-assumptions-and-the-biasvariance-tradeoff" class="section level2">
<h2><span class="header-section-number">9.6</span> Model assumptions and the bias/variance tradeoff</h2>
<p>Recall that we saw an incorrect model assumption leads to bias, such as fitting linear regression when the true mean is non-linear.</p>
<p>When you make model assumptions that are <em>close to the truth</em>, then this has the effect of <em>decreasing variance</em>.</p>
<p>Adding good model assumptions is like adding more data – after all data is information, and a model assumption is also information.</p>
<p>Here’s a demonstration:</p>
<p>Consider <span class="math display">\[ Y = X + \varepsilon, \]</span> where <span class="math inline">\(X\)</span> (predictor) is N(0,1), and <span class="math inline">\(\varepsilon\)</span> (error term) is also N(0,1) (both are independent).</p>
<p>I’ll generate a sample of size 100, 100 times. For each sample, I’ll fit a linear regression model and a loess model. Here are the resulting 100 regression curves for each (the dashed line is the true mean):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">474</span>)
n &lt;-<span class="st"> </span><span class="dv">100</span>
N &lt;-<span class="st"> </span><span class="dv">100</span>
xgrid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>, <span class="dt">length.out=</span><span class="dv">100</span>))
## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop
<span class="kw">tibble</span>(<span class="dt">iter=</span><span class="dv">1</span><span class="op">:</span>N) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(iter) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">do</span>({
    dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x=</span><span class="kw">rnorm</span>(n), 
                  <span class="dt">y=</span>x<span class="op">+</span><span class="kw">rnorm</span>(n))
    <span class="kw">data.frame</span>(
        .,
        xgrid,
        <span class="dt">Local  =</span> <span class="kw">predict</span>(<span class="kw">loess</span>(y<span class="op">~</span>x, <span class="dt">data=</span>dat),
                         <span class="dt">newdata=</span>xgrid),
        <span class="dt">Linear =</span> <span class="kw">predict</span>(<span class="kw">lm</span>(y<span class="op">~</span>x, <span class="dt">data=</span>dat),
                         <span class="dt">newdata=</span>xgrid)
    )
}) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">gather</span>(<span class="dt">key=</span><span class="st">&quot;method&quot;</span>, <span class="dt">value=</span><span class="st">&quot;Prediction&quot;</span>, Local, Linear) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>Prediction)) <span class="op">+</span>
<span class="st">    </span><span class="kw">facet_wrap</span>(<span class="op">~</span><span class="st"> </span>method) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group=</span>iter), <span class="dt">colour=</span>my_accent, <span class="dt">alpha=</span><span class="fl">0.1</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">intercept=</span><span class="dv">0</span>, <span class="dt">slope=</span><span class="dv">1</span>, <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<pre><code>## Warning: Removed 1869 rows containing missing values (geom_path).</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-39-1.png" width="672" /></p>
<p>Notice that the local method has higher variance than the linear regression method.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-in-the-context-of-problem-solving.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="reducible-error.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/vincenzocoia/Interpreting-Regression/edit/master/050-local_ML.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
