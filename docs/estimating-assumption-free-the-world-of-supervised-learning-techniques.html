<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 11 Estimating assumption-free: the world of supervised learning techniques | Interpreting Regression</title>
  <meta name="description" content="A book about the why of regression to help you make decisions about your analysis.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 11 Estimating assumption-free: the world of supervised learning techniques | Interpreting Regression" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book about the why of regression to help you make decisions about your analysis." />
  <meta name="github-repo" content="vincenzocoia/Interpreting-Regression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Estimating assumption-free: the world of supervised learning techniques | Interpreting Regression" />
  
  <meta name="twitter:description" content="A book about the why of regression to help you make decisions about your analysis." />
  

<meta name="author" content="Vincenzo Coia">


<meta name="date" content="2019-08-09">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="estimating-parametric-model-functions.html">
<link rel="next" href="overfitting-the-problem-with-adding-too-many-parameters.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpreting Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#caution"><i class="fa fa-check"></i><b>1.1</b> Caution</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#purpose-of-the-book"><i class="fa fa-check"></i><b>1.2</b> Purpose of the book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#tasks-that-motivate-regression"><i class="fa fa-check"></i><b>1.3</b> Tasks that motivate Regression</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html"><i class="fa fa-check"></i><b>2</b> Regression in the context of problem solving</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#communicating-distillation-3-4"><i class="fa fa-check"></i><b>2.1</b> Communicating (Distillation 3-4)</a></li>
<li class="chapter" data-level="2.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#modelling-distillation-2-3"><i class="fa fa-check"></i><b>2.2</b> Modelling (Distillation 2-3)</a></li>
<li class="chapter" data-level="2.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#asking-useful-statistical-questions-distillation-1-2"><i class="fa fa-check"></i><b>2.3</b> Asking useful statistical questions (Distillation 1-2)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#business-objectives-examples"><i class="fa fa-check"></i><b>2.3.1</b> Business objectives: examples</a></li>
<li class="chapter" data-level="2.3.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-refining"><i class="fa fa-check"></i><b>2.3.2</b> Statistical objectives: refining</a></li>
<li class="chapter" data-level="2.3.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-examples"><i class="fa fa-check"></i><b>2.3.3</b> Statistical objectives: examples</a></li>
<li class="chapter" data-level="2.3.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-questions-are-not-the-full-picture"><i class="fa fa-check"></i><b>2.3.4</b> Statistical questions are not the full picture</a></li>
<li class="chapter" data-level="2.3.5" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-unrelated-to-supervised-learning"><i class="fa fa-check"></i><b>2.3.5</b> Statistical objectives unrelated to supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#prerequisites-to-an-analysis"><i class="fa fa-check"></i><b>2.4</b> Prerequisites to an analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="an-outcome-on-its-own.html"><a href="an-outcome-on-its-own.html"><i class="fa fa-check"></i>An outcome on its own</a></li>
<li class="chapter" data-level="3" data-path="distributions-uncertainty-is-worth-explaining.html"><a href="distributions-uncertainty-is-worth-explaining.html"><i class="fa fa-check"></i><b>3</b> Distributions: Uncertainty is worth explaining</a></li>
<li class="chapter" data-level="4" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html"><i class="fa fa-check"></i><b>4</b> Explaining an uncertain outcome: interpretable quantities</a><ul>
<li class="chapter" data-level="4.1" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#probabilistic-quantities"><i class="fa fa-check"></i><b>4.1</b> Probabilistic Quantities</a></li>
<li class="chapter" data-level="4.2" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#what-is-the-mean-anyway"><i class="fa fa-check"></i><b>4.2</b> What is the mean, anyway?</a></li>
<li class="chapter" data-level="4.3" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#quantiles"><i class="fa fa-check"></i><b>4.3</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>5</b> Estimation</a><ul>
<li class="chapter" data-level="5.1" data-path="estimation.html"><a href="estimation.html#estimation-of-probabilistic-quantities"><i class="fa fa-check"></i><b>5.1</b> Estimation of Probabilistic Quantities</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html"><i class="fa fa-check"></i><b>6</b> Parametric Families of Distributions</a><ul>
<li class="chapter" data-level="6.1" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#parametric-families-of-distributions-1"><i class="fa fa-check"></i><b>6.1</b> Parametric Families of Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#analyses-under-a-distributional-assumption"><i class="fa fa-check"></i><b>6.2</b> Analyses under a Distributional Assumption</a><ul>
<li class="chapter" data-level="6.2.1" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>6.2.1</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="6.2.2" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#usefulness-in-practice"><i class="fa fa-check"></i><b>6.2.2</b> Usefulness in Practice</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html"><i class="fa fa-check"></i>Prediction: harnessing the signal</a></li>
<li class="chapter" data-level="7" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html"><i class="fa fa-check"></i><b>7</b> Reducing uncertainty of the outcome: including predictors</a><ul>
<li class="chapter" data-level="7.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-terminology"><i class="fa fa-check"></i><b>7.1</b> Variable terminology</a><ul>
<li class="chapter" data-level="7.1.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-types"><i class="fa fa-check"></i><b>7.1.1</b> Variable types</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#irreducible-error"><i class="fa fa-check"></i><b>7.2</b> Irreducible Error</a></li>
<li class="chapter" data-level="7.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#in-class-exercises-irreducible-error"><i class="fa fa-check"></i><b>7.3</b> In-class Exercises: Irreducible Error</a><ul>
<li class="chapter" data-level="7.3.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-regression"><i class="fa fa-check"></i><b>7.3.1</b> Oracle regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-classification"><i class="fa fa-check"></i><b>7.3.2</b> Oracle classification</a></li>
<li class="chapter" data-level="7.3.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-random-prediction"><i class="fa fa-check"></i><b>7.3.3</b> (BONUS) Random prediction</a></li>
<li class="chapter" data-level="7.3.4" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-a-more-non-standard-regression"><i class="fa fa-check"></i><b>7.3.4</b> (BONUS) A more non-standard regression</a></li>
<li class="chapter" data-level="7.3.5" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-oracle-mse"><i class="fa fa-check"></i><b>7.3.5</b> (BONUS) Oracle MSE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html"><i class="fa fa-check"></i><b>8</b> The signal: model functions</a><ul>
<li class="chapter" data-level="8.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#linear-quantile-regression"><i class="fa fa-check"></i><b>8.1</b> Linear Quantile Regression</a><ul>
<li class="chapter" data-level="8.1.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#exercise"><i class="fa fa-check"></i><b>8.1.1</b> Exercise</a></li>
<li class="chapter" data-level="8.1.2" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-crossing-quantiles"><i class="fa fa-check"></i><b>8.1.2</b> Problem: Crossing quantiles</a></li>
<li class="chapter" data-level="8.1.3" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-upper-quantiles"><i class="fa fa-check"></i><b>8.1.3</b> Problem: Upper quantiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html"><i class="fa fa-check"></i><b>9</b> The Model-Fitting Paradigm in R</a><ul>
<li class="chapter" data-level="9.1" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html#broom-package"><i class="fa fa-check"></i><b>9.1</b> <code>broom</code> package</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html"><i class="fa fa-check"></i><b>10</b> Estimating parametric model functions</a><ul>
<li class="chapter" data-level="10.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#writing-the-sample-mean-as-an-optimization-problem"><i class="fa fa-check"></i><b>10.1</b> Writing the sample mean as an optimization problem</a></li>
<li class="chapter" data-level="10.2" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#evaluating-model-goodness-quantiles"><i class="fa fa-check"></i><b>10.2</b> Evaluating Model Goodness: Quantiles</a></li>
<li class="chapter" data-level="10.3" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#simple-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="10.3.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#model-specification"><i class="fa fa-check"></i><b>10.3.1</b> Model Specification</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#linear-models-in-general"><i class="fa fa-check"></i><b>10.4</b> Linear models in general</a></li>
<li class="chapter" data-level="10.5" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#reference-treatment-parameterization"><i class="fa fa-check"></i><b>10.5</b> reference-treatment parameterization</a><ul>
<li class="chapter" data-level="10.5.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#more-than-one-category-lab-2"><i class="fa fa-check"></i><b>10.5.1</b> More than one category (Lab 2)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><i class="fa fa-check"></i><b>11</b> Estimating assumption-free: the world of supervised learning techniques</a><ul>
<li class="chapter" data-level="11.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#what-machine-learning-is"><i class="fa fa-check"></i><b>11.1</b> What machine learning is</a></li>
<li class="chapter" data-level="11.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#types-of-supervised-learning"><i class="fa fa-check"></i><b>11.2</b> Types of Supervised Learning</a></li>
<li class="chapter" data-level="11.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#local-regression"><i class="fa fa-check"></i><b>11.3</b> Local Regression</a><ul>
<li class="chapter" data-level="11.3.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#knn"><i class="fa fa-check"></i><b>11.3.1</b> kNN</a></li>
<li class="chapter" data-level="11.3.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess"><i class="fa fa-check"></i><b>11.3.2</b> loess</a></li>
<li class="chapter" data-level="11.3.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#in-class-exercises"><i class="fa fa-check"></i><b>11.3.3</b> In-Class Exercises</a></li>
<li class="chapter" data-level="11.3.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#hyperparameters-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>11.3.4</b> Hyperparameters and the bias/variance tradeoff</a></li>
<li class="chapter" data-level="11.3.5" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#extensions-to-knn-and-loess"><i class="fa fa-check"></i><b>11.3.5</b> Extensions to kNN and loess</a></li>
<li class="chapter" data-level="11.3.6" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#model-assumptions-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>11.3.6</b> Model assumptions and the bias/variance tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#splines-and-loess-regression"><i class="fa fa-check"></i><b>11.4</b> Splines and Loess Regression</a><ul>
<li class="chapter" data-level="11.4.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess-1"><i class="fa fa-check"></i><b>11.4.1</b> Loess</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html"><i class="fa fa-check"></i><b>12</b> Overfitting: The problem with adding too many parameters</a><ul>
<li class="chapter" data-level="12.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#classification-exercise-do-together"><i class="fa fa-check"></i><b>12.1</b> Classification Exercise: Do Together</a></li>
<li class="chapter" data-level="12.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#training-error-vs.generalization-error"><i class="fa fa-check"></i><b>12.2</b> Training Error vs. Generalization Error</a></li>
<li class="chapter" data-level="12.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-complexity"><i class="fa fa-check"></i><b>12.3</b> Model complexity</a><ul>
<li class="chapter" data-level="12.3.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#activity"><i class="fa fa-check"></i><b>12.3.1</b> Activity</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducible-error"><i class="fa fa-check"></i><b>12.4</b> Reducible Error</a><ul>
<li class="chapter" data-level="12.4.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#what-is-it"><i class="fa fa-check"></i><b>12.4.1</b> What is it?</a></li>
<li class="chapter" data-level="12.4.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#example"><i class="fa fa-check"></i><b>12.4.2</b> Example</a></li>
<li class="chapter" data-level="12.4.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#bias-and-variance"><i class="fa fa-check"></i><b>12.4.3</b> Bias and Variance</a></li>
<li class="chapter" data-level="12.4.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducing-reducible-error"><i class="fa fa-check"></i><b>12.4.4</b> Reducing reducible error</a></li>
<li class="chapter" data-level="12.4.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#error-decomposition"><i class="fa fa-check"></i><b>12.4.5</b> Error decomposition</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection"><i class="fa fa-check"></i><b>12.5</b> Model Selection</a><ul>
<li class="chapter" data-level="12.5.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#exercise-cv"><i class="fa fa-check"></i><b>12.5.1</b> Exercise: CV</a></li>
<li class="chapter" data-level="12.5.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#out-of-sample-error"><i class="fa fa-check"></i><b>12.5.2</b> Out-of-sample Error</a></li>
<li class="chapter" data-level="12.5.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#alternative-measures-of-model-goodness"><i class="fa fa-check"></i><b>12.5.3</b> Alternative measures of model goodness</a></li>
<li class="chapter" data-level="12.5.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-and-model-selection-setup"><i class="fa fa-check"></i><b>12.5.4</b> Feature and model selection: setup</a></li>
<li class="chapter" data-level="12.5.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection-1"><i class="fa fa-check"></i><b>12.5.5</b> Model selection</a></li>
<li class="chapter" data-level="12.5.6" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-predictor-selection"><i class="fa fa-check"></i><b>12.5.6</b> Feature (predictor) selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="describing-relationships.html"><a href="describing-relationships.html"><i class="fa fa-check"></i>Describing Relationships</a></li>
<li class="chapter" data-level="13" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html"><i class="fa fa-check"></i><b>13</b> There’s meaning in parameters</a><ul>
<li class="chapter" data-level="13.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#the-types-of-parametric-assumptions"><i class="fa fa-check"></i><b>13.1</b> The types of parametric assumptions</a><ul>
<li class="chapter" data-level="13.1.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#when-defining-a-model-function."><i class="fa fa-check"></i><b>13.1.1</b> 1. When defining a <strong>model function</strong>.</a></li>
<li class="chapter" data-level="13.1.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#when-defining-probability-distributions."><i class="fa fa-check"></i><b>13.1.2</b> 2. When defining <strong>probability distributions</strong>.</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#the-value-of-making-parametric-assumptions"><i class="fa fa-check"></i><b>13.2</b> The value of making parametric assumptions</a><ul>
<li class="chapter" data-level="13.2.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#value-1-reduced-error"><i class="fa fa-check"></i><b>13.2.1</b> Value #1: Reduced Error</a></li>
<li class="chapter" data-level="13.2.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#value-2-interpretation"><i class="fa fa-check"></i><b>13.2.2</b> Value #2: Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#anova"><i class="fa fa-check"></i><b>13.3</b> ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="the-meaning-of-interaction.html"><a href="the-meaning-of-interaction.html"><i class="fa fa-check"></i><b>14</b> The meaning of interaction</a></li>
<li class="chapter" data-level="15" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html"><i class="fa fa-check"></i><b>15</b> Scales and the restricted range problem</a><ul>
<li class="chapter" data-level="15.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#problems"><i class="fa fa-check"></i><b>15.1</b> Problems</a></li>
<li class="chapter" data-level="15.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solutions"><i class="fa fa-check"></i><b>15.2</b> Solutions</a><ul>
<li class="chapter" data-level="15.2.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-1-transformations"><i class="fa fa-check"></i><b>15.2.1</b> Solution 1: Transformations</a></li>
<li class="chapter" data-level="15.2.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-2-link-functions"><i class="fa fa-check"></i><b>15.2.2</b> Solution 2: Link Functions</a></li>
<li class="chapter" data-level="15.2.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-3-scientifically-backed-functions"><i class="fa fa-check"></i><b>15.2.3</b> Solution 3: Scientifically-backed functions</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#glms-in-r"><i class="fa fa-check"></i><b>15.3</b> GLM’s in R</a><ul>
<li class="chapter" data-level="15.3.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#broomaugment"><i class="fa fa-check"></i><b>15.3.1</b> <code>broom::augment()</code></a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#options-for-logistic-regression"><i class="fa fa-check"></i><b>15.4</b> Options for Logistic Regression</a><ul>
<li class="chapter" data-level="15.4.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#models"><i class="fa fa-check"></i><b>15.4.1</b> Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="improving-estimation-through-distributional-assumptions.html"><a href="improving-estimation-through-distributional-assumptions.html"><i class="fa fa-check"></i><b>16</b> Improving estimation through distributional assumptions</a></li>
<li class="chapter" data-level="17" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html"><i class="fa fa-check"></i><b>17</b> When we only want interpretation on some predictors</a><ul>
<li class="chapter" data-level="17.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability-in-gams"><i class="fa fa-check"></i><b>17.1</b> Non-identifiability in GAMS</a><ul>
<li class="chapter" data-level="17.1.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability"><i class="fa fa-check"></i><b>17.1.1</b> Non-identifiability</a></li>
<li class="chapter" data-level="17.1.2" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#question-1b"><i class="fa fa-check"></i><b>17.1.2</b> Question 1b</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="special-cases.html"><a href="special-cases.html"><i class="fa fa-check"></i>Special cases</a></li>
<li class="chapter" data-level="18" data-path="regression-when-data-are-censored-survival-analysis.html"><a href="regression-when-data-are-censored-survival-analysis.html"><i class="fa fa-check"></i><b>18</b> Regression when data are censored: survival analysis</a></li>
<li class="chapter" data-level="19" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html"><i class="fa fa-check"></i><b>19</b> Regression in the presence of outliers: robust regression</a><ul>
<li class="chapter" data-level="19.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#robust-regression-in-r"><i class="fa fa-check"></i><b>19.1</b> Robust Regression in R</a><ul>
<li class="chapter" data-level="19.1.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#heavy-tailed-regression"><i class="fa fa-check"></i><b>19.1.1</b> Heavy Tailed Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="regression-in-the-presence-of-extremes-extreme-value-regression.html"><a href="regression-in-the-presence-of-extremes-extreme-value-regression.html"><i class="fa fa-check"></i><b>20</b> Regression in the presence of extremes: extreme value regression</a></li>
<li class="chapter" data-level="21" data-path="regression-when-data-are-ordinal.html"><a href="regression-when-data-are-ordinal.html"><i class="fa fa-check"></i><b>21</b> Regression when data are ordinal</a></li>
<li class="chapter" data-level="22" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html"><i class="fa fa-check"></i><b>22</b> Regression when data are missing: multiple imputation</a><ul>
<li class="chapter" data-level="22.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#mean-imputation"><i class="fa fa-check"></i><b>22.1</b> Mean Imputation</a></li>
<li class="chapter" data-level="22.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation"><i class="fa fa-check"></i><b>22.2</b> Multiple Imputation</a><ul>
<li class="chapter" data-level="22.2.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#patterns"><i class="fa fa-check"></i><b>22.2.1</b> Patterns</a></li>
<li class="chapter" data-level="22.2.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation-1"><i class="fa fa-check"></i><b>22.2.2</b> Multiple Imputation</a></li>
<li class="chapter" data-level="22.2.3" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#pooling"><i class="fa fa-check"></i><b>22.2.3</b> Pooling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html"><i class="fa fa-check"></i><b>23</b> Regression under many groups: mixed effects models</a><ul>
<li class="chapter" data-level="23.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#motivation-for-lme"><i class="fa fa-check"></i><b>23.1</b> Motivation for LME</a><ul>
<li class="chapter" data-level="23.1.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#definition"><i class="fa fa-check"></i><b>23.1.1</b> Definition</a></li>
<li class="chapter" data-level="23.1.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#r-tools-for-fitting"><i class="fa fa-check"></i><b>23.1.2</b> R Tools for Fitting</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#mixed-effects-models-in-r-tutorial"><i class="fa fa-check"></i><b>23.2</b> Mixed Effects Models in R: tutorial</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html"><i class="fa fa-check"></i><b>24</b> Regression on an entire distribution: Probabilistic Forecasting</a><ul>
<li class="chapter" data-level="24.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasting-what-it-is"><i class="fa fa-check"></i><b>24.1</b> Probabilistic Forecasting: What it is</a></li>
<li class="chapter" data-level="24.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#review-univariate-distribution-estimates"><i class="fa fa-check"></i><b>24.2</b> Review: Univariate distribution estimates</a><ul>
<li class="chapter" data-level="24.2.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#continuous-response"><i class="fa fa-check"></i><b>24.2.1</b> Continuous response</a></li>
<li class="chapter" data-level="24.2.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discrete-response"><i class="fa fa-check"></i><b>24.2.2</b> Discrete Response</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasts-subset-based-learning-methods"><i class="fa fa-check"></i><b>24.3</b> Probabilistic Forecasts: subset-based learning methods</a><ul>
<li class="chapter" data-level="24.3.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#the-techniques"><i class="fa fa-check"></i><b>24.3.1</b> The techniques</a></li>
<li class="chapter" data-level="24.3.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#exercise-1"><i class="fa fa-check"></i><b>24.3.2</b> Exercise</a></li>
<li class="chapter" data-level="24.3.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>24.3.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="24.3.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#evaluating-model-goodness"><i class="fa fa-check"></i><b>24.3.4</b> Evaluating Model Goodness</a></li>
</ul></li>
<li class="chapter" data-level="24.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discussion-points"><i class="fa fa-check"></i><b>24.4</b> Discussion Points</a></li>
<li class="chapter" data-level="24.5" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#when-are-they-not-useful"><i class="fa fa-check"></i><b>24.5</b> When are they not useful?</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html"><i class="fa fa-check"></i><b>25</b> Regression when order matters: time series and spatial analysis</a><ul>
<li class="chapter" data-level="25.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#timeseries-in-base-r"><i class="fa fa-check"></i><b>25.1</b> Timeseries in (base) R</a></li>
<li class="chapter" data-level="25.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#spatial-example"><i class="fa fa-check"></i><b>25.2</b> Spatial Example</a></li>
<li class="chapter" data-level="25.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#a-model-for-river-rock-size"><i class="fa fa-check"></i><b>25.3</b> A Model for River Rock Size</a><ul>
<li class="chapter" data-level="25.3.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#average-rock-size"><i class="fa fa-check"></i><b>25.3.1</b> 1. Average rock size:</a></li>
<li class="chapter" data-level="25.3.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-rock-size"><i class="fa fa-check"></i><b>25.3.2</b> 2. Mean rock size:</a></li>
<li class="chapter" data-level="25.3.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#downstream-fining-curve"><i class="fa fa-check"></i><b>25.3.3</b> 3. Downstream fining curve:</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objectives"><i class="fa fa-check"></i><b>25.4</b> Statistical Objectives</a><ul>
<li class="chapter" data-level="25.4.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#preliminaries-variance-and-correlation"><i class="fa fa-check"></i><b>25.4.1</b> Preliminaries: Variance and Correlation</a></li>
</ul></li>
<li class="chapter" data-level="25.5" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#three-concepts"><i class="fa fa-check"></i><b>25.5</b> Three Concepts</a><ul>
<li class="chapter" data-level="25.5.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#error-variance-sigma_e2leftxright"><i class="fa fa-check"></i><b>25.5.1</b> Error Variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span></a></li>
<li class="chapter" data-level="25.5.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-variance-sigma_m2"><i class="fa fa-check"></i><b>25.5.2</b> Mean Variance <span class="math inline">\(\sigma_{M}^{2}\)</span></a></li>
<li class="chapter" data-level="25.5.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-correlation-rholeftdright"><i class="fa fa-check"></i><b>25.5.3</b> Mean Correlation <span class="math inline">\(\rho\left(d\right)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="25.6" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#estimation-1"><i class="fa fa-check"></i><b>25.6</b> Estimation</a><ul>
<li class="chapter" data-level="25.6.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#constant-error-variance"><i class="fa fa-check"></i><b>25.6.1</b> Constant Error Variance</a></li>
<li class="chapter" data-level="25.6.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#non-constant-error-variance"><i class="fa fa-check"></i><b>25.6.2</b> Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="25.7" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-1-downstream-fining-curve"><i class="fa fa-check"></i><b>25.7</b> Statistical Objective 1: Downstream Fining Curve</a><ul>
<li class="chapter" data-level="25.7.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#regression-form"><i class="fa fa-check"></i><b>25.7.1</b> Regression Form</a></li>
</ul></li>
<li class="chapter" data-level="25.8" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-2-river-profile"><i class="fa fa-check"></i><b>25.8</b> Statistical Objective 2: River Profile</a><ul>
<li class="chapter" data-level="25.8.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#simple-kriging"><i class="fa fa-check"></i><b>25.8.1</b> Simple Kriging</a></li>
<li class="chapter" data-level="25.8.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#universal-kriging"><i class="fa fa-check"></i><b>25.8.2</b> Universal Kriging</a></li>
<li class="chapter" data-level="25.8.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#kriging-under-non-constant-error-variance"><i class="fa fa-check"></i><b>25.8.3</b> Kriging under Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="25.9" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#confidence-intervals-of-the-river-profile"><i class="fa fa-check"></i><b>25.9</b> Confidence Intervals of the River Profile</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/vincenzocoia/Interpreting-Regression" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpreting Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimating-assumption-free-the-world-of-supervised-learning-techniques" class="section level1">
<h1><span class="header-section-number">Chapter 11</span> Estimating assumption-free: the world of supervised learning techniques</h1>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">suppressPackageStartupMessages</span>(<span class="kw">library</span>(tidyverse))</code></pre></div>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;tibble&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;purrr&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;dplyr&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;stringr&#39; was built under R version 3.5.2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Wage &lt;-<span class="st"> </span>ISLR::Wage
NCI60 &lt;-<span class="st"> </span>ISLR::NCI60
baseball &lt;-<span class="st"> </span>Lahman::Teams %&gt;%<span class="st"> </span>tbl_df %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="dt">runs=</span>R, <span class="dt">hits=</span>H)
cow &lt;-<span class="st"> </span><span class="kw">suppressMessages</span>(<span class="kw">read_csv</span>(<span class="st">&quot;data/milk_fat.csv&quot;</span>))
esoph &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(esoph) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">agegp =</span> <span class="kw">as.character</span>(agegp))
titanic &lt;-<span class="st"> </span><span class="kw">na.omit</span>(titanic::titanic_train)</code></pre></div>
<div id="what-machine-learning-is" class="section level2">
<h2><span class="header-section-number">11.1</span> What machine learning is</h2>
<p>What is Machine Learning (ML) (or Statistical Learning)? As the <a href="http://www-bcf.usc.edu/~gareth/ISL/">ISLR book</a> puts it, it’s a “vast set of tools for understanding data”. Before we explain more, we need to consider the two main types of ML:</p>
<ul>
<li><strong>Supervised learning</strong>. (<em>This is the focus of BAIT 509</em>). Consider a “black box” that accepts some input(s), and returns some type of output. Feed it a variety of input, and write down the output each time (to obtain a <em>data set</em>). <em>Supervised learning</em> attempts to learn from these data to re-construct this black box. That is, it’s a way of building a forecaster/prediction tool.</li>
</ul>
<p>You’ve already seen examples throughout MBAN. For example, consider trying to predict someone’s wage (output) based on their age (input). Using the <code>Wage</code> data set from the <code>ISLR</code> R package, here are examples of inputs and outputs:</p>
<pre><code>##   age      wage
## 1  18  75.04315
## 2  24  70.47602
## 3  45 130.98218
## 4  43 154.68529
## 5  50  75.04315
## 6  54 127.11574</code></pre>
<p>We try to model the relationship between age and wage so that we can predict the salary of a new individual, given their age.</p>
<p>An example supervised learning technique is <em>linear regression</em>, which you’ve seen before in BABS 507/508. For an age <code>x</code>, let’s use linear regression to make a prediction that’s quadratic in <code>x</code>. Here’s the fit:</p>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The blue curve represents our attempt to “re-construct” the black box by learning from the existing data. So, for a new individual aged 70, we would predict a salary of about $100,000. A 50-year-old, about $125,000.</p>
<ul>
<li><strong>Unsupervised learning</strong>. (<em>BAIT 509 will not focus on this</em>). Sometimes we can’t see the output of the black box. <em>Unsupervised learning</em> attempts to find structure in the data without any output.</li>
</ul>
<p>For example, consider the following two gene expression measurements (actually two principal components). Are there groups that we can identify here?</p>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>You’ve seen methods for doing this in BABS 507/508, such as k-means.</p>
</div>
<div id="types-of-supervised-learning" class="section level2">
<h2><span class="header-section-number">11.2</span> Types of Supervised Learning</h2>
<p>There are two main types of supervised learning methods – determined entirely by the type of response variable.</p>
<ul>
<li><strong>Regression</strong> is supervised learning when the response is numeric.</li>
<li><strong>Classification</strong> is supervised learning when the response is categorical.</li>
</ul>
<p>We’ll examine both equally in this course.</p>
<p>Note: Don’t confuse classification with <em>clustering</em>! The latter is an unsupervised learning method.</p>
</div>
<div id="local-regression" class="section level2">
<h2><span class="header-section-number">11.3</span> Local Regression</h2>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<p>(BAIT 509 Class Meeting 03)</p>
<p>Let’s turn our attention to the first “new” machine learning methods of the course: <span class="math inline">\(k\)</span> <strong>Nearest Neighbours</strong> (aka kNN or <span class="math inline">\(k\)</span>-NN) and <strong>loess</strong> (aka “LOcal regrESSion”).</p>
<p>The fundamental idea behind these methods is to <em>base your prediction on what happened in similar cases in the past</em>.</p>
<div id="knn" class="section level3">
<h3><span class="header-section-number">11.3.1</span> kNN</h3>
<p>Pick a positive integer <span class="math inline">\(k\)</span>.</p>
<p>To make a prediction of the response at a particular observation of the predictors (I’ll call this the <strong>query point</strong>) – that is, when <span class="math inline">\(X_1=x_1\)</span>, …, <span class="math inline">\(X_p=x_p\)</span>:</p>
<ol style="list-style-type: decimal">
<li>Subset your data to <span class="math inline">\(k\)</span> observations (rows) whose values of the predictors <span class="math inline">\((X_1, \ldots, X_p)\)</span> are closest to <span class="math inline">\((x_1,\ldots,x_p)\)</span>.</li>
<li>For kNN classificiation, use the “most popular vote” (i.e., the modal category) of the subsetted observations. For kNN regression, use the average <span class="math inline">\(Y\)</span> of the remaining subsetted observations.</li>
</ol>
<p>Recall how to calculate distance between two vectors <span class="math inline">\((a_1, \ldots, a_p)\)</span> and <span class="math inline">\((b_1, \ldots, b_p)\)</span>: <span class="math display">\[ \text{distance} = \sqrt{(a_1-b_1)^2 + \cdots + (a_p-b_p)^2}. \]</span> It’s even easier when there’s one predictor: it’s just the absolute value of the difference.</p>
</div>
<div id="loess" class="section level3">
<h3><span class="header-section-number">11.3.2</span> loess</h3>
<p>(This is actually the simplest version of loess, sometimes called a <strong>moving window</strong> approach. We’ll get to the “full” loess).</p>
<p>Pick a positive number <span class="math inline">\(r\)</span> (not necessarily integer).</p>
<p>To make a prediction of the response at a query point (that is, a particular observation of the predictors, <span class="math inline">\(X_1=x_1\)</span>, …, <span class="math inline">\(X_p=x_p\)</span>):</p>
<ol style="list-style-type: decimal">
<li>Subset your data to those observations (rows) having values of the predictors <span class="math inline">\((X_1,\ldots,X_p)\)</span> within <span class="math inline">\(r\)</span> units of <span class="math inline">\((x_1,\ldots,x_p)\)</span>.</li>
<li>For kNN classificiation, use the “most popular vote” (i.e., the modal category) of the subsetted observations. For kNN regression, use the average <span class="math inline">\(Y\)</span> of the remaining subsetted observations.</li>
</ol>
<p>Notice that Step 2 is the same as in kNN.</p>
<p><span class="math inline">\(k\)</span> and <span class="math inline">\(r\)</span> are called <strong>hyperparameters</strong>, because we don’t estimate them – we choose them outright.</p>
</div>
<div id="in-class-exercises" class="section level3">
<h3><span class="header-section-number">11.3.3</span> In-Class Exercises</h3>
<p>Consider the following data set, given by <code>dat</code>. Here’s the top six rows of data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">87</span>)
dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>(<span class="dv">100</span>), <span class="kw">rnorm</span>(<span class="dv">100</span>)+<span class="dv">5</span>)-<span class="dv">3</span>,
              <span class="dt">y =</span> <span class="kw">sin</span>(x^<span class="dv">2</span>/<span class="dv">5</span>)/x +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>)/<span class="dv">10</span> +<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))</code></pre></div>
<p>Here’s a scatterplot of the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(x,y)) +<span class="st"> </span>
<span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">colour=</span><span class="st">&quot;orange&quot;</span>) +
<span class="st">    </span><span class="kw">theme_bw</span>() +<span class="st"> </span>
<span class="st">    </span>rotate_y</code></pre></div>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<div id="exercise-1-mean-at-x0" class="section level4">
<h4><span class="header-section-number">11.3.3.1</span> Exercise 1: Mean at <span class="math inline">\(X=0\)</span></h4>
<p>Let’s check your understanding of loess and kNN. Consider estimating the mean of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X=0\)</span> by using data whose <span class="math inline">\(X\)</span> values are near 0.</p>
<ol style="list-style-type: decimal">
<li><p>Eyeball the above scatterplot of the data. What would you say is a reasonable estimate of the mean of <span class="math inline">\(Y\)</span> at <span class="math inline">\(X=0\)</span>? Why?</p></li>
<li>Estimate using loess and kNN (you choose the hyperparameters).
<ol style="list-style-type: decimal">
<li>Hints for kNN:
<ul>
<li>First, add a new column in the data that stores the <em>distance</em> between <span class="math inline">\(X=0\)</span> and each observation. If that column is named <code>d</code>, you can do this with the following partial code: <code>dat$d &lt;- YOUR_CALCULATION_HERE</code>. Recall that <code>dat$x</code> is a vector of the <code>x</code> column.</li>
<li>Then, arrange the data from smallest distance to largest with <code>arrange(dat)</code> (you’ll need to load the <code>tidyverse</code> package first), and subset <em>that</em> to the first <span class="math inline">\(k\)</span> rows.</li>
</ul></li>
<li>Hints for loess:
<ul>
<li>Subset the data using the <code>filter</code> function. The condition to filter on: you want to keep rows whose distances (<code>d</code>) are …</li>
</ul></li>
</ol></li>
</ol>
<pre><code>k &lt;- 10
r &lt;- 0.5
x0 &lt;- 0
dat$dist &lt;- FILL_THIS_IN
dat &lt;- arrange(dat, dist)  # sort by distance
kNN_prediction &lt;- FILL_THIS_IN
loess_prediction &lt;- FILL_THIS_IN</code></pre>
<ol start="3" style="list-style-type: decimal">
<li><p>What happens when you try to pick an <span class="math inline">\(r\)</span> that is way too small? Say, <span class="math inline">\(r=0.01\)</span>? Why?</p></li>
<li><p>There’s a tradeoff between choosing large and small values of either hyperparameter. What’s good and what’s bad about choosing a large value? What about small values?</p></li>
</ol>
</div>
<div id="exercise-2-regression-curve" class="section level4">
<h4><span class="header-section-number">11.3.3.2</span> Exercise 2: Regression Curve</h4>
<p>Form the <strong>regression curve</strong> / <strong>model function</strong> by doing the estimation over a grid of x values, and connecting the dots:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">xgrid &lt;-<span class="st"> </span><span class="kw">seq</span>(-<span class="dv">5</span>, <span class="dv">4</span>, <span class="dt">length.out=</span><span class="dv">1000</span>)
k &lt;-<span class="st"> </span><span class="dv">10</span>
r &lt;-<span class="st"> </span><span class="fl">0.5</span>
kNN_estimates &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(xgrid, function(x_){
    dat %&gt;%<span class="st"> </span>
<span class="st">        </span><span class="kw">mutate</span>(<span class="dt">d =</span> <span class="kw">abs</span>(x-x_)) %&gt;%<span class="st"> </span>
<span class="st">        </span><span class="kw">arrange</span>(d) %&gt;%<span class="st"> </span>
<span class="st">        </span><span class="kw">summarize</span>(<span class="dt">yhat=</span><span class="kw">mean</span>(y[<span class="dv">1</span>:k])) %&gt;%<span class="st"> </span>
<span class="st">        `</span><span class="dt">[[</span><span class="st">`</span>(<span class="st">&quot;yhat&quot;</span>)
})
loess_estimates &lt;-<span class="st"> </span><span class="kw">map_dbl</span>(xgrid, function(x_){
    dat %&gt;%<span class="st"> </span>
<span class="st">        </span><span class="kw">mutate</span>(<span class="dt">d =</span> <span class="kw">abs</span>(x-x_)) %&gt;%<span class="st"> </span>
<span class="st">        </span><span class="kw">filter</span>(d&lt;r) %&gt;%<span class="st"> </span>
<span class="st">        </span><span class="kw">summarize</span>(<span class="dt">yhat=</span><span class="kw">mean</span>(y)) %&gt;%<span class="st"> </span>
<span class="st">        `</span><span class="dt">[[</span><span class="st">`</span>(<span class="st">&quot;yhat&quot;</span>)
})
est &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x=</span>xgrid, <span class="dt">kNN=</span>kNN_estimates, <span class="dt">loess=</span>loess_estimates) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">gather</span>(<span class="dt">key=</span><span class="st">&quot;method&quot;</span>, <span class="dt">value=</span><span class="st">&quot;estimate&quot;</span>, kNN, loess)
<span class="kw">ggplot</span>() +
<span class="st">    </span><span class="kw">geom_point</span>(<span class="dt">data=</span>dat, <span class="dt">mapping=</span><span class="kw">aes</span>(x,y)) +
<span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data=</span>est, 
              <span class="dt">mapping=</span><span class="kw">aes</span>(x,estimate, <span class="dt">group=</span>method, <span class="dt">colour=</span>method),
              <span class="dt">size=</span><span class="dv">1</span>) +
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p><strong>Exercises</strong>:</p>
<ul>
<li>Play with different values of <span class="math inline">\(k\)</span> and <span class="math inline">\(r\)</span>, and regenerate the plot each time. What effect does increasing these values have on the regression curve? What about decreasing? What would you say is a “good” choice of <span class="math inline">\(k\)</span> and <span class="math inline">\(r\)</span>, and why?</li>
<li>What happens when you choose <span class="math inline">\(k=n=200\)</span>? What happens if you choose <span class="math inline">\(r=10\)</span> or bigger?</li>
</ul>
<p>The phenomenon you see when <span class="math inline">\(k\)</span> and <span class="math inline">\(r\)</span> are very small is called <strong>overfitting</strong>. This means that your model displays patterns that are not actually present. <strong>Underfitting</strong>, on the other hand, is when your model misses patterns in the data that are actually present.</p>
</div>
</div>
<div id="hyperparameters-and-the-biasvariance-tradeoff" class="section level3">
<h3><span class="header-section-number">11.3.4</span> Hyperparameters and the bias/variance tradeoff</h3>
<p>Let’s look at the bias and variance for different values of the hyperparameter in loess.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">400</span>)
N &lt;-<span class="st"> </span><span class="dv">100</span>
xgrid &lt;-<span class="st"> </span><span class="kw">seq</span>(-<span class="dv">7</span>, <span class="fl">6.5</span>, <span class="dt">length.out=</span><span class="dv">300</span>)
true_mean &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(function(x){
    if (x==<span class="dv">0</span>) <span class="kw">return</span>(<span class="kw">exp</span>(<span class="dv">1</span>)) else <span class="kw">return</span>(<span class="kw">sin</span>(x^<span class="dv">2</span>/<span class="dv">5</span>)/x +<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
})
## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop
<span class="kw">expand.grid</span>(<span class="dt">iter=</span><span class="dv">1</span>:N, <span class="dt">r=</span><span class="kw">c</span>(<span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>)) %&gt;%<span class="st"> </span><span class="kw">group_by</span>(iter, r) %&gt;%<span class="st"> </span><span class="kw">do</span>({
    this_r &lt;-<span class="st"> </span><span class="kw">unique</span>(.$r)
    dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>(<span class="dv">100</span>), <span class="kw">rnorm</span>(<span class="dv">100</span>)+<span class="dv">5</span>)-<span class="dv">3</span>,
                  <span class="dt">y =</span> <span class="kw">sin</span>(x^<span class="dv">2</span>/<span class="dv">5</span>)/x +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>)/<span class="dv">10</span> +<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
    <span class="kw">data.frame</span>(
        .,
        <span class="dt">x =</span> xgrid,
        <span class="dt">yhat =</span> <span class="kw">ksmooth</span>(dat$x, dat$y, <span class="dt">kernel=</span><span class="st">&quot;box&quot;</span>, 
                         <span class="dt">bandwidth=</span>this_r,
                         <span class="dt">x.points=</span>xgrid)$y
    )
}) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">ungroup</span>() %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">r=</span><span class="kw">paste0</span>(<span class="st">&quot;bandwidth=&quot;</span>, r)) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>yhat)) +
<span class="st">    </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>r, <span class="dt">ncol=</span><span class="dv">1</span>) +
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group=</span>iter, <span class="dt">colour=</span><span class="st">&quot;Estimates&quot;</span>), <span class="dt">alpha=</span><span class="fl">0.1</span>) +
<span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun=</span>true_mean,
                  <span class="dt">mapping=</span><span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;True mean&quot;</span>)) +
<span class="st">    </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">scale_colour_brewer</span>(<span class="st">&quot;&quot;</span>, <span class="dt">palette=</span><span class="st">&quot;Dark2&quot;</span>) +
<span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;y&quot;</span>) +<span class="st"> </span>rotate_y</code></pre></div>
<pre><code>## Warning: Removed 3240 rows containing missing values (geom_path).</code></pre>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>You can see the bias/variance tradeoff here:</p>
<ul>
<li>Notice that the estimates get <em>narrower</em> as the bandwidth increases – this means the variance reduces as the bandwidth increases.</li>
<li>Notice that the estimates become biased as the bandwidth increases.</li>
</ul>
<p>A similar phenomenon exists with kNN regression.</p>
<p>Notice some other things about these plots:</p>
<ul>
<li>There’s more variance whenever there’s less data – that’s at the tails, and (by design) at around <span class="math inline">\(X=0\)</span>.</li>
<li>Estimates don’t exist sometimes, if no data fall in the “window”. You can see that the tails are cut short when the bandwidth is small.</li>
</ul>
</div>
<div id="extensions-to-knn-and-loess" class="section level3">
<h3><span class="header-section-number">11.3.5</span> Extensions to kNN and loess</h3>
<div id="kernel-weighting" class="section level4">
<h4><span class="header-section-number">11.3.5.1</span> Kernel weighting</h4>
<p>kNN and loess can be generalized by downweighing points that are further from the query point. In particular, we take a weighted average.</p>
<p>Suppose <span class="math inline">\(y_1, \ldots, y_n\)</span> are <span class="math inline">\(n\)</span> realizations of the response <span class="math inline">\(Y\)</span>. If we assign (respective) weights <span class="math inline">\(w_1, \ldots, w_n\)</span> to these realizations, then the <strong>weighted average</strong> is <span class="math display">\[ \frac{\sum_{i=1}^n w_i y_i}{\sum_{i=1}^n w_i}. \]</span></p>
<p>If the response is categorical, then each subsetted observation gives a “weighted vote”. Sum up the weights corresponding to each category to obtain the total “number” of votes.</p>
<p>We obtain these weights using a <strong>kernel function</strong>. A kernel function is any non-increasing function over the positive real numbers. Plug in the distance between the observed predictor(s) and the query point to obtain the weight. Some examples of kernel functions are plotted below.</p>
</div>
<div id="local-polynomials" class="section level4">
<h4><span class="header-section-number">11.3.5.2</span> Local polynomials</h4>
<p>Another extension of loess is to consider <strong>local polynomials</strong>. The idea here is, after subsetting the data lying within <span class="math inline">\(r\)</span> units of the query point, add the following two steps:</p>
<ol style="list-style-type: decimal">
<li>Fit a linear (or quadratic) regression model to the subsetted data only.
<ul>
<li>This is the “local polynomial”. You can think of this as like a “mini linear regression”.</li>
</ul></li>
<li>Obtain your prediction by evaluating the regression curve at the query point.</li>
<li>Throw away your fitted local polynomial.</li>
</ol>
<p>OK, the 3rd step isn’t really a true step, but I like to include it to emphasize that we only evaluate the local polynomial at the query point.</p>
<p>Note:</p>
<ul>
<li>We <em>could</em> fit higher order polynomials, but that tends to overfit the data.</li>
<li>We <em>could</em> fit any other curve locally besides a polynomial, but polynomials are justified by the Taylor approximation.</li>
<li>Local polynomials with degree=0 is the same as “not doing” local polynomials.</li>
</ul>
</div>
<div id="combination" class="section level4">
<h4><span class="header-section-number">11.3.5.3</span> Combination</h4>
<p>You can combine kernel weighting with local polynomials. When you fit the local polynomial to the subsetted data, you can run a <em>weighted</em> regression. Instead of minimizing the sum of squared errors, we minimize the <em>weighted</em> sum of squared errors.</p>
</div>
<div id="other-distances" class="section level4">
<h4><span class="header-section-number">11.3.5.4</span> Other distances</h4>
<p>We don’t have to use the “usual” notion of distance. The formula I gave you earlier (above) is called the <em>Euclidean distance</em>, or L2 norms. There’s also the L1 norm (also called the manhattan distance, which is distance by moving along the axes/rectangularly) and L0 norm (number of predictors having non-zero univariate distance).</p>
</div>
<div id="scaling" class="section level4">
<h4><span class="header-section-number">11.3.5.5</span> Scaling</h4>
<p>When you’re using two or more predictors, your predictors might be on different scales. This means distances aren’t weighed equally, depending on the direction. Instead of measuring distance on the original scale of the predictors, consider re-scaling the predictors by subtracting the mean and dividing by the standard deviation for each predictor.</p>
</div>
<div id="demonstration" class="section level4">
<h4><span class="header-section-number">11.3.5.6</span> Demonstration</h4>
<p>Let’s look at the same example, but with kernel downweighting and local polynomials.</p>
<p>Warning! The “bandwidth” hyperparameter in this plot is parameterized differently than in the previous plot, but carries the same interpretation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">400</span>)
N &lt;-<span class="st"> </span><span class="dv">100</span>
xgrid &lt;-<span class="st"> </span><span class="kw">seq</span>(-<span class="dv">7</span>, <span class="fl">6.5</span>, <span class="dt">length.out=</span><span class="dv">300</span>)
true_mean &lt;-<span class="st"> </span><span class="kw">Vectorize</span>(function(x){
    if (x==<span class="dv">0</span>) <span class="kw">return</span>(<span class="kw">exp</span>(<span class="dv">1</span>)) else <span class="kw">return</span>(<span class="kw">sin</span>(x^<span class="dv">2</span>/<span class="dv">5</span>)/x +<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
})
## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop
<span class="kw">expand.grid</span>(<span class="dt">iter=</span><span class="dv">1</span>:N, <span class="dt">r=</span><span class="kw">c</span>(<span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>), <span class="dt">d=</span><span class="dv">0</span>:<span class="dv">2</span>) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">group_by</span>(iter, r, d) %&gt;%<span class="st"> </span><span class="kw">do</span>({
    this_r &lt;-<span class="st"> </span><span class="kw">unique</span>(.$r)
    this_d &lt;-<span class="st"> </span><span class="kw">unique</span>(.$d)
    dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>(<span class="dv">100</span>), <span class="kw">rnorm</span>(<span class="dv">100</span>)+<span class="dv">5</span>)-<span class="dv">3</span>,
                  <span class="dt">y =</span> <span class="kw">sin</span>(x^<span class="dv">2</span>/<span class="dv">5</span>)/x +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">200</span>)/<span class="dv">10</span> +<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
    <span class="kw">data.frame</span>(
        .,
        <span class="dt">x =</span> xgrid,
        <span class="dt">yhat =</span> <span class="kw">predict</span>(<span class="kw">loess</span>(y~x, <span class="dt">data=</span>dat, 
                             <span class="dt">span=</span>this_r, <span class="dt">degree=</span>this_d),
                       <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">x=</span>xgrid))
    )
}) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">ungroup</span>() %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">r=</span><span class="kw">paste0</span>(<span class="st">&quot;bandwidth=&quot;</span>, r),
           <span class="dt">d=</span><span class="kw">paste0</span>(<span class="st">&quot;degree=&quot;</span>, d)) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>yhat)) +
<span class="st">    </span><span class="kw">facet_grid</span>(r ~<span class="st"> </span>d) +
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group=</span>iter, <span class="dt">colour=</span><span class="st">&quot;Estimates&quot;</span>), <span class="dt">alpha=</span><span class="fl">0.1</span>) +
<span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun=</span>true_mean,
                  <span class="dt">mapping=</span><span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;True mean&quot;</span>)) +
<span class="st">    </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">scale_colour_brewer</span>(<span class="st">&quot;&quot;</span>, <span class="dt">palette=</span><span class="st">&quot;Dark2&quot;</span>) +
<span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;y&quot;</span>) +<span class="st"> </span>rotate_y</code></pre></div>
<pre><code>## Warning: Removed 8034 rows containing missing values (geom_path).</code></pre>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-9-1.png" width="960" /></p>
<p>Notice:</p>
<ul>
<li>For small bandwidth, increasing the degree of the poynomial just results in more variance – degree=0 looks best for this bandwidth.</li>
<li>But by increasing the degree (inc. variance, dec. bias) <em>and</em> increasing the bandwidth (dec. variance, inc. bias), we end up getting an overall better fit: low bias, low variance. Bandwidth=0.5 and Degree=2 here seem to work best.</li>
</ul>
</div>
</div>
<div id="model-assumptions-and-the-biasvariance-tradeoff" class="section level3">
<h3><span class="header-section-number">11.3.6</span> Model assumptions and the bias/variance tradeoff</h3>
<p>Recall that we saw an incorrect model assumption leads to bias, such as fitting linear regression when the true mean is non-linear.</p>
<p>When you make model assumptions that are <em>close to the truth</em>, then this has the effect of <em>decreasing variance</em>.</p>
<p>Adding good model assumptions is like adding more data – after all data is information, and a model assumption is also information.</p>
<p>Here’s a demonstration:</p>
<p>Consider <span class="math display">\[ Y = X + \varepsilon, \]</span> where <span class="math inline">\(X\)</span> (predictor) is N(0,1), and <span class="math inline">\(\varepsilon\)</span> (error term) is also N(0,1) (both are independent).</p>
<p>I’ll generate a sample of size 100, 100 times. For each sample, I’ll fit a linear regression model and a loess model. Here are the resulting 100 regression curves for each (the dashed line is the true mean):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">474</span>)
n &lt;-<span class="st"> </span><span class="dv">100</span>
N &lt;-<span class="st"> </span><span class="dv">100</span>
xgrid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">seq</span>(-<span class="dv">4</span>,<span class="dv">4</span>, <span class="dt">length.out=</span><span class="dv">100</span>))
## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop
<span class="kw">tibble</span>(<span class="dt">iter=</span><span class="dv">1</span>:N) %&gt;%<span class="st"> </span><span class="kw">group_by</span>(iter) %&gt;%<span class="st"> </span><span class="kw">do</span>({
    dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x=</span><span class="kw">rnorm</span>(n), 
                  <span class="dt">y=</span>x+<span class="kw">rnorm</span>(n))
    <span class="kw">data.frame</span>(
        .,
        xgrid,
        <span class="dt">Local  =</span> <span class="kw">predict</span>(<span class="kw">loess</span>(y~x, <span class="dt">data=</span>dat),
                         <span class="dt">newdata=</span>xgrid),
        <span class="dt">Linear =</span> <span class="kw">predict</span>(<span class="kw">lm</span>(y~x, <span class="dt">data=</span>dat),
                         <span class="dt">newdata=</span>xgrid)
    )
}) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">gather</span>(<span class="dt">key=</span><span class="st">&quot;method&quot;</span>, <span class="dt">value=</span><span class="st">&quot;Prediction&quot;</span>, Local, Linear) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>Prediction)) +
<span class="st">    </span><span class="kw">facet_wrap</span>(~<span class="st"> </span>method) +
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group=</span>iter), <span class="dt">colour=</span><span class="st">&quot;orange&quot;</span>, <span class="dt">alpha=</span><span class="fl">0.1</span>) +
<span class="st">    </span><span class="kw">geom_abline</span>(<span class="dt">intercept=</span><span class="dv">0</span>, <span class="dt">slope=</span><span class="dv">1</span>, <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>) +
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<pre><code>## Warning: Removed 1869 rows containing missing values (geom_path).</code></pre>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Notice that the local method has higher variance than the linear regression method.</p>
</div>
</div>
<div id="splines-and-loess-regression" class="section level2">
<h2><span class="header-section-number">11.4</span> Splines and Loess Regression</h2>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<p>This tutorial describes spline and loess regression in R.</p>
<p>You can think of splines as regression between knots. We’ll use the <code>splines</code> package to do this.</p>
<p>Here’s some generated data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>)
y &lt;-<span class="st"> </span>x^<span class="dv">2</span> +<span class="st"> </span><span class="kw">rnorm</span>(<span class="dv">1000</span>)
<span class="kw">qplot</span>(x, y, <span class="dt">alpha=</span><span class="kw">I</span>(<span class="fl">0.5</span>)) +<span class="st"> </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>First, we need to “set up” the regression by placing knots.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x2 &lt;-<span class="st"> </span>splines::<span class="kw">ns</span>(x, <span class="dt">knots=</span><span class="kw">c</span>(-<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">2</span>))</code></pre></div>
<p>Now we can do regression between these knots, with the natural spline shape (as opposed to linear):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y ~<span class="st"> </span>x2)
<span class="kw">qplot</span>(x, y, <span class="dt">alpha=</span><span class="kw">I</span>(<span class="fl">0.5</span>)) +
<span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x=</span>x, <span class="dt">y=</span><span class="kw">predict</span>(fit)), 
              <span class="dt">mapping=</span><span class="kw">aes</span>(x, y), <span class="dt">colour=</span><span class="st">&quot;blue&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<div id="loess-1" class="section level3">
<h3><span class="header-section-number">11.4.1</span> Loess</h3>
<p>Loess is “local regression”, and is based on the idea of estimating the mean response based on similar observed data in the predictor space.</p>
<p><strong>Kernel smoothing</strong> is a method that estimates the mean using a weighted sample average, where observations that are further away in the predictor space get downweighted more, according to some kernel function.</p>
<p><strong>Local polynomials</strong> is a method that takes kernel smoothing one step further: instead of a weighted sample average, we use nearby data (in the predictor space) to fit a polynomial regression, and then fit a polynomial regression model.</p>
<p>There is a more basic version, too: a <strong>“moving window”</strong>, described next, before seeing how loess is done in R.</p>
<div id="the-moving-window" class="section level4">
<h4><span class="header-section-number">11.4.1.1</span> The “Moving Window”</h4>
<p>The “moving window” approach is a special type of kernel smoother. For a given value of the predictor <span class="math inline">\(X=x\)</span>, the mean response is estimated as the sample average of all response values whose predictor values are “near” <span class="math inline">\(x\)</span> – within some distance <span class="math inline">\(h\)</span>.</p>
<p>For example, if you want to estimate the mean number of “runs” of a baseball team when walks=100 and hits=1000, only look at cases where “walks” is approximately 100, “hits” is approximately 1000, and then average the response.</p>
<p>It’s a special case of kernel regression, with a kernel function of <span class="math display">\[ k\left(t\right) = I\left(|t-x| &lt; h \right), \]</span> sometimes called the “boxcar” function.</p>
<p><strong>Note the similarity to kNN!</strong> kNN regression uses the nearest <span class="math inline">\(k\)</span> points, resulting in a variable distance <span class="math inline">\(h\)</span>, whereas the moving window regression uses a fixed distance <span class="math inline">\(h\)</span>, resulting in a variable number of points <span class="math inline">\(k\)</span> used.</p>
</div>
<div id="ggplot2" class="section level4">
<h4><span class="header-section-number">11.4.1.2</span> <code>ggplot2</code></h4>
<p><code>ggplot2</code> comes with a fairly powerful tool for plotting a smoother, with <code>geom_smooth</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(x, y) +
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;loess&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>You can choose the bandwidth through the <code>span</code> argument:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(x, y) +
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;loess&quot;</span>, <span class="dt">span=</span><span class="fl">0.1</span>) +
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<p>Too wiggly. The default looks fine.</p>
<p>Note that <code>geom_smooth</code> can fit <code>lm</code> and <code>glm</code> fits too:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(x, y) +
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">formula=</span>y~x+<span class="kw">I</span>(x^<span class="dv">2</span>)) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="manual-method" class="section level4">
<h4><span class="header-section-number">11.4.1.3</span> Manual method</h4>
<p>You can use the <code>loess</code> or <code>ksmooth</code> function in R to do kernel smoothing yourself. You get more flexibility here, since you can get things such as predictions. It works just like the other regression functions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(fit &lt;-<span class="st"> </span><span class="kw">loess</span>(y ~<span class="st"> </span>x))</code></pre></div>
<pre><code>## Call:
## loess(formula = y ~ x)
## 
## Number of Observations: 1000 
## Equivalent Number of Parameters: 5.18 
## Residual Standard Error: 1.037</code></pre>
<p>We can make predictions as usual:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit)
<span class="kw">qplot</span>(x, y) +
<span class="st">    </span><span class="kw">geom_line</span>(<span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>yhat), <span class="dt">mapping=</span><span class="kw">aes</span>(x, y), 
              <span class="dt">colour=</span><span class="st">&quot;blue&quot;</span>) +<span class="st"> </span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="090-Estimating_assumption_free_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>If you want the standard errors of the predictions, you can indicate this with <code>se=TRUE</code> in the <code>predict</code> function.</p>
<p>Some key things that you might want to change in your kernel smoothing regression, through arguments in the <code>loess</code> function:</p>
<ul>
<li>Bandwidth/smoothing parameter. Indicate through the <code>span</code> argument.</li>
<li>Degree of the local polynomial fitted. Indicate through the <code>degree</code> argument.</li>
<li>Kernel function.</li>
</ul>
<p>The kernel function is not readily specified in <code>loess</code>. But you can use the <code>ksmooth</code>, where you’re allowed to specify a “box” or “normal” kernel.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="estimating-parametric-model-functions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="overfitting-the-problem-with-adding-too-many-parameters.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/vincenzocoia/Interpreting-Regression/edit/master/090-Estimating_assumption_free.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Interpreting-Regression.pdf", "Interpreting-Regression.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
