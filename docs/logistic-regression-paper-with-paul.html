<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 20 Logistic Regression paper with Paul | Interpreting Regression</title>
  <meta name="description" content="My tutorials for regression analysis, in the form of a bookdown book.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 20 Logistic Regression paper with Paul | Interpreting Regression" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="My tutorials for regression analysis, in the form of a bookdown book." />
  <meta name="github-repo" content="vincenzocoia/Interpreting-Regression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 20 Logistic Regression paper with Paul | Interpreting Regression" />
  
  <meta name="twitter:description" content="My tutorials for regression analysis, in the form of a bookdown book." />
  

<meta name="author" content="Vincenzo Coia">


<meta name="date" content="2019-03-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="glms-in-r.html">
<link rel="next" href="robust-regression-in-r.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpreting Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#caution"><i class="fa fa-check"></i><b>1.1</b> Caution</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#purpose-of-the-book"><i class="fa fa-check"></i><b>1.2</b> Purpose of the book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#tasks-that-motivate-regression"><i class="fa fa-check"></i><b>1.3</b> Tasks that motivate Regression</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>2</b> Distributions</a><ul>
<li class="chapter" data-level="2.1" data-path="distributions.html"><a href="distributions.html#probabilistic-quantities"><i class="fa fa-check"></i><b>2.1</b> Probabilistic Quantities</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>3</b> Estimation</a><ul>
<li class="chapter" data-level="3.1" data-path="estimation.html"><a href="estimation.html#estimation-of-probabilistic-quantities"><i class="fa fa-check"></i><b>3.1</b> Estimation of Probabilistic Quantities</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="writing-the-sample-mean-as-an-optimization-problem.html"><a href="writing-the-sample-mean-as-an-optimization-problem.html"><i class="fa fa-check"></i><b>4</b> Writing the sample mean as an optimization problem</a></li>
<li class="chapter" data-level="5" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html"><i class="fa fa-check"></i><b>5</b> Probabilistic Forecasting</a><ul>
<li class="chapter" data-level="5.1" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#probabilistic-forecasting-what-it-is"><i class="fa fa-check"></i><b>5.1</b> Probabilistic Forecasting: What it is</a></li>
<li class="chapter" data-level="5.2" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#review-univariate-distribution-estimates"><i class="fa fa-check"></i><b>5.2</b> Review: Univariate distribution estimates</a><ul>
<li class="chapter" data-level="5.2.1" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#continuous-response"><i class="fa fa-check"></i><b>5.2.1</b> Continuous response</a></li>
<li class="chapter" data-level="5.2.2" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#discrete-response"><i class="fa fa-check"></i><b>5.2.2</b> Discrete Response</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#probabilistic-forecasts-subset-based-learning-methods"><i class="fa fa-check"></i><b>5.3</b> Probabilistic Forecasts: subset-based learning methods</a><ul>
<li class="chapter" data-level="5.3.1" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#the-techniques"><i class="fa fa-check"></i><b>5.3.1</b> The techniques</a></li>
<li class="chapter" data-level="5.3.2" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#exercise"><i class="fa fa-check"></i><b>5.3.2</b> Exercise</a></li>
<li class="chapter" data-level="5.3.3" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>5.3.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="5.3.4" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#evaluating-model-goodness"><i class="fa fa-check"></i><b>5.3.4</b> Evaluating Model Goodness</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#discussion-points"><i class="fa fa-check"></i><b>5.4</b> Discussion Points</a></li>
<li class="chapter" data-level="5.5" data-path="probabilistic-forecasting.html"><a href="probabilistic-forecasting.html#when-are-they-not-useful"><i class="fa fa-check"></i><b>5.5</b> When are they not useful?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="quantile-regression.html"><a href="quantile-regression.html"><i class="fa fa-check"></i><b>6</b> Quantile Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="quantile-regression.html"><a href="quantile-regression.html#what-is-the-mean-anyway"><i class="fa fa-check"></i><b>6.1</b> What is the mean, anyway?</a></li>
<li class="chapter" data-level="6.2" data-path="quantile-regression.html"><a href="quantile-regression.html#linear-quantile-regression"><i class="fa fa-check"></i><b>6.2</b> Linear Quantile Regression</a></li>
<li class="chapter" data-level="6.3" data-path="quantile-regression.html"><a href="quantile-regression.html#exercise-1"><i class="fa fa-check"></i><b>6.3</b> Exercise</a></li>
<li class="chapter" data-level="6.4" data-path="quantile-regression.html"><a href="quantile-regression.html#problem-crossing-quantiles"><i class="fa fa-check"></i><b>6.4</b> Problem: Crossing quantiles</a></li>
<li class="chapter" data-level="6.5" data-path="quantile-regression.html"><a href="quantile-regression.html#problem-upper-quantiles"><i class="fa fa-check"></i><b>6.5</b> Problem: Upper quantiles</a></li>
<li class="chapter" data-level="6.6" data-path="quantile-regression.html"><a href="quantile-regression.html#evaluating-model-goodness-1"><i class="fa fa-check"></i><b>6.6</b> Evaluating Model Goodness</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html"><i class="fa fa-check"></i><b>7</b> Introduction to Machine Learning</a><ul>
<li class="chapter" data-level="7.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#what-machine-learning-is"><i class="fa fa-check"></i><b>7.1</b> What machine learning is</a></li>
<li class="chapter" data-level="7.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#variable-terminology"><i class="fa fa-check"></i><b>7.2</b> Variable terminology</a><ul>
<li class="chapter" data-level="7.2.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#variable-types"><i class="fa fa-check"></i><b>7.2.1</b> Variable types</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#types-of-supervised-learning"><i class="fa fa-check"></i><b>7.3</b> Types of Supervised Learning</a></li>
<li class="chapter" data-level="7.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#together-linear-regression-example"><i class="fa fa-check"></i><b>7.4</b> Together: Linear Regression Example</a></li>
<li class="chapter" data-level="7.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#irreducible-error"><i class="fa fa-check"></i><b>7.5</b> Irreducible Error</a></li>
<li class="chapter" data-level="7.6" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#in-class-exercises-irreducible-error"><i class="fa fa-check"></i><b>7.6</b> In-class Exercises: Irreducible Error</a><ul>
<li class="chapter" data-level="7.6.1" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#oracle-regression"><i class="fa fa-check"></i><b>7.6.1</b> Oracle regression</a></li>
<li class="chapter" data-level="7.6.2" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#oracle-classification"><i class="fa fa-check"></i><b>7.6.2</b> Oracle classification</a></li>
<li class="chapter" data-level="7.6.3" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#bonus-random-prediction"><i class="fa fa-check"></i><b>7.6.3</b> (BONUS) Random prediction</a></li>
<li class="chapter" data-level="7.6.4" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#bonus-a-more-non-standard-regression"><i class="fa fa-check"></i><b>7.6.4</b> (BONUS) A more non-standard regression</a></li>
<li class="chapter" data-level="7.6.5" data-path="introduction-to-machine-learning.html"><a href="introduction-to-machine-learning.html#bonus-oracle-mse"><i class="fa fa-check"></i><b>7.6.5</b> (BONUS) Oracle MSE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html"><i class="fa fa-check"></i><b>8</b> Regression in the context of problem solving</a><ul>
<li class="chapter" data-level="8.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#communicating-distillation-3-4"><i class="fa fa-check"></i><b>8.1</b> Communicating (Distillation 3-4)</a></li>
<li class="chapter" data-level="8.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#modelling-distillation-2-3"><i class="fa fa-check"></i><b>8.2</b> Modelling (Distillation 2-3)</a></li>
<li class="chapter" data-level="8.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#asking-useful-statistical-questions-distillation-1-2"><i class="fa fa-check"></i><b>8.3</b> Asking useful statistical questions (Distillation 1-2)</a><ul>
<li class="chapter" data-level="8.3.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#business-objectives-examples"><i class="fa fa-check"></i><b>8.3.1</b> Business objectives: examples</a></li>
<li class="chapter" data-level="8.3.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-refining"><i class="fa fa-check"></i><b>8.3.2</b> Statistical objectives: refining</a></li>
<li class="chapter" data-level="8.3.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-examples"><i class="fa fa-check"></i><b>8.3.3</b> Statistical objectives: examples</a></li>
<li class="chapter" data-level="8.3.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-questions-are-not-the-full-picture"><i class="fa fa-check"></i><b>8.3.4</b> Statistical questions are not the full picture</a></li>
<li class="chapter" data-level="8.3.5" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-unrelated-to-supervised-learning"><i class="fa fa-check"></i><b>8.3.5</b> Statistical objectives unrelated to supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#prerequisites-to-an-analysis"><i class="fa fa-check"></i><b>8.4</b> Prerequisites to an analysis</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="local-regression.html"><a href="local-regression.html"><i class="fa fa-check"></i><b>9</b> Local Regression</a><ul>
<li class="chapter" data-level="9.1" data-path="local-regression.html"><a href="local-regression.html#knn"><i class="fa fa-check"></i><b>9.1</b> kNN</a></li>
<li class="chapter" data-level="9.2" data-path="local-regression.html"><a href="local-regression.html#loess"><i class="fa fa-check"></i><b>9.2</b> loess</a></li>
<li class="chapter" data-level="9.3" data-path="local-regression.html"><a href="local-regression.html#in-class-exercises"><i class="fa fa-check"></i><b>9.3</b> In-Class Exercises</a><ul>
<li class="chapter" data-level="9.3.1" data-path="local-regression.html"><a href="local-regression.html#exercise-1-mean-at-x0"><i class="fa fa-check"></i><b>9.3.1</b> Exercise 1: Mean at <span class="math inline">\(X=0\)</span></a></li>
<li class="chapter" data-level="9.3.2" data-path="local-regression.html"><a href="local-regression.html#exercise-2-regression-curve"><i class="fa fa-check"></i><b>9.3.2</b> Exercise 2: Regression Curve</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="local-regression.html"><a href="local-regression.html#hyperparameters-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>9.4</b> Hyperparameters and the bias/variance tradeoff</a></li>
<li class="chapter" data-level="9.5" data-path="local-regression.html"><a href="local-regression.html#extensions-to-knn-and-loess"><i class="fa fa-check"></i><b>9.5</b> Extensions to kNN and loess</a><ul>
<li class="chapter" data-level="9.5.1" data-path="local-regression.html"><a href="local-regression.html#kernel-weighting"><i class="fa fa-check"></i><b>9.5.1</b> Kernel weighting</a></li>
<li class="chapter" data-level="9.5.2" data-path="local-regression.html"><a href="local-regression.html#local-polynomials"><i class="fa fa-check"></i><b>9.5.2</b> Local polynomials</a></li>
<li class="chapter" data-level="9.5.3" data-path="local-regression.html"><a href="local-regression.html#combination"><i class="fa fa-check"></i><b>9.5.3</b> Combination</a></li>
<li class="chapter" data-level="9.5.4" data-path="local-regression.html"><a href="local-regression.html#other-distances"><i class="fa fa-check"></i><b>9.5.4</b> Other distances</a></li>
<li class="chapter" data-level="9.5.5" data-path="local-regression.html"><a href="local-regression.html#scaling"><i class="fa fa-check"></i><b>9.5.5</b> Scaling</a></li>
<li class="chapter" data-level="9.5.6" data-path="local-regression.html"><a href="local-regression.html#demonstration"><i class="fa fa-check"></i><b>9.5.6</b> Demonstration</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="local-regression.html"><a href="local-regression.html#model-assumptions-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>9.6</b> Model assumptions and the bias/variance tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="reducible-error.html"><a href="reducible-error.html"><i class="fa fa-check"></i><b>10</b> Reducible Error</a><ul>
<li class="chapter" data-level="10.1" data-path="reducible-error.html"><a href="reducible-error.html#classification-exercise-do-together"><i class="fa fa-check"></i><b>10.1</b> Classification Exercise: Do Together</a></li>
<li class="chapter" data-level="10.2" data-path="reducible-error.html"><a href="reducible-error.html#training-error-vs.generalization-error"><i class="fa fa-check"></i><b>10.2</b> Training Error vs. Generalization Error</a></li>
<li class="chapter" data-level="10.3" data-path="reducible-error.html"><a href="reducible-error.html#model-complexity"><i class="fa fa-check"></i><b>10.3</b> Model complexity</a><ul>
<li class="chapter" data-level="10.3.1" data-path="reducible-error.html"><a href="reducible-error.html#activity"><i class="fa fa-check"></i><b>10.3.1</b> Activity</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="reducible-error.html"><a href="reducible-error.html#reducible-error-1"><i class="fa fa-check"></i><b>10.4</b> Reducible Error</a><ul>
<li class="chapter" data-level="10.4.1" data-path="reducible-error.html"><a href="reducible-error.html#what-is-it"><i class="fa fa-check"></i><b>10.4.1</b> What is it?</a></li>
<li class="chapter" data-level="10.4.2" data-path="reducible-error.html"><a href="reducible-error.html#example"><i class="fa fa-check"></i><b>10.4.2</b> Example</a></li>
<li class="chapter" data-level="10.4.3" data-path="reducible-error.html"><a href="reducible-error.html#bias-and-variance"><i class="fa fa-check"></i><b>10.4.3</b> Bias and Variance</a></li>
<li class="chapter" data-level="10.4.4" data-path="reducible-error.html"><a href="reducible-error.html#reducing-reducible-error"><i class="fa fa-check"></i><b>10.4.4</b> Reducing reducible error</a></li>
<li class="chapter" data-level="10.4.5" data-path="reducible-error.html"><a href="reducible-error.html#error-decomposition"><i class="fa fa-check"></i><b>10.4.5</b> Error decomposition</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>11</b> Model Selection</a><ul>
<li class="chapter" data-level="11.1" data-path="model-selection.html"><a href="model-selection.html#exercise-cv"><i class="fa fa-check"></i><b>11.1</b> Exercise: CV</a></li>
<li class="chapter" data-level="11.2" data-path="model-selection.html"><a href="model-selection.html#out-of-sample-error"><i class="fa fa-check"></i><b>11.2</b> Out-of-sample Error</a><ul>
<li class="chapter" data-level="11.2.1" data-path="model-selection.html"><a href="model-selection.html#the-fundamental-problem"><i class="fa fa-check"></i><b>11.2.1</b> The fundamental problem</a></li>
<li class="chapter" data-level="11.2.2" data-path="model-selection.html"><a href="model-selection.html#solution-1-use-a-hold-out-set."><i class="fa fa-check"></i><b>11.2.2</b> Solution 1: Use a hold-out set.</a></li>
<li class="chapter" data-level="11.2.3" data-path="model-selection.html"><a href="model-selection.html#solution-2-cross-validation"><i class="fa fa-check"></i><b>11.2.3</b> Solution 2: Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="model-selection.html"><a href="model-selection.html#alternative-measures-of-model-goodness"><i class="fa fa-check"></i><b>11.3</b> Alternative measures of model goodness</a></li>
<li class="chapter" data-level="11.4" data-path="model-selection.html"><a href="model-selection.html#feature-and-model-selection-setup"><i class="fa fa-check"></i><b>11.4</b> Feature and model selection: setup</a></li>
<li class="chapter" data-level="11.5" data-path="model-selection.html"><a href="model-selection.html#model-selection-1"><i class="fa fa-check"></i><b>11.5</b> Model selection</a></li>
<li class="chapter" data-level="11.6" data-path="model-selection.html"><a href="model-selection.html#feature-predictor-selection"><i class="fa fa-check"></i><b>11.6</b> Feature (predictor) selection</a><ul>
<li class="chapter" data-level="11.6.1" data-path="model-selection.html"><a href="model-selection.html#specialized-metrics-for-feature-selection"><i class="fa fa-check"></i><b>11.6.1</b> Specialized metrics for feature selection</a></li>
<li class="chapter" data-level="11.6.2" data-path="model-selection.html"><a href="model-selection.html#greedy-selection"><i class="fa fa-check"></i><b>11.6.2</b> Greedy Selection</a></li>
<li class="chapter" data-level="11.6.3" data-path="model-selection.html"><a href="model-selection.html#regularization"><i class="fa fa-check"></i><b>11.6.3</b> Regularization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="splines-and-loess-regression.html"><a href="splines-and-loess-regression.html"><i class="fa fa-check"></i><b>12</b> Splines and Loess Regression</a><ul>
<li class="chapter" data-level="12.1" data-path="splines-and-loess-regression.html"><a href="splines-and-loess-regression.html#loess-1"><i class="fa fa-check"></i><b>12.1</b> Loess</a><ul>
<li class="chapter" data-level="12.1.1" data-path="splines-and-loess-regression.html"><a href="splines-and-loess-regression.html#the-moving-window"><i class="fa fa-check"></i><b>12.1.1</b> The “Moving Window”</a></li>
<li class="chapter" data-level="12.1.2" data-path="splines-and-loess-regression.html"><a href="splines-and-loess-regression.html#ggplot2"><i class="fa fa-check"></i><b>12.1.2</b> <code>ggplot2</code></a></li>
<li class="chapter" data-level="12.1.3" data-path="splines-and-loess-regression.html"><a href="splines-and-loess-regression.html#manual-method"><i class="fa fa-check"></i><b>12.1.3</b> Manual method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="model-fitting-in-r.html"><a href="model-fitting-in-r.html"><i class="fa fa-check"></i><b>13</b> Model fitting in R</a><ul>
<li class="chapter" data-level="13.1" data-path="model-fitting-in-r.html"><a href="model-fitting-in-r.html#broom-package"><i class="fa fa-check"></i><b>13.1</b> <code>broom</code> package</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html"><i class="fa fa-check"></i><b>14</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="14.1" data-path="simple-linear-regression.html"><a href="simple-linear-regression.html#model-specification"><i class="fa fa-check"></i><b>14.1</b> Model Specification</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="linear-models-in-general.html"><a href="linear-models-in-general.html"><i class="fa fa-check"></i><b>15</b> Linear models in general</a></li>
<li class="chapter" data-level="16" data-path="reference-treatment-parameterization.html"><a href="reference-treatment-parameterization.html"><i class="fa fa-check"></i><b>16</b> reference-treatment parameterization</a><ul>
<li class="chapter" data-level="16.1" data-path="reference-treatment-parameterization.html"><a href="reference-treatment-parameterization.html#more-than-one-category-lab-2"><i class="fa fa-check"></i><b>16.1</b> More than one category (Lab 2)</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="non-identifiability-in-gams.html"><a href="non-identifiability-in-gams.html"><i class="fa fa-check"></i><b>17</b> Non-identifiability in GAMS</a><ul>
<li class="chapter" data-level="17.1" data-path="non-identifiability-in-gams.html"><a href="non-identifiability-in-gams.html#non-identifiability"><i class="fa fa-check"></i><b>17.1</b> Non-identifiability</a></li>
<li class="chapter" data-level="17.2" data-path="non-identifiability-in-gams.html"><a href="non-identifiability-in-gams.html#question-1b"><i class="fa fa-check"></i><b>17.2</b> Question 1b</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="anova.html"><a href="anova.html"><i class="fa fa-check"></i><b>18</b> ANOVA</a><ul>
<li class="chapter" data-level="18.1" data-path="anova.html"><a href="anova.html#the-types-of-parametric-assumptions"><i class="fa fa-check"></i><b>18.1</b> The types of parametric assumptions</a><ul>
<li class="chapter" data-level="18.1.1" data-path="anova.html"><a href="anova.html#when-defining-a-model-function."><i class="fa fa-check"></i><b>18.1.1</b> 1. When defining a <strong>model function</strong>.</a></li>
<li class="chapter" data-level="18.1.2" data-path="anova.html"><a href="anova.html#when-defining-probability-distributions."><i class="fa fa-check"></i><b>18.1.2</b> 2. When defining <strong>probability distributions</strong>.</a></li>
</ul></li>
<li class="chapter" data-level="18.2" data-path="anova.html"><a href="anova.html#the-value-of-making-parametric-assumptions"><i class="fa fa-check"></i><b>18.2</b> The value of making parametric assumptions</a><ul>
<li class="chapter" data-level="18.2.1" data-path="anova.html"><a href="anova.html#value-1-reduced-error"><i class="fa fa-check"></i><b>18.2.1</b> Value #1: Reduced Error</a></li>
<li class="chapter" data-level="18.2.2" data-path="anova.html"><a href="anova.html#value-2-interpretation"><i class="fa fa-check"></i><b>18.2.2</b> Value #2: Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="anova.html"><a href="anova.html#problems"><i class="fa fa-check"></i><b>18.3</b> Problems</a></li>
<li class="chapter" data-level="18.4" data-path="anova.html"><a href="anova.html#solutions"><i class="fa fa-check"></i><b>18.4</b> Solutions</a><ul>
<li class="chapter" data-level="18.4.1" data-path="anova.html"><a href="anova.html#solution-1-transformations"><i class="fa fa-check"></i><b>18.4.1</b> Solution 1: Transformations</a></li>
<li class="chapter" data-level="18.4.2" data-path="anova.html"><a href="anova.html#solution-2-link-functions"><i class="fa fa-check"></i><b>18.4.2</b> Solution 2: Link Functions</a></li>
<li class="chapter" data-level="18.4.3" data-path="anova.html"><a href="anova.html#solution-3-scientifically-backed-functions"><i class="fa fa-check"></i><b>18.4.3</b> Solution 3: Scientifically-backed functions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="glms-in-r.html"><a href="glms-in-r.html"><i class="fa fa-check"></i><b>19</b> GLM’s in R</a><ul>
<li class="chapter" data-level="19.0.1" data-path="glms-in-r.html"><a href="glms-in-r.html#broomaugment"><i class="fa fa-check"></i><b>19.0.1</b> <code>broom::augment()</code></a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="logistic-regression-paper-with-paul.html"><a href="logistic-regression-paper-with-paul.html"><i class="fa fa-check"></i><b>20</b> Logistic Regression paper with Paul</a><ul>
<li class="chapter" data-level="20.1" data-path="logistic-regression-paper-with-paul.html"><a href="logistic-regression-paper-with-paul.html#the-traditional-approach"><i class="fa fa-check"></i><b>20.1</b> The Traditional Approach</a><ul>
<li class="chapter" data-level="20.1.1" data-path="logistic-regression-paper-with-paul.html"><a href="logistic-regression-paper-with-paul.html#the-linear-probability-model"><i class="fa fa-check"></i><b>20.1.1</b> The Linear Probability Model</a></li>
<li class="chapter" data-level="20.1.2" data-path="logistic-regression-paper-with-paul.html"><a href="logistic-regression-paper-with-paul.html#the-logistic-model"><i class="fa fa-check"></i><b>20.1.2</b> The Logistic Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="robust-regression-in-r.html"><a href="robust-regression-in-r.html"><i class="fa fa-check"></i><b>21</b> Robust Regression in R</a><ul>
<li class="chapter" data-level="21.0.1" data-path="robust-regression-in-r.html"><a href="robust-regression-in-r.html#heavy-tailed-regression"><i class="fa fa-check"></i><b>21.0.1</b> Heavy Tailed Regression</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="from-linear-regression-to-mixed-effects-models.html"><a href="from-linear-regression-to-mixed-effects-models.html"><i class="fa fa-check"></i><b>22</b> From Linear Regression to Mixed Effects Models</a><ul>
<li class="chapter" data-level="22.1" data-path="from-linear-regression-to-mixed-effects-models.html"><a href="from-linear-regression-to-mixed-effects-models.html#motivation-for-lme"><i class="fa fa-check"></i><b>22.1</b> Motivation for LME</a></li>
<li class="chapter" data-level="22.2" data-path="from-linear-regression-to-mixed-effects-models.html"><a href="from-linear-regression-to-mixed-effects-models.html#definition"><i class="fa fa-check"></i><b>22.2</b> Definition</a></li>
<li class="chapter" data-level="22.3" data-path="from-linear-regression-to-mixed-effects-models.html"><a href="from-linear-regression-to-mixed-effects-models.html#r-tools-for-fitting"><i class="fa fa-check"></i><b>22.3</b> R Tools for Fitting</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="mixed-effects-models-in-r-tutorial.html"><a href="mixed-effects-models-in-r-tutorial.html"><i class="fa fa-check"></i><b>23</b> Mixed Effects Models in R: tutorial</a></li>
<li class="chapter" data-level="24" data-path="dsci-562-tutorial-missing-data.html"><a href="dsci-562-tutorial-missing-data.html"><i class="fa fa-check"></i><b>24</b> DSCI 562 Tutorial: Missing Data</a><ul>
<li class="chapter" data-level="24.1" data-path="dsci-562-tutorial-missing-data.html"><a href="dsci-562-tutorial-missing-data.html#mean-imputation"><i class="fa fa-check"></i><b>24.1</b> Mean Imputation</a></li>
<li class="chapter" data-level="24.2" data-path="dsci-562-tutorial-missing-data.html"><a href="dsci-562-tutorial-missing-data.html#multiple-imputation"><i class="fa fa-check"></i><b>24.2</b> Multiple Imputation</a><ul>
<li class="chapter" data-level="24.2.1" data-path="dsci-562-tutorial-missing-data.html"><a href="dsci-562-tutorial-missing-data.html#patterns"><i class="fa fa-check"></i><b>24.2.1</b> Patterns</a></li>
<li class="chapter" data-level="24.2.2" data-path="dsci-562-tutorial-missing-data.html"><a href="dsci-562-tutorial-missing-data.html#multiple-imputation-1"><i class="fa fa-check"></i><b>24.2.2</b> Multiple Imputation</a></li>
<li class="chapter" data-level="24.2.3" data-path="dsci-562-tutorial-missing-data.html"><a href="dsci-562-tutorial-missing-data.html#pooling"><i class="fa fa-check"></i><b>24.2.3</b> Pooling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="25" data-path="spatial.html"><a href="spatial.html"><i class="fa fa-check"></i><b>25</b> Spatial</a><ul>
<li class="chapter" data-level="25.1" data-path="spatial.html"><a href="spatial.html#a-model-for-river-rock-size"><i class="fa fa-check"></i><b>25.1</b> A Model for River Rock Size</a></li>
<li class="chapter" data-level="25.2" data-path="spatial.html"><a href="spatial.html#statistical-objectives"><i class="fa fa-check"></i><b>25.2</b> Statistical Objectives</a><ul>
<li class="chapter" data-level="25.2.1" data-path="spatial.html"><a href="spatial.html#preliminaries-variance-and-correlation"><i class="fa fa-check"></i><b>25.2.1</b> Preliminaries: Variance and Correlation</a></li>
</ul></li>
<li class="chapter" data-level="25.3" data-path="spatial.html"><a href="spatial.html#three-concepts"><i class="fa fa-check"></i><b>25.3</b> Three Concepts</a><ul>
<li class="chapter" data-level="25.3.1" data-path="spatial.html"><a href="spatial.html#error-variance-sigma_e2leftxright"><i class="fa fa-check"></i><b>25.3.1</b> Error Variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span></a></li>
<li class="chapter" data-level="25.3.2" data-path="spatial.html"><a href="spatial.html#mean-variance-sigma_m2"><i class="fa fa-check"></i><b>25.3.2</b> Mean Variance <span class="math inline">\(\sigma_{M}^{2}\)</span></a></li>
<li class="chapter" data-level="25.3.3" data-path="spatial.html"><a href="spatial.html#mean-correlation-rholeftdright"><i class="fa fa-check"></i><b>25.3.3</b> Mean Correlation <span class="math inline">\(\rho\left(d\right)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="spatial.html"><a href="spatial.html#estimation-1"><i class="fa fa-check"></i><b>25.4</b> Estimation</a><ul>
<li class="chapter" data-level="25.4.1" data-path="spatial.html"><a href="spatial.html#constant-error-variance"><i class="fa fa-check"></i><b>25.4.1</b> Constant Error Variance</a></li>
<li class="chapter" data-level="25.4.2" data-path="spatial.html"><a href="spatial.html#non-constant-error-variance"><i class="fa fa-check"></i><b>25.4.2</b> Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="25.5" data-path="spatial.html"><a href="spatial.html#statistical-objective-1-downstream-fining-curve"><i class="fa fa-check"></i><b>25.5</b> Statistical Objective 1: Downstream Fining Curve</a><ul>
<li class="chapter" data-level="25.5.1" data-path="spatial.html"><a href="spatial.html#regression-form"><i class="fa fa-check"></i><b>25.5.1</b> Regression Form</a></li>
</ul></li>
<li class="chapter" data-level="25.6" data-path="spatial.html"><a href="spatial.html#statistical-objective-2-river-profile"><i class="fa fa-check"></i><b>25.6</b> Statistical Objective 2: River Profile</a><ul>
<li class="chapter" data-level="25.6.1" data-path="spatial.html"><a href="spatial.html#simple-kriging"><i class="fa fa-check"></i><b>25.6.1</b> Simple Kriging</a></li>
<li class="chapter" data-level="25.6.2" data-path="spatial.html"><a href="spatial.html#universal-kriging"><i class="fa fa-check"></i><b>25.6.2</b> Universal Kriging</a></li>
<li class="chapter" data-level="25.6.3" data-path="spatial.html"><a href="spatial.html#kriging-under-non-constant-error-variance"><i class="fa fa-check"></i><b>25.6.3</b> Kriging under Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="25.7" data-path="spatial.html"><a href="spatial.html#confidence-intervals-of-the-river-profile"><i class="fa fa-check"></i><b>25.7</b> Confidence Intervals of the River Profile</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="timeseries-in-base-r.html"><a href="timeseries-in-base-r.html"><i class="fa fa-check"></i><b>26</b> Timeseries in (base) R</a></li>
<li class="divider"></li>
<li><a href="https://github.com/vincenzocoia/Interpreting-Regression" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpreting Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression-paper-with-paul" class="section level1">
<h1><span class="header-section-number">Chapter 20</span> Logistic Regression paper with Paul</h1>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<p>The focus of this paper is on medical studies with a binary response, such as survival/death or healthy/diseased. Typically, we are more interested in the probability of the worse outcome and therefore refer to the probability as “risk”, denoted by <span class="math inline">\(\pi\)</span>. Ultimately, we would like to know how certain covariatessuch as smokingaffect the risk. As a means of comparison, it is common to define “exposure” and “baseline” levels of each covariate. For instance, a smoker (exposed) as compared to a non-smoker (baseline) in the case of a binary variable, or exposure to a certain amount of lead as compared to the population average baseline in the case of a continuous variable. Some popular interpretable quantities (IQ’s) which compare exposure risk <span class="math inline">\(\pi_{E}\)</span> to baseline (unexposed) risk <span class="math inline">\(\pi_{B}\)</span> are</p>
<ol style="list-style-type: decimal">
<li><p>the <em>risk difference</em>, <span class="math inline">\(\pi_{E}-\pi_{B}\)</span>,</p></li>
<li><p>the reciprocal risk difference, or <em>number needed to treat</em> (NNT) (or sometimes <em>number needed to harm</em>),</p></li>
<li><p>the <em>relative risk</em>, <span class="math inline">\(\pi_{E}/\pi_{B}\)</span>, and</p></li>
<li><p>the <em>odds ratio</em>,</p></li>
</ol>
<p>These IQ’s consider all other factors to be equal.</p>
<p>The techniques outlined in this paper are generic to either experimental or observational studies, though a strong focus is put on observational studies. In these situations, we must take into account some confounders which would have otherwise been controlled for in an experimental study. As such, we would like to compute the IQ’s after adjusting for the confounders, which can be done by including the confounders as other covariates in the model. To distinguish between the types of covariates, the non-confounder explanatory variables will be called interaction variables.</p>
<p>Let <span class="math inline">\(Y\)</span> denote the binary response. It is convenient to express this variable as <span class="math inline">\(1\)</span> for the outcome of interest (such as diseased) and <span class="math inline">\(0\)</span> for the other outcome, because then its expectation is equal to the risk. Then we have <span class="math display">\[Y|X=x \sim \text{Bernoulli}\left(\pi\left(x\right)\right),\]</span> where <span class="math inline">\(\pi\left(x\right)=E\left(Y\left|X=x\right.\right)\)</span>. The difficulty lies in specifying a model for <span class="math inline">\(\pi\left(x\right)\)</span>. Traditionally, it is modelled using a generalized linear model (GLM), (EQUATION), where <span class="math inline">\(g\)</span> is some increasing function known as the link function, and <span class="math inline">\(\boldsymbol{\beta}=\left(\beta_{1},\ldots,\beta_{p}\right)^{T}\)</span> is a vector of regression parameters. Note that for our purposes it is more convenient to keep the absolute term <span class="math inline">\(\beta_{0}\)</span> separate from the vector <span class="math inline">\(\boldsymbol{\beta}\)</span>. We will discuss two GLM models in Section 2, then discuss the LEXPIT approach of Kovalchik and others (2013) in Section 3. Section 4 is reserved for an appraisal of the LEXPIT method, and a simulation in Section 5 evaluates the LEXPIT model empirically.</p>
<div id="the-traditional-approach" class="section level2">
<h2><span class="header-section-number">20.1</span> The Traditional Approach</h2>
<div id="the-linear-probability-model" class="section level3">
<h3><span class="header-section-number">20.1.1</span> The Linear Probability Model</h3>
<p>A first inclination may be to model the mean as one would in the case of multiple linear regression that is, as a linear combination of the covariates. The link function <span class="math inline">\(g\)</span> is the identity, and the model becomes (EQUATION). Kovalchik and others (2013) refer to this as the “Binomial Linear Model”, or BLM, though it is more commonly known as the “Linear Probability Model”, or LPM (see, for example, Aldrich and Nelson, 1984; Amemiya, 1977; Horrace and Oaxaca, 2006). In this paper, the model is referred to as the LPM.</p>
<p>Before proceeding with any further discussion, the validity of this model must be enforced. The Bernoulli distribution requires <span class="math inline">\(0\leq\pi(X)\leq1\)</span> for all <span class="math inline">\(x\)</span> <span class="math inline">\(\boldsymbol{x}\in XX\)</span> to be a valid distribution. Validity can be ensured by restricting the parameter space of <span class="math inline">\(\left(\beta_{0},\boldsymbol{\beta}\right)\)</span> to . However, the parameter space can be severely restricted depending on the covariate space. For example, if predictor <span class="math inline">\(k\)</span> of is unbounded, then the only allowable value for <span class="math inline">\(\beta_{k}\)</span> is zero. In other words, any covariate in the LPM that has an unbounded range cannot technically be included in the LPM. Further, even if component <span class="math inline">\(k\)</span> is bounded, if it has a large range, then the slope is restricted to be small.</p>
<p>One reason why the LPM is used, despite the above restrictions, is for access to <em>constant interpretable quantities</em> that is, IQ’s discussed in section 1 which do not depend on other covariates. In an LPM, the risk difference by increasing <span class="math inline">\(X_{k}\)</span> by one unit is simply given by <span class="math inline">\(\beta_{k}\)</span>, and the NNT is <span class="math inline">\(1/\beta_{k}\)</span>. However, the relative risk and odds ratio are non-constant, as they are functions of the other covariates.</p>
<p>Since the LPM is just a multiple linear regression model, the regression parameters can be estimated without bias by ordinary least squares (OLS). However, we do not necessarily have homoskedastic errors, since <span class="math inline">\(Var(Y|X=x)\)</span> differs with the covariates. As such, the efficiency of the OLS estimator can be improved by the weighted least squares estimator with weights <span class="math inline">\(1/\sigma\left(\boldsymbol{x}\right)\)</span>. Since these weights are unknown, an iterative algorithm is used, which calculates weights using the fitted probabilities from parameter estimates of the previous step to compute a new “re-weighted” estimator. Iterating this beginning with the OLS estimator converges to the <em>iteratively re-weighted least squares</em> (IRLS) estimator. Amemiya (1977) shows that the IRLS is identical to the maximum likelihood estimator (MLE).</p>
<p>An alternative model which is sometimes confused for the LPM (for example, see Horrace and Oaxaca, 2006) is to allow for an arbitrary parameter space by taking <span class="math inline">\(\pi(x)\)</span> to be zero when <span class="math inline">\(\eta\)</span> is less than zero, and unity otherwise. This model, which I call the “truncated LPM” (TLPM), is (EQUATION) where the inverse-link function <span class="math inline">\(T\)</span> is the ramp function (actually, <span class="math inline">\(T\)</span> is not quite an inverse-link function because it is non-invertible, but this is unimportant). However, as one can see by the differing link function, this is not the LPM, although it is often mistaken for the LPM. Horrace and Oaxaca (2006) mistake the TLPM for the LPM, and in doing so, show that estimation of the model parameters through OLS or IRLS provide biased and inconsistent estimators. This is a good reason why the TLPM should not be used unless a different method of estimation is considered.</p>
</div>
<div id="the-logistic-model" class="section level3">
<h3><span class="header-section-number">20.1.2</span> The Logistic Model</h3>
<p>To rid the parameter space of restrictions, one may consider link functions similar to the ramp function (preferably smooth) to ensure <span class="math inline">\(0\leq\pi(x)\leq1\)</span>. Popular choices are logit, probit, the inverse Gumbel distribution function, or the angular function (Cox and Snell, 1989). Each of these link functions ensures a valid probability for an arbitrary parameter space. The logit link function is a popular choice because it has the best interpretability. It models the log-odds as a linear function of the covariates that is, (EQUATION). This model is known as the logistic regression model, and can be written equivalently as , where is the inverse logit function. The logistic model stands out over models with other link functions because a constant IQ can be obtained from it the odds ratio by increasing <span class="math inline">\(X_{k}\)</span> by one unit is simply <span class="math inline">\(\exp\left(\beta_{k}\right)\)</span>. However, of the interpretable quantities discussed in Section 1, the odds ratio is the most difficult to interpret. Though, if both risks are smallthe “rare disease assumption” with risks under <span class="math inline">\(0.1\)</span> then the odds ratio is a good approximation to the relative risk, which is easier to interpret. The lack of an easy interpretable constant IQ is why some researchers will opt for the LPM instead of the logistic model when the rare disease assumption is invalid. Indeed, this is one major reason behind the study done by Kovalchik and others (2013). One other method to decide is through Goodness of Fit criteria, which was the other deciding factor of Kovalchik and others.</p>
<p>Conveniently, the log odds appears in the likelihood of the logistic model, which simplifies some computations. This leads to the MLE which solves the equation . However, occasionally it is possible that no MLE exists when there is a <span class="math inline">\(\left(b_{0},\boldsymbol{b}\right)\in\mathcal{F}\)</span> such that <span class="math inline">\(b_{0}+\boldsymbol{x}_{i}^{T}\boldsymbol{b}&gt;0\)</span> has <span class="math inline">\(Y_{i}=1\)</span> and <span class="math inline">\(b_{0}+\boldsymbol{x}_{i}^{T}\boldsymbol{b}&lt;0\)</span> has <span class="math inline">\(Y_{i}=0\)</span> for each <span class="math inline">\(i=1,\ldots,n\)</span> (Albert and Anderson, 1984). This is called the case of “complete separation”, and the likelihood has no maximum, so a “perfect fit” is made by infinitely pushing the covariate data to the tails of the expit curve. This is not an issue with the LPM model.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="glms-in-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="robust-regression-in-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/vincenzocoia/Interpreting-Regression/edit/master/091-logistic_regression_report_with_paul.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
