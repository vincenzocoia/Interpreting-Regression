<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 25 Regression when order matters: time series and spatial analysis | Interpreting Regression</title>
  <meta name="description" content="My tutorials for regression analysis, in the form of a bookdown book.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 25 Regression when order matters: time series and spatial analysis | Interpreting Regression" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="My tutorials for regression analysis, in the form of a bookdown book." />
  <meta name="github-repo" content="vincenzocoia/Interpreting-Regression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 25 Regression when order matters: time series and spatial analysis | Interpreting Regression" />
  
  <meta name="twitter:description" content="My tutorials for regression analysis, in the form of a bookdown book." />
  

<meta name="author" content="Vincenzo Coia">


<meta name="date" content="2019-05-29">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regression-on-an-entire-distribution-probabilistic-forecasting.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpreting Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#caution"><i class="fa fa-check"></i><b>1.1</b> Caution</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#purpose-of-the-book"><i class="fa fa-check"></i><b>1.2</b> Purpose of the book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#tasks-that-motivate-regression"><i class="fa fa-check"></i><b>1.3</b> Tasks that motivate Regression</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html"><i class="fa fa-check"></i><b>2</b> Regression in the context of problem solving</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#communicating-distillation-3-4"><i class="fa fa-check"></i><b>2.1</b> Communicating (Distillation 3-4)</a></li>
<li class="chapter" data-level="2.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#modelling-distillation-2-3"><i class="fa fa-check"></i><b>2.2</b> Modelling (Distillation 2-3)</a></li>
<li class="chapter" data-level="2.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#asking-useful-statistical-questions-distillation-1-2"><i class="fa fa-check"></i><b>2.3</b> Asking useful statistical questions (Distillation 1-2)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#business-objectives-examples"><i class="fa fa-check"></i><b>2.3.1</b> Business objectives: examples</a></li>
<li class="chapter" data-level="2.3.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-refining"><i class="fa fa-check"></i><b>2.3.2</b> Statistical objectives: refining</a></li>
<li class="chapter" data-level="2.3.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-examples"><i class="fa fa-check"></i><b>2.3.3</b> Statistical objectives: examples</a></li>
<li class="chapter" data-level="2.3.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-questions-are-not-the-full-picture"><i class="fa fa-check"></i><b>2.3.4</b> Statistical questions are not the full picture</a></li>
<li class="chapter" data-level="2.3.5" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-unrelated-to-supervised-learning"><i class="fa fa-check"></i><b>2.3.5</b> Statistical objectives unrelated to supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#prerequisites-to-an-analysis"><i class="fa fa-check"></i><b>2.4</b> Prerequisites to an analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="an-outcome-on-its-own.html"><a href="an-outcome-on-its-own.html"><i class="fa fa-check"></i>An outcome on its own</a></li>
<li class="chapter" data-level="3" data-path="distributions-uncertainty-is-worth-explaining.html"><a href="distributions-uncertainty-is-worth-explaining.html"><i class="fa fa-check"></i><b>3</b> Distributions: Uncertainty is worth explaining</a></li>
<li class="chapter" data-level="4" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html"><i class="fa fa-check"></i><b>4</b> Explaining an uncertain outcome: interpretable quantities</a><ul>
<li class="chapter" data-level="4.1" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#probabilistic-quantities"><i class="fa fa-check"></i><b>4.1</b> Probabilistic Quantities</a></li>
<li class="chapter" data-level="4.2" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#what-is-the-mean-anyway"><i class="fa fa-check"></i><b>4.2</b> What is the mean, anyway?</a></li>
<li class="chapter" data-level="4.3" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#quantiles"><i class="fa fa-check"></i><b>4.3</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>5</b> Estimation</a><ul>
<li class="chapter" data-level="5.1" data-path="estimation.html"><a href="estimation.html#estimation-of-probabilistic-quantities"><i class="fa fa-check"></i><b>5.1</b> Estimation of Probabilistic Quantities</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html"><i class="fa fa-check"></i><b>6</b> Parametric Families of Distributions</a></li>
<li class="chapter" data-level="" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html"><i class="fa fa-check"></i>Prediction: harnessing the signal</a></li>
<li class="chapter" data-level="7" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html"><i class="fa fa-check"></i><b>7</b> Reducing uncertainty of the outcome: including predictors</a><ul>
<li class="chapter" data-level="7.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-terminology"><i class="fa fa-check"></i><b>7.1</b> Variable terminology</a><ul>
<li class="chapter" data-level="7.1.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-types"><i class="fa fa-check"></i><b>7.1.1</b> Variable types</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#irreducible-error"><i class="fa fa-check"></i><b>7.2</b> Irreducible Error</a></li>
<li class="chapter" data-level="7.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#in-class-exercises-irreducible-error"><i class="fa fa-check"></i><b>7.3</b> In-class Exercises: Irreducible Error</a><ul>
<li class="chapter" data-level="7.3.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-regression"><i class="fa fa-check"></i><b>7.3.1</b> Oracle regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-classification"><i class="fa fa-check"></i><b>7.3.2</b> Oracle classification</a></li>
<li class="chapter" data-level="7.3.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-random-prediction"><i class="fa fa-check"></i><b>7.3.3</b> (BONUS) Random prediction</a></li>
<li class="chapter" data-level="7.3.4" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-a-more-non-standard-regression"><i class="fa fa-check"></i><b>7.3.4</b> (BONUS) A more non-standard regression</a></li>
<li class="chapter" data-level="7.3.5" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-oracle-mse"><i class="fa fa-check"></i><b>7.3.5</b> (BONUS) Oracle MSE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html"><i class="fa fa-check"></i><b>8</b> The signal: model functions</a><ul>
<li class="chapter" data-level="8.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#linear-quantile-regression"><i class="fa fa-check"></i><b>8.1</b> Linear Quantile Regression</a><ul>
<li class="chapter" data-level="8.1.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#exercise"><i class="fa fa-check"></i><b>8.1.1</b> Exercise</a></li>
<li class="chapter" data-level="8.1.2" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-crossing-quantiles"><i class="fa fa-check"></i><b>8.1.2</b> Problem: Crossing quantiles</a></li>
<li class="chapter" data-level="8.1.3" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-upper-quantiles"><i class="fa fa-check"></i><b>8.1.3</b> Problem: Upper quantiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html"><i class="fa fa-check"></i><b>9</b> The Model-Fitting Paradigm in R</a><ul>
<li class="chapter" data-level="9.1" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html#broom-package"><i class="fa fa-check"></i><b>9.1</b> <code>broom</code> package</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html"><i class="fa fa-check"></i><b>10</b> Estimating parametric model functions</a><ul>
<li class="chapter" data-level="10.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#writing-the-sample-mean-as-an-optimization-problem"><i class="fa fa-check"></i><b>10.1</b> Writing the sample mean as an optimization problem</a></li>
<li class="chapter" data-level="10.2" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#evaluating-model-goodness-quantiles"><i class="fa fa-check"></i><b>10.2</b> Evaluating Model Goodness: Quantiles</a></li>
<li class="chapter" data-level="10.3" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#simple-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="10.3.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#model-specification"><i class="fa fa-check"></i><b>10.3.1</b> Model Specification</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#linear-models-in-general"><i class="fa fa-check"></i><b>10.4</b> Linear models in general</a></li>
<li class="chapter" data-level="10.5" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#reference-treatment-parameterization"><i class="fa fa-check"></i><b>10.5</b> reference-treatment parameterization</a><ul>
<li class="chapter" data-level="10.5.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#more-than-one-category-lab-2"><i class="fa fa-check"></i><b>10.5.1</b> More than one category (Lab 2)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><i class="fa fa-check"></i><b>11</b> Estimating assumption-free: the world of supervised learning techniques</a><ul>
<li class="chapter" data-level="11.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#what-machine-learning-is"><i class="fa fa-check"></i><b>11.1</b> What machine learning is</a></li>
<li class="chapter" data-level="11.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#types-of-supervised-learning"><i class="fa fa-check"></i><b>11.2</b> Types of Supervised Learning</a></li>
<li class="chapter" data-level="11.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#local-regression"><i class="fa fa-check"></i><b>11.3</b> Local Regression</a><ul>
<li class="chapter" data-level="11.3.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#knn"><i class="fa fa-check"></i><b>11.3.1</b> kNN</a></li>
<li class="chapter" data-level="11.3.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess"><i class="fa fa-check"></i><b>11.3.2</b> loess</a></li>
<li class="chapter" data-level="11.3.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#in-class-exercises"><i class="fa fa-check"></i><b>11.3.3</b> In-Class Exercises</a></li>
<li class="chapter" data-level="11.3.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#hyperparameters-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>11.3.4</b> Hyperparameters and the bias/variance tradeoff</a></li>
<li class="chapter" data-level="11.3.5" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#extensions-to-knn-and-loess"><i class="fa fa-check"></i><b>11.3.5</b> Extensions to kNN and loess</a></li>
<li class="chapter" data-level="11.3.6" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#model-assumptions-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>11.3.6</b> Model assumptions and the bias/variance tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#splines-and-loess-regression"><i class="fa fa-check"></i><b>11.4</b> Splines and Loess Regression</a><ul>
<li class="chapter" data-level="11.4.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess-1"><i class="fa fa-check"></i><b>11.4.1</b> Loess</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html"><i class="fa fa-check"></i><b>12</b> Overfitting: The problem with adding too many parameters</a><ul>
<li class="chapter" data-level="12.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#classification-exercise-do-together"><i class="fa fa-check"></i><b>12.1</b> Classification Exercise: Do Together</a></li>
<li class="chapter" data-level="12.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#training-error-vs.generalization-error"><i class="fa fa-check"></i><b>12.2</b> Training Error vs. Generalization Error</a></li>
<li class="chapter" data-level="12.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-complexity"><i class="fa fa-check"></i><b>12.3</b> Model complexity</a><ul>
<li class="chapter" data-level="12.3.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#activity"><i class="fa fa-check"></i><b>12.3.1</b> Activity</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducible-error"><i class="fa fa-check"></i><b>12.4</b> Reducible Error</a><ul>
<li class="chapter" data-level="12.4.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#what-is-it"><i class="fa fa-check"></i><b>12.4.1</b> What is it?</a></li>
<li class="chapter" data-level="12.4.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#example"><i class="fa fa-check"></i><b>12.4.2</b> Example</a></li>
<li class="chapter" data-level="12.4.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#bias-and-variance"><i class="fa fa-check"></i><b>12.4.3</b> Bias and Variance</a></li>
<li class="chapter" data-level="12.4.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducing-reducible-error"><i class="fa fa-check"></i><b>12.4.4</b> Reducing reducible error</a></li>
<li class="chapter" data-level="12.4.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#error-decomposition"><i class="fa fa-check"></i><b>12.4.5</b> Error decomposition</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection"><i class="fa fa-check"></i><b>12.5</b> Model Selection</a><ul>
<li class="chapter" data-level="12.5.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#exercise-cv"><i class="fa fa-check"></i><b>12.5.1</b> Exercise: CV</a></li>
<li class="chapter" data-level="12.5.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#out-of-sample-error"><i class="fa fa-check"></i><b>12.5.2</b> Out-of-sample Error</a></li>
<li class="chapter" data-level="12.5.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#alternative-measures-of-model-goodness"><i class="fa fa-check"></i><b>12.5.3</b> Alternative measures of model goodness</a></li>
<li class="chapter" data-level="12.5.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-and-model-selection-setup"><i class="fa fa-check"></i><b>12.5.4</b> Feature and model selection: setup</a></li>
<li class="chapter" data-level="12.5.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection-1"><i class="fa fa-check"></i><b>12.5.5</b> Model selection</a></li>
<li class="chapter" data-level="12.5.6" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-predictor-selection"><i class="fa fa-check"></i><b>12.5.6</b> Feature (predictor) selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="describing-relationships.html"><a href="describing-relationships.html"><i class="fa fa-check"></i>Describing Relationships</a></li>
<li class="chapter" data-level="13" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html"><i class="fa fa-check"></i><b>13</b> There’s meaning in parameters</a><ul>
<li class="chapter" data-level="13.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#the-types-of-parametric-assumptions"><i class="fa fa-check"></i><b>13.1</b> The types of parametric assumptions</a><ul>
<li class="chapter" data-level="13.1.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#when-defining-a-model-function."><i class="fa fa-check"></i><b>13.1.1</b> 1. When defining a <strong>model function</strong>.</a></li>
<li class="chapter" data-level="13.1.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#when-defining-probability-distributions."><i class="fa fa-check"></i><b>13.1.2</b> 2. When defining <strong>probability distributions</strong>.</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#the-value-of-making-parametric-assumptions"><i class="fa fa-check"></i><b>13.2</b> The value of making parametric assumptions</a><ul>
<li class="chapter" data-level="13.2.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#value-1-reduced-error"><i class="fa fa-check"></i><b>13.2.1</b> Value #1: Reduced Error</a></li>
<li class="chapter" data-level="13.2.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#value-2-interpretation"><i class="fa fa-check"></i><b>13.2.2</b> Value #2: Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#anova"><i class="fa fa-check"></i><b>13.3</b> ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="the-meaning-of-interaction.html"><a href="the-meaning-of-interaction.html"><i class="fa fa-check"></i><b>14</b> The meaning of interaction</a></li>
<li class="chapter" data-level="15" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html"><i class="fa fa-check"></i><b>15</b> Scales and the restricted range problem</a><ul>
<li class="chapter" data-level="15.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#problems"><i class="fa fa-check"></i><b>15.1</b> Problems</a></li>
<li class="chapter" data-level="15.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solutions"><i class="fa fa-check"></i><b>15.2</b> Solutions</a><ul>
<li class="chapter" data-level="15.2.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-1-transformations"><i class="fa fa-check"></i><b>15.2.1</b> Solution 1: Transformations</a></li>
<li class="chapter" data-level="15.2.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-2-link-functions"><i class="fa fa-check"></i><b>15.2.2</b> Solution 2: Link Functions</a></li>
<li class="chapter" data-level="15.2.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-3-scientifically-backed-functions"><i class="fa fa-check"></i><b>15.2.3</b> Solution 3: Scientifically-backed functions</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#glms-in-r"><i class="fa fa-check"></i><b>15.3</b> GLM’s in R</a><ul>
<li class="chapter" data-level="15.3.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#broomaugment"><i class="fa fa-check"></i><b>15.3.1</b> <code>broom::augment()</code></a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#options-for-logistic-regression"><i class="fa fa-check"></i><b>15.4</b> Options for Logistic Regression</a><ul>
<li class="chapter" data-level="15.4.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#models"><i class="fa fa-check"></i><b>15.4.1</b> Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="improving-estimation-through-distributional-assumptions.html"><a href="improving-estimation-through-distributional-assumptions.html"><i class="fa fa-check"></i><b>16</b> Improving estimation through distributional assumptions</a></li>
<li class="chapter" data-level="17" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html"><i class="fa fa-check"></i><b>17</b> When we only want interpretation on some predictors</a><ul>
<li class="chapter" data-level="17.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability-in-gams"><i class="fa fa-check"></i><b>17.1</b> Non-identifiability in GAMS</a><ul>
<li class="chapter" data-level="17.1.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability"><i class="fa fa-check"></i><b>17.1.1</b> Non-identifiability</a></li>
<li class="chapter" data-level="17.1.2" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#question-1b"><i class="fa fa-check"></i><b>17.1.2</b> Question 1b</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="special-cases.html"><a href="special-cases.html"><i class="fa fa-check"></i>Special cases</a></li>
<li class="chapter" data-level="18" data-path="regression-when-data-are-censored-survival-analysis.html"><a href="regression-when-data-are-censored-survival-analysis.html"><i class="fa fa-check"></i><b>18</b> Regression when data are censored: survival analysis</a></li>
<li class="chapter" data-level="19" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html"><i class="fa fa-check"></i><b>19</b> Regression in the presence of outliers: robust regression</a><ul>
<li class="chapter" data-level="19.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#robust-regression-in-r"><i class="fa fa-check"></i><b>19.1</b> Robust Regression in R</a><ul>
<li class="chapter" data-level="19.1.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#heavy-tailed-regression"><i class="fa fa-check"></i><b>19.1.1</b> Heavy Tailed Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="regression-in-the-presence-of-extremes-extreme-value-regression.html"><a href="regression-in-the-presence-of-extremes-extreme-value-regression.html"><i class="fa fa-check"></i><b>20</b> Regression in the presence of extremes: extreme value regression</a></li>
<li class="chapter" data-level="21" data-path="regression-when-data-are-ordinal.html"><a href="regression-when-data-are-ordinal.html"><i class="fa fa-check"></i><b>21</b> Regression when data are ordinal</a></li>
<li class="chapter" data-level="22" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html"><i class="fa fa-check"></i><b>22</b> Regression when data are missing: multiple imputation</a><ul>
<li class="chapter" data-level="22.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#mean-imputation"><i class="fa fa-check"></i><b>22.1</b> Mean Imputation</a></li>
<li class="chapter" data-level="22.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation"><i class="fa fa-check"></i><b>22.2</b> Multiple Imputation</a><ul>
<li class="chapter" data-level="22.2.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#patterns"><i class="fa fa-check"></i><b>22.2.1</b> Patterns</a></li>
<li class="chapter" data-level="22.2.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation-1"><i class="fa fa-check"></i><b>22.2.2</b> Multiple Imputation</a></li>
<li class="chapter" data-level="22.2.3" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#pooling"><i class="fa fa-check"></i><b>22.2.3</b> Pooling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html"><i class="fa fa-check"></i><b>23</b> Regression under many groups: mixed effects models</a><ul>
<li class="chapter" data-level="23.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#motivation-for-lme"><i class="fa fa-check"></i><b>23.1</b> Motivation for LME</a><ul>
<li class="chapter" data-level="23.1.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#definition"><i class="fa fa-check"></i><b>23.1.1</b> Definition</a></li>
<li class="chapter" data-level="23.1.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#r-tools-for-fitting"><i class="fa fa-check"></i><b>23.1.2</b> R Tools for Fitting</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#mixed-effects-models-in-r-tutorial"><i class="fa fa-check"></i><b>23.2</b> Mixed Effects Models in R: tutorial</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html"><i class="fa fa-check"></i><b>24</b> Regression on an entire distribution: Probabilistic Forecasting</a><ul>
<li class="chapter" data-level="24.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasting-what-it-is"><i class="fa fa-check"></i><b>24.1</b> Probabilistic Forecasting: What it is</a></li>
<li class="chapter" data-level="24.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#review-univariate-distribution-estimates"><i class="fa fa-check"></i><b>24.2</b> Review: Univariate distribution estimates</a><ul>
<li class="chapter" data-level="24.2.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#continuous-response"><i class="fa fa-check"></i><b>24.2.1</b> Continuous response</a></li>
<li class="chapter" data-level="24.2.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discrete-response"><i class="fa fa-check"></i><b>24.2.2</b> Discrete Response</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasts-subset-based-learning-methods"><i class="fa fa-check"></i><b>24.3</b> Probabilistic Forecasts: subset-based learning methods</a><ul>
<li class="chapter" data-level="24.3.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#the-techniques"><i class="fa fa-check"></i><b>24.3.1</b> The techniques</a></li>
<li class="chapter" data-level="24.3.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#exercise-1"><i class="fa fa-check"></i><b>24.3.2</b> Exercise</a></li>
<li class="chapter" data-level="24.3.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>24.3.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="24.3.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#evaluating-model-goodness"><i class="fa fa-check"></i><b>24.3.4</b> Evaluating Model Goodness</a></li>
</ul></li>
<li class="chapter" data-level="24.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discussion-points"><i class="fa fa-check"></i><b>24.4</b> Discussion Points</a></li>
<li class="chapter" data-level="24.5" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#when-are-they-not-useful"><i class="fa fa-check"></i><b>24.5</b> When are they not useful?</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html"><i class="fa fa-check"></i><b>25</b> Regression when order matters: time series and spatial analysis</a><ul>
<li class="chapter" data-level="25.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#timeseries-in-base-r"><i class="fa fa-check"></i><b>25.1</b> Timeseries in (base) R</a></li>
<li class="chapter" data-level="25.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#spatial-example"><i class="fa fa-check"></i><b>25.2</b> Spatial Example</a></li>
<li class="chapter" data-level="25.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#a-model-for-river-rock-size"><i class="fa fa-check"></i><b>25.3</b> A Model for River Rock Size</a><ul>
<li class="chapter" data-level="25.3.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#average-rock-size"><i class="fa fa-check"></i><b>25.3.1</b> 1. Average rock size:</a></li>
<li class="chapter" data-level="25.3.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-rock-size"><i class="fa fa-check"></i><b>25.3.2</b> 2. Mean rock size:</a></li>
<li class="chapter" data-level="25.3.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#downstream-fining-curve"><i class="fa fa-check"></i><b>25.3.3</b> 3. Downstream fining curve:</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objectives"><i class="fa fa-check"></i><b>25.4</b> Statistical Objectives</a><ul>
<li class="chapter" data-level="25.4.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#preliminaries-variance-and-correlation"><i class="fa fa-check"></i><b>25.4.1</b> Preliminaries: Variance and Correlation</a></li>
</ul></li>
<li class="chapter" data-level="25.5" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#three-concepts"><i class="fa fa-check"></i><b>25.5</b> Three Concepts</a><ul>
<li class="chapter" data-level="25.5.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#error-variance-sigma_e2leftxright"><i class="fa fa-check"></i><b>25.5.1</b> Error Variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span></a></li>
<li class="chapter" data-level="25.5.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-variance-sigma_m2"><i class="fa fa-check"></i><b>25.5.2</b> Mean Variance <span class="math inline">\(\sigma_{M}^{2}\)</span></a></li>
<li class="chapter" data-level="25.5.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-correlation-rholeftdright"><i class="fa fa-check"></i><b>25.5.3</b> Mean Correlation <span class="math inline">\(\rho\left(d\right)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="25.6" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#estimation-1"><i class="fa fa-check"></i><b>25.6</b> Estimation</a><ul>
<li class="chapter" data-level="25.6.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#constant-error-variance"><i class="fa fa-check"></i><b>25.6.1</b> Constant Error Variance</a></li>
<li class="chapter" data-level="25.6.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#non-constant-error-variance"><i class="fa fa-check"></i><b>25.6.2</b> Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="25.7" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-1-downstream-fining-curve"><i class="fa fa-check"></i><b>25.7</b> Statistical Objective 1: Downstream Fining Curve</a><ul>
<li class="chapter" data-level="25.7.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#regression-form"><i class="fa fa-check"></i><b>25.7.1</b> Regression Form</a></li>
</ul></li>
<li class="chapter" data-level="25.8" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-2-river-profile"><i class="fa fa-check"></i><b>25.8</b> Statistical Objective 2: River Profile</a><ul>
<li class="chapter" data-level="25.8.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#simple-kriging"><i class="fa fa-check"></i><b>25.8.1</b> Simple Kriging</a></li>
<li class="chapter" data-level="25.8.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#universal-kriging"><i class="fa fa-check"></i><b>25.8.2</b> Universal Kriging</a></li>
<li class="chapter" data-level="25.8.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#kriging-under-non-constant-error-variance"><i class="fa fa-check"></i><b>25.8.3</b> Kriging under Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="25.9" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#confidence-intervals-of-the-river-profile"><i class="fa fa-check"></i><b>25.9</b> Confidence Intervals of the River Profile</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/vincenzocoia/Interpreting-Regression" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpreting Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-when-order-matters-time-series-and-spatial-analysis" class="section level1">
<h1><span class="header-section-number">Chapter 25</span> Regression when order matters: time series and spatial analysis</h1>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<div id="timeseries-in-base-r" class="section level2">
<h2><span class="header-section-number">25.1</span> Timeseries in (base) R</h2>
<p><strong>To add</strong>: <code>times()</code> function to extract times from a <code>ts</code> object. How to deal with the <code>start</code> and <code>end</code> arguments when declaring a <code>ts</code> object. <code>tsibble</code>.</p>
<p>This tutorial demonstrates <code>timeseries</code> objects, and <code>stl</code> decomposition.</p>
<p>Let’s make a periodic time series with a trend. The data can be contained in a vector:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p &lt;-<span class="st"> </span><span class="dv">10</span>
n &lt;-<span class="st"> </span><span class="dv">20</span><span class="op">*</span>p
dat &lt;-<span class="st"> </span><span class="dv">100</span> <span class="op">+</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">:</span>n) <span class="op">+</span><span class="st"> </span><span class="dv">5</span><span class="op">*</span><span class="kw">sin</span>(<span class="dv">1</span><span class="op">:</span>n <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>pi<span class="op">/</span>p) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(n)</code></pre></div>
<p>It’s sometimes useful to make an object of type <code>timeseries</code>. Do this with the <code>ts</code> function in R. But, if there’s a cycle, we’ll need to indicate that in the <code>frequency</code> argument, which is the number of observations per cycle. In this case, the period is 10.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(datts &lt;-<span class="st"> </span><span class="kw">ts</span>(dat, <span class="dt">frequency =</span> p))</code></pre></div>
<pre><code>## Time Series:
## Start = c(1, 1) 
## End = c(20, 10) 
## Frequency = 10 
##   [1] 103.21754 105.91580 104.83864 104.95717 102.39761 100.16522  99.05889
##   [8]  95.45752  98.82458 102.38751 105.87370 107.90868 107.99882 105.15553
##  [15] 101.85446 100.97420  99.30592  99.43404 103.68415 103.96424 108.49725
##  [22] 107.82864 110.13901 105.91080 104.69072 102.47787 100.28452 102.28791
##  [29] 102.98962 105.07804 108.48962 109.90927 111.88973 107.97297 105.49662
##  [36] 102.94541  99.37422 100.46106 101.93015 105.08458 108.37343 112.91619
##  [43] 110.54639 109.85470 107.58933 104.30931 101.13197 103.51062 103.70481
##  [50] 107.80956 112.64128 112.17872 111.69756 109.02738 107.65240 102.82118
##  [57] 102.55838 101.59620 102.58794 108.39129 108.22220 111.68111 112.38680
##  [64] 109.89085 107.64715 103.95064 104.05229 104.58863 105.80332 110.13285
##  [71] 111.74829 113.48269 112.55860 112.38119 107.30248 106.90023 104.05653
##  [78] 105.55904 105.79333 106.69931 112.30687 114.16458 112.60809 112.06290
##  [85] 107.88068 106.05464 105.06568 104.82710 107.07606 109.59339 112.85742
##  [92] 114.17000 116.02830 112.32351 111.54292 106.05268 103.95440 106.38489
##  [99] 107.75039 108.63916 112.98128 115.25202 113.28984 113.32760 110.88573
## [106] 107.29349 104.92971 106.31116 109.04618 110.50209 113.04955 116.03563
## [113] 113.86822 114.25950 110.16575 108.30924 105.93633 105.57833 108.23428
## [120] 111.13209 112.69828 116.58603 116.62520 114.48533 110.87280 109.05280
## [127] 107.64914 108.50262 108.03575 110.17362 114.24526 114.86152 117.16433
## [134] 115.62492 111.14091 108.68635 108.89239 106.14608 109.30500 111.86138
## [141] 114.12111 116.30034 116.54151 114.48581 112.69920 108.40186 109.04775
## [148] 108.65297 110.73409 110.89099 114.51551 115.45327 117.66094 115.21570
## [155] 112.25735 109.01924 108.33898 105.89822 109.48126 114.40889 115.69664
## [162] 116.23322 119.59398 116.06116 113.98155 109.75989 108.58419 106.47712
## [169] 110.16012 113.75350 115.48076 117.40757 119.30334 116.94545 112.45239
## [176] 110.55772 109.97846 108.06207 109.81071 114.74283 116.27391 116.81357
## [183] 117.35878 114.74181 113.03169 109.94563 109.22033 109.27958 110.81624
## [190] 112.57402 116.78555 117.82568 118.46380 117.92256 113.27395 110.26927
## [197] 108.16507 109.00154 110.52807 112.46218</code></pre>
<p>You can plot this object too. You’ll get a nice looking time series plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(datts)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-104-1.png" width="672" /></p>
<p>And now you can decompose the trend, seasonal component, and error terms with <code>stl</code>. Note that <code>stl</code> requires a <code>timeseries</code> object! Be sure to put <code>s.window=&quot;periodic&quot;</code> in the <code>stl</code> function to use the periodicity of the <code>timeseries</code> object. Notice that there are options to change the bandwidths of the loess estimation, along with the degree of the local polynomial, with the <code>_.degree</code> and <code>_.window</code> arguments.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span><span class="kw">stl</span>(datts, <span class="dt">s.window=</span><span class="st">&quot;periodic&quot;</span>)</code></pre></div>
<p>The estimates are contained in the <code>$time.series</code> part of the output:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(fit<span class="op">$</span>time.series)</code></pre></div>
<pre><code>##         seasonal    trend  remainder
## [1,]  2.81645334 100.9702 -0.5691419
## [2,]  4.49487288 101.1557  0.2652700
## [3,]  4.81270600 101.3411 -1.3151472
## [4,]  2.85720184 101.5169  0.5830240
## [5,] -0.09010948 101.6928  0.7949093
## [6,] -2.98838094 101.8728  1.2808160</code></pre>
</div>
<div id="spatial-example" class="section level2">
<h2><span class="header-section-number">25.2</span> Spatial Example</h2>
<p>Rocks were sampled at 54 sites along the river within a period of two days. The procedure to take one sample involves taking an underwater photo at a glide site along the river, and by using computer software, obtaining the lengths of the intermediate axes of each rock over 8mm in the photo area. To ensure accuracy of measurements, 25 photos of the same area are taken and combined.</p>
</div>
<div id="a-model-for-river-rock-size" class="section level2">
<h2><span class="header-section-number">25.3</span> A Model for River Rock Size</h2>
<p>The variable <span class="math inline">\(x\)</span> refers to a location of some distance downstream the river for example, distance downstream the Meadows campground. For the 54 sample sites, the locations are denoted by <span class="math inline">\(x_{1},\ldots,x_{54}\)</span>. There are three concepts related to rock size.</p>
<div id="average-rock-size" class="section level3">
<h3><span class="header-section-number">25.3.1</span> 1. Average rock size:</h3>
<p>This is the average size of sampled rocks at a location, had that location been sampled. For location <span class="math inline">\(x\)</span>, the value of the sample average rock size is denoted <span class="math inline">\(m\left(x\right)\)</span>. These values are known for 54 sample sites, which are denoted for brevity as <span class="math inline">\(m_{1},\ldots,m_{54}\)</span>, represented as the dots in Figure</p>
</div>
<div id="mean-rock-size" class="section level3">
<h3><span class="header-section-number">25.3.2</span> 2. Mean rock size:</h3>
<p>At a particular river location, this can be thought of as the average rock size in the bed load in the hypothetical situation where the river flows forever in the same condition as during the sampling period. At any location, this quantity is unknown, and will thus be referred to as a “mean” instead of an “average” (an average is a known calculable quantity). At a location <span class="math inline">\(x\)</span>, the mean rock size will be denoted <span class="math inline">\(M\left(x\right)\)</span>, and is represented by the solid line in Figure</p>
</div>
<div id="downstream-fining-curve" class="section level3">
<h3><span class="header-section-number">25.3.3</span> 3. Downstream fining curve:</h3>
<p>This can be thought of as an “overall trend” for rock size from upstream to downstream. The mean rock size <span class="math inline">\(M\left(x\right)\)</span> should “on average” follow this curve. For location <span class="math inline">\(x\)</span>, the value of the downstream fining curve will be denoted <span class="math inline">\(T\left(x\right)\)</span>, represented as the dashed line in Figure . At any location, this is an unknown quantity.</p>
</div>
</div>
<div id="statistical-objectives" class="section level2">
<h2><span class="header-section-number">25.4</span> Statistical Objectives</h2>
<p>Using the three concepts of rock size in Section, the following statistical objectives can be pursued to address the scientific objectives:</p>
<ol style="list-style-type: decimal">
<li><p>Estimate the downstream fining curve <span class="math inline">\(T\left(x\right)\)</span> (i.e. the dashed line in Figure) for the range of the study area;</p></li>
<li><p>Estimate the mean rock size <span class="math inline">\(M\left(x\right)\)</span> (i.e. the solid line in Figure) for the range of the study area, along with confidence bands</p></li>
</ol>
<p>These objectives are addressed in Sections, but first some preliminaries are needed, discussed in Section.</p>
<div id="preliminaries-variance-and-correlation" class="section level3">
<h3><span class="header-section-number">25.4.1</span> Preliminaries: Variance and Correlation</h3>
<p>Most of the techniques introduced in Sections require three descriptions of rock size, which are defined in Section. The procedure for fitting these descriptions to the data is discussed in Section . For a detailed review of these concepts, see Chapter 2 in reference <span class="citation">@Geostatistics</span>, for example.</p>
</div>
</div>
<div id="three-concepts" class="section level2">
<h2><span class="header-section-number">25.5</span> Three Concepts</h2>
<div id="error-variance-sigma_e2leftxright" class="section level3">
<h3><span class="header-section-number">25.5.1</span> Error Variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span></h3>
<p>At location <span class="math inline">\(x\)</span>, the error variance is the variability of the average rock size <span class="math inline">\(m\left(x\right)\)</span> in comparison to the true mean rock size <span class="math inline">\(M\left(x\right)\)</span>. In other words, it is the variance of <span class="math inline">\(\left(m\left(x\right)-M\left(x\right)\right)\)</span>.</p>
</div>
<div id="mean-variance-sigma_m2" class="section level3">
<h3><span class="header-section-number">25.5.2</span> Mean Variance <span class="math inline">\(\sigma_{M}^{2}\)</span></h3>
<p>At location <span class="math inline">\(x\)</span>, the mean variance is the variability of the true mean <span class="math inline">\(M\left(x\right)\)</span> around the downstream fining curve <span class="math inline">\(T\left(x\right)\)</span> (i.e. how “tightly” <span class="math inline">\(M\left(x\right)\)</span> follows <span class="math inline">\(T\left(x\right)\)</span>). It may be reasonable to assume that this variance is constant, unless there is evidence that this variability changes significantly throughout the study area. In what follows we will make this assumption; denote the variance of the true mean rock size as <span class="math inline">\(\sigma_{M}^{2}\)</span>.</p>
</div>
<div id="mean-correlation-rholeftdright" class="section level3">
<h3><span class="header-section-number">25.5.3</span> Mean Correlation <span class="math inline">\(\rho\left(d\right)\)</span></h3>
<p>The mean correlation measures how closely related two mean rock sizes are at different locations (and equivalently, the average rock sizes at those locations). The mean rock sizes at two locations immediately next to each other are expected to be closely related, whereas mean rock sizes at two locations that are far apart may not be related. A key assumption used in Sections is <em>isotropy</em>, that is, the correlation between mean rock sizes at two sites only depends on the distance between the sites, and not on the actual locations along the river. This assumption is of course not exactly true. For example, the relationship between rocks at sites upstream and downstream the dam should be different than the relationship between two equally-spaced sites located without the dam between them. However, the assumption of isotropy should be viewed as an approximation to reality.</p>
<p>If <span class="math inline">\(d\)</span> is the distance between two sites, then <span class="math inline">\(\rho\left(d\right)\)</span> denotes the correlation between mean rock sizes at those sites. In technical terms, <span class="math inline">\(\rho\left(d\right)=\text{Corr}\left(M\left(x\right),M\left(x+d\right)\right)\)</span> for all <span class="math inline">\(x\)</span>, <span class="math inline">\(x+d\)</span> in the study area.</p>
</div>
</div>
<div id="estimation-1" class="section level2">
<h2><span class="header-section-number">25.6</span> Estimation</h2>
<p>The quantities introduced in Section <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span>, <span class="math inline">\(\sigma_{M}^{2}\)</span>, and <span class="math inline">\(\rho\left(d\right)\)</span> are unknown and need to be estimated, because most of the techniques in Sections use them. There is a relatively simple way to do this estimation in the case that the error variance, <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span>, is constant, which is discussed in Section . To account for the changing error variance, an estimation technique is suggested in Section, which may require some manual computations depending on the capabilities of the software you end up using.</p>
<div id="constant-error-variance" class="section level3">
<h3><span class="header-section-number">25.6.1</span> Constant Error Variance</h3>
<p>Although the error variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span> is influenced by sample size <span class="math inline">\(n\left(x\right)\)</span> and distribution variance <span class="math inline">\(\sigma^{2}\left(x\right)\)</span>, approximating the error variance as a constant should be acceptable if the error variances are small compared to the mean variance, <span class="math inline">\(\sigma_{M}^{2}\)</span>. Then, any differences in <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span> amongst different locations would be minuscule relative to <span class="math inline">\(\sigma_{M}^{2}\)</span>. Because your sample sizes are quite large (at least 100), the assumption of small error variances might be reasonable. However, if that assumption is not close to the truth, then the error bars described in Section would be overly wide at locations where the error variance is small (and mean rock size is small), and vice-versa. Since the error variance is assumed constant here, we will denote it <span class="math inline">\(\sigma_{E}^{2}\)</span>, without the <span class="math inline">\(\left(x\right)\)</span>, since it is assumed not to change with <span class="math inline">\(x\)</span>.</p>
<p>One tool that incorporates all the quantities in Section is the <em>variogram</em>, often denoted by <span class="math inline">\(\gamma\left(d\right)\)</span>. For locations separated by a distance of <span class="math inline">\(d\)</span>, it is defined as half the variance of the difference of the sample averages at those sites. Using symbols, the variogram is defined as <span class="math display">\[\gamma\left(d\right)=\frac{1}{2}\text{Var}\left\{ m\left(x\right)-m\left(x+d\right)\right\} ,\]</span> where “Var” means “variance of”. Working out the math, the variogram can be expressed as <span class="math display">\[\gamma\left(d\right)=\sigma_{M}^{2}\left[1-\rho\left(d\right)\right]+\sigma_{E}^{2},\label{eq:Variogram}\]</span> which contains all the quantities we need to estimate. Thus, estimating <span class="math inline">\(\sigma_{E}^{2}\)</span>, <span class="math inline">\(\sigma_{M}^{2}\)</span>, and <span class="math inline">\(\rho\left(d\right)\)</span> amounts to estimating the variogram. A plot of the variogram against <span class="math inline">\(d\)</span> might look something like that in Figure . The bottom dashed line is called a <em>nugget</em>, and the top dashed line is called a <em>sill</em>. The nugget equals <span class="math inline">\(\sigma_{E}^{2}\)</span> and the difference between the sill and the nugget equals <span class="math inline">\(\sigma_{M}^{2}\)</span>.</p>
<p>To estimate a variogram, an <em>empirical variogram</em> is typically used (a “data version” of the variogram). An empirical variogram can be viewed as a scatterplot, where one point is plotted for each pair of sample sites. In particular, for two different site numbers <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, a point is plotted with a vertical value of <span class="math inline">\(0.5\left(m_{i}-m_{j}\right)^{2}\)</span> and a horizontal value of <span class="math inline">\(\left|x_{i}-x_{j}\right|\)</span>, where <span class="math inline">\(\left|\cdot\right|\)</span> refers to the absolute value. Then nonlinear regression is used to fit a variogram model to the data in this scatterplot.</p>
<p>In R, you will need to specify the “model type”. The model type refers to the form of the mean correlation, <span class="math inline">\(\rho\left(d\right)\)</span>. In Figure , the form is exponential, but there are many other forms one can choose, including gaussian, matern, or spherical (see Section 2.5 in Reference <span class="citation">@Geostatistics</span> for a discussion of these models). There is no such thing as a “correct” model, but some may be good approximations to the truth. You can assess whether a model is a good approximation by visually checking whether a plot of the fitted variogram is “close” to the empirical (data) variogram. For a more formal assessment, you can choose the model that results in the smallest Akaike Information Criterion (AIC) value, which quantifies a compromise between model simplicity and goodness-of-fit. Yet another option is to assess the residual plot, where a “residual” here is the difference between a point on the empirical variogram and the theoretical variogram. Although this residual plot is slightly different from “traditional” residual plots (since the points are not independent), ensuring the residuals are roughly centered around zero is still useful for assessing the model fit (you were wondering where an “analysis of residuals” could be used in your analysis this is one place). If several models seem to be a good approximation, then it is best to choose the simplest one (the exponential form is one of the simplest, and most popular).</p>
</div>
<div id="non-constant-error-variance" class="section level3">
<h3><span class="header-section-number">25.6.2</span> Non-Constant Error Variance</h3>
<p>If you fit a variogram model to your data as discussed in Section , and you find that the nugget is fairly large in comparison to the sill, then it is probably not realistic to approximate the error variance by a constant. This is because the fluctuations in the error variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span> (which we know exist) would no longer be small relative to the mean variance <span class="math inline">\(\sigma_{M}^{2}\)</span>.</p>
<p>In the case that you would like to account for the differences in error variance, the variogram then depends on the actual locations being compared, as opposed to just the distance between the locations. For sites <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, the variogram can be expressed as</p>
<p><span class="math display">\[\gamma\left(x,y\right)=\sigma_{M}^{2}\left[1-\rho\left(\left|x-y\right|\right)\right]+\frac{1}{2}\left[\sigma_{E}^{2}\left(x\right)+\sigma_{E}^{2}\left(y\right)\right].\]</span></p>
<p>Compared to Equation , the term on the right is no longer constant. This means that each point in the empirical variogram (as described at the end of Section ) would have a different nugget and sill; in other words, they do not come from a common variogram. As such, direct regression cannot be used to estimate the parameters. However, if we knew the values of <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span> at the sample sites, then you could follow these steps to estimate <span class="math inline">\(\sigma_{M}^{2}\)</span> and <span class="math inline">\(\rho\left(d\right)\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>For all (different) pairs of sites <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, create a modified empirical variogram with dependent variable <span class="math inline">\(0.5\left(m_{i}-m_{j}\right)^{2}-0.5\left[\sigma_{E}^{2}\left(x_{i}\right)-\sigma_{E}^{2}\left(x_{j}\right)\right]\)</span> and independent variable <span class="math inline">\(\left|x_{i}-x_{j}\right|\)</span> ;</p></li>
<li><p>Choose a form for <span class="math inline">\(\rho\left(d\right)\)</span> this could be exponential, gaussian, etc. (see Section 2.5 in Reference <span class="citation">@Geostatistics</span> for a discussion of these models);</p></li>
<li><p>Fit a nonlinear regression model using the mean structure <span class="math inline">\(\gamma\left(d\right)=\sigma_{M}^{2}\left[1-\rho\left(d\right)\right]\)</span> (see Reference <span class="citation">@Nonlinear [@Regression]</span> for details on nonlinear regression);</p></li>
</ol>
<p>Nonlinear regression will provide estimates for <span class="math inline">\(\sigma_{M}^{2}\)</span> and <span class="math inline">\(\rho\left(d\right)\)</span>. As before, an analysis of residuals would be useful here to assess the goodness-of-fit of the regression.</p>
<p>It only remains to estimate <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span> at the sample sites. The simplest case is to use the sample variance of the individual rock sizes at a site, then dividing by the total number of rocks counted in that sample to get the estimate.</p>
<p>One might believe that the variances of rock size at two sites with equal means are approximately equal. If this belief is close to the truth, then there is a more efficient way to estimate the error variances by “pooling” all of the data. This can be done by simple regression of the sample variances of the individual rock sizes (computed by the excel functions or ) against the sample averages. Then <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span> can be estimated as the value of the regression curve evaluated at <span class="math inline">\(x\)</span>, then dividing by the sample size at that site.</p>
</div>
</div>
<div id="statistical-objective-1-downstream-fining-curve" class="section level2">
<h2><span class="header-section-number">25.7</span> Statistical Objective 1: Downstream Fining Curve</h2>
<p>regression of <span class="math inline">\(m_{1},\ldots,m_{54}\)</span> against <span class="math inline">\(x_{1},\ldots,x_{54}\)</span>. there are two other things you might want to change in the regression.</p>
<div id="regression-form" class="section level3">
<h3><span class="header-section-number">25.7.1</span> Regression Form</h3>
<p>there might be a theoretical form that more accurately describes the curve, such as an exponential model: <span class="math display">\[T\left(x\right)=ae^{-bx}.\]</span> If such a theoretical model exists, you may need to consider nonlinear regression, as discussed in Reference <span class="citation">@Nonlinear [@Regression]</span>. If there is no theoretical form, then using a straight line here is probably a reasonable choice as a rough approximation.</p>
</div>
</div>
<div id="statistical-objective-2-river-profile" class="section level2">
<h2><span class="header-section-number">25.8</span> Statistical Objective 2: River Profile</h2>
<p>The goal of this section is to estimate the mean rock size <span class="math inline">\(M\left(x\right)\)</span> at a location <span class="math inline">\(x\)</span> along the study site of the river. Doing this for all locations in the study site, you can obtain a “river profile” of mean rock size.</p>
<p><em>Kriging</em> is a method of estimating mean rock size <span class="math inline">\(M\left(x\right)\)</span> that takes the variances and correlation of the data into consideration. In the case when there is no error variance, Kriging will smoothly “connect the dots” of your data <span class="math inline">\(m_{1},\ldots,m_{54}\)</span>. There are many types of Kriging, but there are two that are directly relevant for this project Simple Kriging and Universal Kriging. These are discussed in Sections and when approximating the error variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span> by a constant, and in Section when allowing the error variance to be non-constant. For a summary of many types of Kriging, see Reference <span class="citation">@arcGIS_Kriging</span>. For an extensive overview of Kriging, see Chapter 3 in Reference <span class="citation">@Geostatistics</span>.</p>
<p>The idea behind ** Kriging is to estimate mean rock size <span class="math inline">\(M\left(x\right)\)</span> by using a “weighted average” of <span class="math inline">\(m_{1},\ldots,m_{54}\)</span>. That is, instead of using the regular average <span class="math inline">\(\frac{1}{54}m_{1}+\cdots+\frac{1}{54}m_{54}\)</span>, Kriging uses <span class="math inline">\(w_{1}\left(x\right)m_{1}+\cdots+w_{54}\left(x\right)m_{54}\)</span> to estimate <span class="math inline">\(M\left(x\right)\)</span>, where the weights <span class="math inline">\(w_{1}\left(x\right),\ldots,w_{54}\left(x\right)\)</span> are nonnegative numbers adding to 1 that depend on the location <span class="math inline">\(x\)</span>. The weights are chosen to minimize variability in the estimate (technically, the “mean squared error”).</p>
<div id="simple-kriging" class="section level3">
<h3><span class="header-section-number">25.8.1</span> Simple Kriging</h3>
<p>Simple Kriging is Kriging when the downstream fining curve, <span class="math inline">\(T\left(x\right)\)</span>, is known. Of course, the downstream fining curve is unknown, but you could get an estimate of it, as discussed in Section . Regarding the implementation of Kriging, if you are not prompted to specify <span class="math inline">\(T\left(x\right)\)</span>, then whichever software you use will likely assume <span class="math inline">\(T\left(x\right)=0\)</span>. In this case, you would need to adjust for this by subtracting <span class="math inline">\(T\left(x_{1}\right),\ldots,T\left(x_{54}\right)\)</span> from your data <span class="math inline">\(m_{1},\ldots,m_{54}\)</span> before running the Kriging procedure, then add <span class="math inline">\(T\left(x\right)\)</span> to your result.</p>
</div>
<div id="universal-kriging" class="section level3">
<h3><span class="header-section-number">25.8.2</span> Universal Kriging</h3>
<p>Universal Kriging is Kriging when the downstream fining curve, <span class="math inline">\(T\left(x\right)\)</span>, is a straight line. In this case, there is no need to estimate <span class="math inline">\(T\left(x\right)\)</span> separately as in Section the estimation is “built-in” to the procedure. It is best to use Universal Kriging if you choose <span class="math inline">\(T\left(x\right)\)</span> to be linear and if you do not estimate <span class="math inline">\(T\left(x\right)\)</span> using GLS.</p>
</div>
<div id="kriging-under-non-constant-error-variance" class="section level3">
<h3><span class="header-section-number">25.8.3</span> Kriging under Non-Constant Error Variance</h3>
<p>To implement Kriging, it is likely that most software would use the variogram estimate in place of the estimates of <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span>, <span class="math inline">\(\sigma_{M}^{2}\)</span>, and <span class="math inline">\(\rho\left(d\right)\)</span>. However, if you want to account for <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span> being different for different locations, then the Kriging procedure would need to use the individual estimates of <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span>, <span class="math inline">\(\sigma_{M}^{2}\)</span>, and <span class="math inline">\(\rho\left(d\right)\)</span> instead of the variogram itself (recall Section for estimating these quantities under the non-constant error variance consideration). See Chapter 3 in Reference <span class="citation">@Geostatistics</span> to find formulas for the Kriging estimator.</p>
</div>
</div>
<div id="confidence-intervals-of-the-river-profile" class="section level2">
<h2><span class="header-section-number">25.9</span> Confidence Intervals of the River Profile</h2>
<p>Various software that have Kriging capabilities most likely also have the ability to construct confidence intervals when prompted. A confidence interval at a location <span class="math inline">\(x\)</span> is an interval that covers the true mean rock size, <span class="math inline">\(M\left(x\right)\)</span>, with approximately some pre-specified chance (such as 95%). When the intervals are plotted over a range of locations, they form a confidence “band” around the river profile.</p>
<p>However, be aware that the confidence intervals in the case of Simple Kriging will be “too narrow”, unless the estimate of the downstream fining curve is quite precise. A confidence interval that is “too narrow” has the implication that the amount of confidence you claim (such as 95%) is actually more than the actual confidence level. This is because the Simple Kriging procedure treats the downstream fining curve <span class="math inline">\(T\left(x\right)\)</span> as known, and does not incorporate the uncertainty involved in estimating <span class="math inline">\(T\left(x\right)\)</span>.</p>
<p>It is likely that the confidence intervals provided by the software are based on the assumption that the data are normally distributed. Since your data <span class="math inline">\(m_{1},\ldots,m_{54}\)</span> are averages, and there are many rocks in each site, this assumption of normality is probably reasonable due to the Central Limit Theorem even though the original rock size distributions are not normally distributed. However, this relies on the assumption that the presence of each rock from a sampling site is not influenced from other rocks (i.e. they are independent). It is not clear whether this is true, but the influence from other rocks might be small enough to make independence a reasonable assumption.</p>
<p>As usual, if you decide to account for the non-constant nature of the error variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span>, then you may need to compute the variance of the mean estimate. These formulas are found in Chapter 3 of Reference <span class="citation">@Geostatistics</span>. You can compute a 95% confidence interval using the variance of the mean estimate (denoting this variance as <span class="math inline">\(V\left(x\right)\)</span> and the estimated mean as <span class="math inline">\(\hat{M}\left(x\right)\)</span>) by <span class="math inline">\(\hat{M}\left(x\right)\pm1.96\sqrt{V\left(x\right)}\)</span>.</p>
<p><span>1</span> Chilès, J.P., and Delfiner, P. (2012) Geostatistics: Modeling Spatial Uncertainty. Second Edition. Wiley.</p>
<p><a href="http://help.arcgis.com/en/arcgisdesktop/10.0/help/index.html\#//00310000003q000000" class="uri">http://help.arcgis.com/en/arcgisdesktop/10.0/help/index.html\#//00310000003q000000</a></p>
<p><a href="http://www2.sas.com/proceedings/sugi27/p213-27.pdf" class="uri">http://www2.sas.com/proceedings/sugi27/p213-27.pdf</a></p>
<p>Huet, S., Bouvier, A., Poursat, M.-A., and Jolivet, E. (2003) Statistical Tools for Nonlinear Regression: A Practical Guide with S-Plus and R Examples. Second Edition. Springer.</p>
<p>Anton, H. (2010) Elementary Linear Algebra. Tenth Edition. Wiley.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-on-an-entire-distribution-probabilistic-forecasting.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/vincenzocoia/Interpreting-Regression/edit/master/221-Regression_when_order_matters.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
