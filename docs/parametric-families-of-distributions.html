<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 6 Parametric Families of Distributions | Interpreting Regression</title>
  <meta name="description" content="A book about the why of regression to help you make decisions about your analysis.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 6 Parametric Families of Distributions | Interpreting Regression" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book about the why of regression to help you make decisions about your analysis." />
  <meta name="github-repo" content="vincenzocoia/Interpreting-Regression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Parametric Families of Distributions | Interpreting Regression" />
  
  <meta name="twitter:description" content="A book about the why of regression to help you make decisions about your analysis." />
  

<meta name="author" content="Vincenzo Coia">


<meta name="date" content="2019-07-30">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="estimation.html">
<link rel="next" href="prediction-harnessing-the-signal.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpreting Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#caution"><i class="fa fa-check"></i><b>1.1</b> Caution</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#purpose-of-the-book"><i class="fa fa-check"></i><b>1.2</b> Purpose of the book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#tasks-that-motivate-regression"><i class="fa fa-check"></i><b>1.3</b> Tasks that motivate Regression</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html"><i class="fa fa-check"></i><b>2</b> Regression in the context of problem solving</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#communicating-distillation-3-4"><i class="fa fa-check"></i><b>2.1</b> Communicating (Distillation 3-4)</a></li>
<li class="chapter" data-level="2.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#modelling-distillation-2-3"><i class="fa fa-check"></i><b>2.2</b> Modelling (Distillation 2-3)</a></li>
<li class="chapter" data-level="2.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#asking-useful-statistical-questions-distillation-1-2"><i class="fa fa-check"></i><b>2.3</b> Asking useful statistical questions (Distillation 1-2)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#business-objectives-examples"><i class="fa fa-check"></i><b>2.3.1</b> Business objectives: examples</a></li>
<li class="chapter" data-level="2.3.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-refining"><i class="fa fa-check"></i><b>2.3.2</b> Statistical objectives: refining</a></li>
<li class="chapter" data-level="2.3.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-examples"><i class="fa fa-check"></i><b>2.3.3</b> Statistical objectives: examples</a></li>
<li class="chapter" data-level="2.3.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-questions-are-not-the-full-picture"><i class="fa fa-check"></i><b>2.3.4</b> Statistical questions are not the full picture</a></li>
<li class="chapter" data-level="2.3.5" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-unrelated-to-supervised-learning"><i class="fa fa-check"></i><b>2.3.5</b> Statistical objectives unrelated to supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#prerequisites-to-an-analysis"><i class="fa fa-check"></i><b>2.4</b> Prerequisites to an analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="an-outcome-on-its-own.html"><a href="an-outcome-on-its-own.html"><i class="fa fa-check"></i>An outcome on its own</a></li>
<li class="chapter" data-level="3" data-path="distributions-uncertainty-is-worth-explaining.html"><a href="distributions-uncertainty-is-worth-explaining.html"><i class="fa fa-check"></i><b>3</b> Distributions: Uncertainty is worth explaining</a></li>
<li class="chapter" data-level="4" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html"><i class="fa fa-check"></i><b>4</b> Explaining an uncertain outcome: interpretable quantities</a><ul>
<li class="chapter" data-level="4.1" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#probabilistic-quantities"><i class="fa fa-check"></i><b>4.1</b> Probabilistic Quantities</a></li>
<li class="chapter" data-level="4.2" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#what-is-the-mean-anyway"><i class="fa fa-check"></i><b>4.2</b> What is the mean, anyway?</a></li>
<li class="chapter" data-level="4.3" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#quantiles"><i class="fa fa-check"></i><b>4.3</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>5</b> Estimation</a><ul>
<li class="chapter" data-level="5.1" data-path="estimation.html"><a href="estimation.html#estimation-of-probabilistic-quantities"><i class="fa fa-check"></i><b>5.1</b> Estimation of Probabilistic Quantities</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html"><i class="fa fa-check"></i><b>6</b> Parametric Families of Distributions</a><ul>
<li class="chapter" data-level="6.1" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#parametric-families-of-distributions-1"><i class="fa fa-check"></i><b>6.1</b> Parametric Families of Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>6.2</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="6.2.1" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#how-mle-works"><i class="fa fa-check"></i><b>6.2.1</b> How MLE works</a></li>
<li class="chapter" data-level="6.2.2" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#motivating-example"><i class="fa fa-check"></i><b>6.2.2</b> Motivating Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html"><i class="fa fa-check"></i>Prediction: harnessing the signal</a><ul>
<li class="chapter" data-level="6.3" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html#variable-terminology"><i class="fa fa-check"></i><b>6.3</b> Variable terminology</a><ul>
<li class="chapter" data-level="6.3.1" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html#variable-types"><i class="fa fa-check"></i><b>6.3.1</b> Variable types</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html#irreducible-error"><i class="fa fa-check"></i><b>6.4</b> Irreducible Error</a></li>
<li class="chapter" data-level="6.5" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html#in-class-exercises-irreducible-error"><i class="fa fa-check"></i><b>6.5</b> In-class Exercises: Irreducible Error</a><ul>
<li class="chapter" data-level="6.5.1" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html#oracle-regression"><i class="fa fa-check"></i><b>6.5.1</b> Oracle regression</a></li>
<li class="chapter" data-level="6.5.2" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html#oracle-classification"><i class="fa fa-check"></i><b>6.5.2</b> Oracle classification</a></li>
<li class="chapter" data-level="6.5.3" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html#bonus-random-prediction"><i class="fa fa-check"></i><b>6.5.3</b> (BONUS) Random prediction</a></li>
<li class="chapter" data-level="6.5.4" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html#bonus-a-more-non-standard-regression"><i class="fa fa-check"></i><b>6.5.4</b> (BONUS) A more non-standard regression</a></li>
<li class="chapter" data-level="6.5.5" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html#bonus-oracle-mse"><i class="fa fa-check"></i><b>6.5.5</b> (BONUS) Oracle MSE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html"><i class="fa fa-check"></i><b>7</b> The signal: model functions</a><ul>
<li class="chapter" data-level="7.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#linear-quantile-regression"><i class="fa fa-check"></i><b>7.1</b> Linear Quantile Regression</a><ul>
<li class="chapter" data-level="7.1.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#exercise"><i class="fa fa-check"></i><b>7.1.1</b> Exercise</a></li>
<li class="chapter" data-level="7.1.2" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-crossing-quantiles"><i class="fa fa-check"></i><b>7.1.2</b> Problem: Crossing quantiles</a></li>
<li class="chapter" data-level="7.1.3" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-upper-quantiles"><i class="fa fa-check"></i><b>7.1.3</b> Problem: Upper quantiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html"><i class="fa fa-check"></i><b>8</b> The Model-Fitting Paradigm in R</a><ul>
<li class="chapter" data-level="8.1" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html#broom-package"><i class="fa fa-check"></i><b>8.1</b> <code>broom</code> package</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html"><i class="fa fa-check"></i><b>9</b> Estimating parametric model functions</a><ul>
<li class="chapter" data-level="9.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#writing-the-sample-mean-as-an-optimization-problem"><i class="fa fa-check"></i><b>9.1</b> Writing the sample mean as an optimization problem</a></li>
<li class="chapter" data-level="9.2" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#evaluating-model-goodness-quantiles"><i class="fa fa-check"></i><b>9.2</b> Evaluating Model Goodness: Quantiles</a></li>
<li class="chapter" data-level="9.3" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#simple-linear-regression"><i class="fa fa-check"></i><b>9.3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="9.3.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#model-specification"><i class="fa fa-check"></i><b>9.3.1</b> Model Specification</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#linear-models-in-general"><i class="fa fa-check"></i><b>9.4</b> Linear models in general</a></li>
<li class="chapter" data-level="9.5" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#reference-treatment-parameterization"><i class="fa fa-check"></i><b>9.5</b> reference-treatment parameterization</a><ul>
<li class="chapter" data-level="9.5.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#more-than-one-category-lab-2"><i class="fa fa-check"></i><b>9.5.1</b> More than one category (Lab 2)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><i class="fa fa-check"></i><b>10</b> Estimating assumption-free: the world of supervised learning techniques</a><ul>
<li class="chapter" data-level="10.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#what-machine-learning-is"><i class="fa fa-check"></i><b>10.1</b> What machine learning is</a></li>
<li class="chapter" data-level="10.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#types-of-supervised-learning"><i class="fa fa-check"></i><b>10.2</b> Types of Supervised Learning</a></li>
<li class="chapter" data-level="10.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#local-regression"><i class="fa fa-check"></i><b>10.3</b> Local Regression</a><ul>
<li class="chapter" data-level="10.3.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#knn"><i class="fa fa-check"></i><b>10.3.1</b> kNN</a></li>
<li class="chapter" data-level="10.3.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess"><i class="fa fa-check"></i><b>10.3.2</b> loess</a></li>
<li class="chapter" data-level="10.3.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#in-class-exercises"><i class="fa fa-check"></i><b>10.3.3</b> In-Class Exercises</a></li>
<li class="chapter" data-level="10.3.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#hyperparameters-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>10.3.4</b> Hyperparameters and the bias/variance tradeoff</a></li>
<li class="chapter" data-level="10.3.5" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#extensions-to-knn-and-loess"><i class="fa fa-check"></i><b>10.3.5</b> Extensions to kNN and loess</a></li>
<li class="chapter" data-level="10.3.6" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#model-assumptions-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>10.3.6</b> Model assumptions and the bias/variance tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#splines-and-loess-regression"><i class="fa fa-check"></i><b>10.4</b> Splines and Loess Regression</a><ul>
<li class="chapter" data-level="10.4.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess-1"><i class="fa fa-check"></i><b>10.4.1</b> Loess</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html"><i class="fa fa-check"></i><b>11</b> Overfitting: The problem with adding too many parameters</a><ul>
<li class="chapter" data-level="11.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#classification-exercise-do-together"><i class="fa fa-check"></i><b>11.1</b> Classification Exercise: Do Together</a></li>
<li class="chapter" data-level="11.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#training-error-vs.generalization-error"><i class="fa fa-check"></i><b>11.2</b> Training Error vs. Generalization Error</a></li>
<li class="chapter" data-level="11.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-complexity"><i class="fa fa-check"></i><b>11.3</b> Model complexity</a><ul>
<li class="chapter" data-level="11.3.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#activity"><i class="fa fa-check"></i><b>11.3.1</b> Activity</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducible-error"><i class="fa fa-check"></i><b>11.4</b> Reducible Error</a><ul>
<li class="chapter" data-level="11.4.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#what-is-it"><i class="fa fa-check"></i><b>11.4.1</b> What is it?</a></li>
<li class="chapter" data-level="11.4.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#example"><i class="fa fa-check"></i><b>11.4.2</b> Example</a></li>
<li class="chapter" data-level="11.4.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#bias-and-variance"><i class="fa fa-check"></i><b>11.4.3</b> Bias and Variance</a></li>
<li class="chapter" data-level="11.4.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducing-reducible-error"><i class="fa fa-check"></i><b>11.4.4</b> Reducing reducible error</a></li>
<li class="chapter" data-level="11.4.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#error-decomposition"><i class="fa fa-check"></i><b>11.4.5</b> Error decomposition</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection"><i class="fa fa-check"></i><b>11.5</b> Model Selection</a><ul>
<li class="chapter" data-level="11.5.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#exercise-cv"><i class="fa fa-check"></i><b>11.5.1</b> Exercise: CV</a></li>
<li class="chapter" data-level="11.5.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#out-of-sample-error"><i class="fa fa-check"></i><b>11.5.2</b> Out-of-sample Error</a></li>
<li class="chapter" data-level="11.5.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#alternative-measures-of-model-goodness"><i class="fa fa-check"></i><b>11.5.3</b> Alternative measures of model goodness</a></li>
<li class="chapter" data-level="11.5.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-and-model-selection-setup"><i class="fa fa-check"></i><b>11.5.4</b> Feature and model selection: setup</a></li>
<li class="chapter" data-level="11.5.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection-1"><i class="fa fa-check"></i><b>11.5.5</b> Model selection</a></li>
<li class="chapter" data-level="11.5.6" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-predictor-selection"><i class="fa fa-check"></i><b>11.5.6</b> Feature (predictor) selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="describing-relationships.html"><a href="describing-relationships.html"><i class="fa fa-check"></i>Describing Relationships</a><ul>
<li class="chapter" data-level="11.6" data-path="describing-relationships.html"><a href="describing-relationships.html#the-types-of-parametric-assumptions"><i class="fa fa-check"></i><b>11.6</b> The types of parametric assumptions</a><ul>
<li class="chapter" data-level="11.6.1" data-path="describing-relationships.html"><a href="describing-relationships.html#when-defining-a-model-function."><i class="fa fa-check"></i><b>11.6.1</b> 1. When defining a <strong>model function</strong>.</a></li>
<li class="chapter" data-level="11.6.2" data-path="describing-relationships.html"><a href="describing-relationships.html#when-defining-probability-distributions."><i class="fa fa-check"></i><b>11.6.2</b> 2. When defining <strong>probability distributions</strong>.</a></li>
</ul></li>
<li class="chapter" data-level="11.7" data-path="describing-relationships.html"><a href="describing-relationships.html#the-value-of-making-parametric-assumptions"><i class="fa fa-check"></i><b>11.7</b> The value of making parametric assumptions</a><ul>
<li class="chapter" data-level="11.7.1" data-path="describing-relationships.html"><a href="describing-relationships.html#value-1-reduced-error"><i class="fa fa-check"></i><b>11.7.1</b> Value #1: Reduced Error</a></li>
<li class="chapter" data-level="11.7.2" data-path="describing-relationships.html"><a href="describing-relationships.html#value-2-interpretation"><i class="fa fa-check"></i><b>11.7.2</b> Value #2: Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="11.8" data-path="describing-relationships.html"><a href="describing-relationships.html#anova"><i class="fa fa-check"></i><b>11.8</b> ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="the-meaning-of-interaction.html"><a href="the-meaning-of-interaction.html"><i class="fa fa-check"></i><b>12</b> The meaning of interaction</a></li>
<li class="chapter" data-level="13" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html"><i class="fa fa-check"></i><b>13</b> Scales and the restricted range problem</a><ul>
<li class="chapter" data-level="13.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#problems"><i class="fa fa-check"></i><b>13.1</b> Problems</a></li>
<li class="chapter" data-level="13.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solutions"><i class="fa fa-check"></i><b>13.2</b> Solutions</a><ul>
<li class="chapter" data-level="13.2.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-1-transformations"><i class="fa fa-check"></i><b>13.2.1</b> Solution 1: Transformations</a></li>
<li class="chapter" data-level="13.2.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-2-link-functions"><i class="fa fa-check"></i><b>13.2.2</b> Solution 2: Link Functions</a></li>
<li class="chapter" data-level="13.2.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-3-scientifically-backed-functions"><i class="fa fa-check"></i><b>13.2.3</b> Solution 3: Scientifically-backed functions</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#glms-in-r"><i class="fa fa-check"></i><b>13.3</b> GLM’s in R</a><ul>
<li class="chapter" data-level="13.3.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#broomaugment"><i class="fa fa-check"></i><b>13.3.1</b> <code>broom::augment()</code></a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#options-for-logistic-regression"><i class="fa fa-check"></i><b>13.4</b> Options for Logistic Regression</a><ul>
<li class="chapter" data-level="13.4.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#models"><i class="fa fa-check"></i><b>13.4.1</b> Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="improving-estimation-through-distributional-assumptions.html"><a href="improving-estimation-through-distributional-assumptions.html"><i class="fa fa-check"></i><b>14</b> Improving estimation through distributional assumptions</a></li>
<li class="chapter" data-level="15" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html"><i class="fa fa-check"></i><b>15</b> When we only want interpretation on some predictors</a><ul>
<li class="chapter" data-level="15.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability-in-gams"><i class="fa fa-check"></i><b>15.1</b> Non-identifiability in GAMS</a><ul>
<li class="chapter" data-level="15.1.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability"><i class="fa fa-check"></i><b>15.1.1</b> Non-identifiability</a></li>
<li class="chapter" data-level="15.1.2" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#question-1b"><i class="fa fa-check"></i><b>15.1.2</b> Question 1b</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="special-cases.html"><a href="special-cases.html"><i class="fa fa-check"></i>Special cases</a></li>
<li class="chapter" data-level="16" data-path="regression-when-data-are-censored-survival-analysis.html"><a href="regression-when-data-are-censored-survival-analysis.html"><i class="fa fa-check"></i><b>16</b> Regression when data are censored: survival analysis</a></li>
<li class="chapter" data-level="17" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html"><i class="fa fa-check"></i><b>17</b> Regression in the presence of outliers: robust regression</a><ul>
<li class="chapter" data-level="17.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#robust-regression-in-r"><i class="fa fa-check"></i><b>17.1</b> Robust Regression in R</a><ul>
<li class="chapter" data-level="17.1.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#heavy-tailed-regression"><i class="fa fa-check"></i><b>17.1.1</b> Heavy Tailed Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="regression-in-the-presence-of-extremes-extreme-value-regression.html"><a href="regression-in-the-presence-of-extremes-extreme-value-regression.html"><i class="fa fa-check"></i><b>18</b> Regression in the presence of extremes: extreme value regression</a></li>
<li class="chapter" data-level="19" data-path="regression-when-data-are-ordinal.html"><a href="regression-when-data-are-ordinal.html"><i class="fa fa-check"></i><b>19</b> Regression when data are ordinal</a></li>
<li class="chapter" data-level="20" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html"><i class="fa fa-check"></i><b>20</b> Regression when data are missing: multiple imputation</a><ul>
<li class="chapter" data-level="20.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#mean-imputation"><i class="fa fa-check"></i><b>20.1</b> Mean Imputation</a></li>
<li class="chapter" data-level="20.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation"><i class="fa fa-check"></i><b>20.2</b> Multiple Imputation</a><ul>
<li class="chapter" data-level="20.2.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#patterns"><i class="fa fa-check"></i><b>20.2.1</b> Patterns</a></li>
<li class="chapter" data-level="20.2.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation-1"><i class="fa fa-check"></i><b>20.2.2</b> Multiple Imputation</a></li>
<li class="chapter" data-level="20.2.3" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#pooling"><i class="fa fa-check"></i><b>20.2.3</b> Pooling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html"><i class="fa fa-check"></i><b>21</b> Regression under many groups: mixed effects models</a><ul>
<li class="chapter" data-level="21.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#motivation-for-lme"><i class="fa fa-check"></i><b>21.1</b> Motivation for LME</a><ul>
<li class="chapter" data-level="21.1.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#definition"><i class="fa fa-check"></i><b>21.1.1</b> Definition</a></li>
<li class="chapter" data-level="21.1.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#r-tools-for-fitting"><i class="fa fa-check"></i><b>21.1.2</b> R Tools for Fitting</a></li>
</ul></li>
<li class="chapter" data-level="21.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#mixed-effects-models-in-r-tutorial"><i class="fa fa-check"></i><b>21.2</b> Mixed Effects Models in R: tutorial</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html"><i class="fa fa-check"></i><b>22</b> Regression on an entire distribution: Probabilistic Forecasting</a><ul>
<li class="chapter" data-level="22.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasting-what-it-is"><i class="fa fa-check"></i><b>22.1</b> Probabilistic Forecasting: What it is</a></li>
<li class="chapter" data-level="22.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#review-univariate-distribution-estimates"><i class="fa fa-check"></i><b>22.2</b> Review: Univariate distribution estimates</a><ul>
<li class="chapter" data-level="22.2.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#continuous-response"><i class="fa fa-check"></i><b>22.2.1</b> Continuous response</a></li>
<li class="chapter" data-level="22.2.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discrete-response"><i class="fa fa-check"></i><b>22.2.2</b> Discrete Response</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasts-subset-based-learning-methods"><i class="fa fa-check"></i><b>22.3</b> Probabilistic Forecasts: subset-based learning methods</a><ul>
<li class="chapter" data-level="22.3.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#the-techniques"><i class="fa fa-check"></i><b>22.3.1</b> The techniques</a></li>
<li class="chapter" data-level="22.3.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#exercise-1"><i class="fa fa-check"></i><b>22.3.2</b> Exercise</a></li>
<li class="chapter" data-level="22.3.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>22.3.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="22.3.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#evaluating-model-goodness"><i class="fa fa-check"></i><b>22.3.4</b> Evaluating Model Goodness</a></li>
</ul></li>
<li class="chapter" data-level="22.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discussion-points"><i class="fa fa-check"></i><b>22.4</b> Discussion Points</a></li>
<li class="chapter" data-level="22.5" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#when-are-they-not-useful"><i class="fa fa-check"></i><b>22.5</b> When are they not useful?</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html"><i class="fa fa-check"></i><b>23</b> Regression when order matters: time series and spatial analysis</a><ul>
<li class="chapter" data-level="23.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#timeseries-in-base-r"><i class="fa fa-check"></i><b>23.1</b> Timeseries in (base) R</a></li>
<li class="chapter" data-level="23.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#spatial-example"><i class="fa fa-check"></i><b>23.2</b> Spatial Example</a></li>
<li class="chapter" data-level="23.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#a-model-for-river-rock-size"><i class="fa fa-check"></i><b>23.3</b> A Model for River Rock Size</a><ul>
<li class="chapter" data-level="23.3.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#average-rock-size"><i class="fa fa-check"></i><b>23.3.1</b> 1. Average rock size:</a></li>
<li class="chapter" data-level="23.3.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-rock-size"><i class="fa fa-check"></i><b>23.3.2</b> 2. Mean rock size:</a></li>
<li class="chapter" data-level="23.3.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#downstream-fining-curve"><i class="fa fa-check"></i><b>23.3.3</b> 3. Downstream fining curve:</a></li>
</ul></li>
<li class="chapter" data-level="23.4" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objectives"><i class="fa fa-check"></i><b>23.4</b> Statistical Objectives</a><ul>
<li class="chapter" data-level="23.4.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#preliminaries-variance-and-correlation"><i class="fa fa-check"></i><b>23.4.1</b> Preliminaries: Variance and Correlation</a></li>
</ul></li>
<li class="chapter" data-level="23.5" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#three-concepts"><i class="fa fa-check"></i><b>23.5</b> Three Concepts</a><ul>
<li class="chapter" data-level="23.5.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#error-variance-sigma_e2leftxright"><i class="fa fa-check"></i><b>23.5.1</b> Error Variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span></a></li>
<li class="chapter" data-level="23.5.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-variance-sigma_m2"><i class="fa fa-check"></i><b>23.5.2</b> Mean Variance <span class="math inline">\(\sigma_{M}^{2}\)</span></a></li>
<li class="chapter" data-level="23.5.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-correlation-rholeftdright"><i class="fa fa-check"></i><b>23.5.3</b> Mean Correlation <span class="math inline">\(\rho\left(d\right)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="23.6" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#estimation-1"><i class="fa fa-check"></i><b>23.6</b> Estimation</a><ul>
<li class="chapter" data-level="23.6.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#constant-error-variance"><i class="fa fa-check"></i><b>23.6.1</b> Constant Error Variance</a></li>
<li class="chapter" data-level="23.6.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#non-constant-error-variance"><i class="fa fa-check"></i><b>23.6.2</b> Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="23.7" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-1-downstream-fining-curve"><i class="fa fa-check"></i><b>23.7</b> Statistical Objective 1: Downstream Fining Curve</a><ul>
<li class="chapter" data-level="23.7.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#regression-form"><i class="fa fa-check"></i><b>23.7.1</b> Regression Form</a></li>
</ul></li>
<li class="chapter" data-level="23.8" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-2-river-profile"><i class="fa fa-check"></i><b>23.8</b> Statistical Objective 2: River Profile</a><ul>
<li class="chapter" data-level="23.8.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#simple-kriging"><i class="fa fa-check"></i><b>23.8.1</b> Simple Kriging</a></li>
<li class="chapter" data-level="23.8.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#universal-kriging"><i class="fa fa-check"></i><b>23.8.2</b> Universal Kriging</a></li>
<li class="chapter" data-level="23.8.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#kriging-under-non-constant-error-variance"><i class="fa fa-check"></i><b>23.8.3</b> Kriging under Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="23.9" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#confidence-intervals-of-the-river-profile"><i class="fa fa-check"></i><b>23.9</b> Confidence Intervals of the River Profile</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/vincenzocoia/Interpreting-Regression" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpreting Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="parametric-families-of-distributions" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Parametric Families of Distributions</h1>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<p>Concepts:</p>
<ul>
<li>Common scales: Positive ratio scale, binary, (0,1)</li>
<li>Different data generating processes give rise to various <em>parametric families</em> of distributions. We’ll explain a good chunk of them.</li>
<li>These are useful in data analysis because they narrow down the things that need to be estimated. Improving estimator quality by parametric distributional assumptions and MLE</li>
</ul>
<div id="parametric-families-of-distributions-1" class="section level2">
<h2><span class="header-section-number">6.1</span> Parametric Families of Distributions</h2>
<p>What is meant by “family”? This means that there are more than one of them, and that a specific distribution from the family can be characterized the family’s parameters. This is what is meant by “parametric” – the distribution can be distilled down to a set of parameters.</p>
<p>For example, to identify a Gaussian distribution, we need to know the mean and variance. Note that some families are characterized by parameters that do not necessarily have an interpretation (or at least an easy one) – for example, to identify a Beta distribution, we need to know the two shape parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
<p><strong>Technicality that you can safely skip</strong>: It’s not that there are “set” parameters that identify a distribution. For example, although a Beta distribution is identified by the two shape parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, the distribution can also be uniquely identified by its mean and variance. Or, by two quantiles. In general, we need as many pieces of information as there are parameters. This needs to be done in an <em>identifiable</em> way, however. For example, it’s not enough to identify an Exponential distibution by its skewness, because all Exponential distributions have a skewness of 2! The way in which we identify a distribution from parameters is called the <em>parameterization</em> of the distribution.</p>
</div>
<div id="maximum-likelihood-estimation" class="section level2">
<h2><span class="header-section-number">6.2</span> Maximum Likelihood Estimation</h2>
<p>In the previous chapter, we estimated parameters like means and probabilities using “sample versions” of these parameters. Usually, these estimators perform well in the univariate setting, but there are some circumstances where they do not. There is a technique called <em>Maximum Likelihood Estimation</em> (MLE) that approximates the data distribution using a parametric family, and if done carefully, allows for significant improvements to estimation.</p>
<p>This chapter first explains what MLE is and how to implement the technique, as well as why and when it would be of use.</p>
<div id="how-mle-works" class="section level3">
<h3><span class="header-section-number">6.2.1</span> How MLE works</h3>
<p>Maximum likelihood estimation is a way of estimating parameters <em>by first estimating the data distribution</em> from a specified parametric family. The steps are as follows.</p>
<ol style="list-style-type: decimal">
<li><strong>Make a distributional assumption</strong>: Choose a parametric family of distributions that you think is a decent approximation to the data distribution.</li>
<li><strong>Estimate</strong>: From that family, choose the distribution that fits the data “best”.</li>
<li><strong>Extract</strong>: Using the fitted distribution, extract the parameter(s) of interest (such as the mean and/or quantiles).</li>
<li><strong>Check the assumption</strong>: Check that the fitted distribution is a reasonable approximation to the data distribution (not required for estimation, but is good practice).</li>
</ol>
<p>Let’s look at these steps in turn. As an example, consider the following data set of damages caused by hurricanes, in billions of USD. The distribution of the 144 observations is depicted in the following histogram:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># library(extRemes)</span>
<span class="kw">library</span>(tidyverse)</code></pre></div>
<pre><code>## ── Attaching packages ──────────────────────────────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ ggplot2 3.1.1          ✔ purrr   0.3.2     
## ✔ tibble  2.1.3          ✔ dplyr   0.8.3     
## ✔ tidyr   0.8.3.9000     ✔ stringr 1.4.0     
## ✔ readr   1.1.1          ✔ forcats 0.3.0</code></pre>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;tibble&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;purrr&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;dplyr&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;stringr&#39; was built under R version 3.5.2</code></pre>
<pre><code>## ── Conflicts ─────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(damage, <span class="dt">package =</span> <span class="st">&quot;extRemes&quot;</span>)
(hurricane_hist &lt;-<span class="st"> </span><span class="kw">ggplot</span>(damage, <span class="kw">aes</span>(Dam)) <span class="op">+</span>
<span class="st">        </span><span class="kw">geom_histogram</span>(<span class="kw">aes</span>(<span class="dt">y =</span> ..density..), <span class="dt">bins =</span> <span class="dv">50</span>) <span class="op">+</span>
<span class="st">        </span><span class="kw">theme_bw</span>() <span class="op">+</span>
<span class="st">        </span><span class="kw">scale_x_continuous</span>(<span class="st">&quot;Damage (billions of USD)&quot;</span>, 
                           <span class="dt">labels =</span> scales<span class="op">::</span><span class="kw">dollar_format</span>()) <span class="op">+</span>
<span class="st">        </span><span class="kw">ylab</span>(<span class="st">&quot;Density&quot;</span>))</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<div id="step-1-make-a-distributional-assumption" class="section level4">
<h4><span class="header-section-number">6.2.1.1</span> Step 1: Make a distributional assumption</h4>
<p>This step requires choosing a family to approximate the data distribution (you can find several families defined in the previous chapter). Here are two general guidelines that may help you choose a distribution family.</p>
<ol style="list-style-type: decimal">
<li>Visually match the variable characteristics and shape of the data distribution to a distribution family.</li>
<li>If possible, think about the process that “generated” the data, and match that process to the data-generating process defining a family.</li>
</ol>
<p>Always keep in mind that there is almost never a “correct” choice! Remember, we are making an approximation here, not seeking a “true” distribution family.</p>
<p><strong>Example 1.</strong> In the hurricane damages example, the data variable is a positive continuous variable, and has a histogram that appears to decay starting from a damage of zero. The selected distribution should accomodate this – Weibull, Gamma, or GPD can all accomodate this. But the family should allow for heavy-tailed distributions to accomodate the two large data values that we see in the histogram, leaving a GPD as a good candidate. As for the way the data are “generated”, the data are inherently recorded because they are extreme, and this matches the way that a GPD is derived.</p>
<p><strong>Example 2.</strong> In the house prices example, the family should also accomodate a positive continuous random variable, should be unimodal with two tails, and be skewed to the right. A Weibull, Lognormal, or Gamma distribution so far seem like good candidates. Since the log of house prices looks like a Gaussian distribution, we choose a Lognormal distribution, since this is how the Lognormal family is defined.</p>
</div>
<div id="step-2-estimate" class="section level4">
<h4><span class="header-section-number">6.2.1.2</span> Step 2: Estimate</h4>
<p>So far, we’ve selected a <em>family</em> of distributions. Now, in this step, we need to select the distribution from this family that best matches the data. This step is the namesake of MLE.</p>
<p>The key is to <em>select the distribution for which the observed data are most likely to have been drawn from</em>. We can do this through a quantity called the <em>likelihood</em>, which can be calculated for any distribution that has a density/pmf, then finding the distribution that has the largest likelihood. Let’s break these two concepts down.</p>
<ol style="list-style-type: decimal">
<li><strong>Likelihood</strong>.</li>
</ol>
<p>The likelihood is a useful way to measure how well a distribution fits the data. To calculate the likelihood, denoted <span class="math inline">\(\mathcal{L}\)</span>, from data <span class="math inline">\(y_1, \ldots, y_n\)</span> and a distribution with density/pmf <span class="math inline">\(f\)</span>, calculate the product of the densities/pmf’s evaluated at the data: <span class="math display">\[\mathcal{L} = \prod_{i=1}^n f(y_i).\]</span></p>
<p>When <span class="math inline">\(f\)</span> is a pmf, you can interpret the likelihood as the probability of observing the data under the distribution <span class="math inline">\(f\)</span>. When <span class="math inline">\(f\)</span> is a density, the interpretation of likelihood is less tangible, and is the probability density of observing the data under the distribution <span class="math inline">\(f\)</span>. These interpretations are exactly true if the data are independent, but are still approximately true if data are “almost” independent. Even with non-independent data, the likelihood is still a useful measurement.</p>
<p>A similar quantity to the likelihood is the <em>negative log likelihood</em> (nllh), defined as <span class="math display">\[\ell = -\log\mathcal{L} = -\sum_{i=1}^n \log f(y_i).\]</span> The nllh is numerically more convenient than the likelihood. For example, the likelihood (<span class="math inline">\(\mathcal{L}\)</span>) tends to be an extremely small number, whereas the nllh typically is not. For instance, 100 draws from a N(0,1) distribution results in a likelihood that’s typically around <span class="math inline">\(3 \times 10^{-62},\)</span> whereas the nllh is typically around 141.5. Note that <em>minimizing</em> the nllh is the same as maximizing the likelihood.</p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Finding the distribution that has the largest likelihood</strong></li>
</ol>
<p>Remember that each distribution in the distribution family that we selected can be represented by its parameters – for example, the Normal distribution by its mean and variance, or the Beta distribution by <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. This means that we can view the likelihood as a function of the family’s parameters, and optimize this function! This can sometimes be done using calculus, but is most often done numerically.</p>
<p>We end up with estimates of the distribution’s parameters, which is the same thing as having an estimate of the data’s distribution. Don’t stop here if you are looking to estimate something other than the distibution’s parameters – move on to Step 3.</p>
<p><strong>Example 1</strong>. For the hurricane example, first find the GPD shape and scale parameters that maximize the likelihood (or, minimize the nllh).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nllh &lt;-<span class="st"> </span><span class="cf">function</span>(parameters) {
    scale &lt;-<span class="st"> </span>parameters[<span class="dv">1</span>]
    shape &lt;-<span class="st"> </span>parameters[<span class="dv">2</span>]
    <span class="cf">if</span> (scale <span class="op">&lt;=</span><span class="st"> </span><span class="dv">0</span>) <span class="kw">return</span>(<span class="ot">Inf</span>)
    <span class="op">-</span><span class="kw">sum</span>(evd<span class="op">::</span><span class="kw">dgpd</span>(damage<span class="op">$</span>Dam, <span class="dt">scale =</span> scale, <span class="dt">shape =</span> shape, <span class="dt">log =</span> <span class="ot">TRUE</span>))
}
optim_fit &lt;-<span class="st"> </span><span class="kw">optim</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">1</span>), nllh)
gpd_scale &lt;-<span class="st"> </span>optim_fit<span class="op">$</span>par[<span class="dv">1</span>]
gpd_shape &lt;-<span class="st"> </span>optim_fit<span class="op">$</span>par[<span class="dv">2</span>]</code></pre></div>
<p>Take a look at the GPD density corresponding to these parameters, and compared to the histogram of the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">hurricane_hist <span class="op">+</span>
<span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> evd<span class="op">::</span>dgpd, <span class="dt">scale =</span> gpd_scale, <span class="dt">shape =</span> gpd_shape, 
                  <span class="dt">colour =</span> <span class="st">&quot;blue&quot;</span>)</code></pre></div>
<pre><code>## Warning: Ignoring unknown parameters: scale, shape</code></pre>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="step-3" class="section level4">
<h4><span class="header-section-number">6.2.1.3</span> Step 3</h4>
</div>
<div id="step-4" class="section level4">
<h4><span class="header-section-number">6.2.1.4</span> Step 4</h4>
<p>either with a sample density/mass function, and/or a QQ-plot.</p>
</div>
</div>
<div id="motivating-example" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Motivating Example</h3>
<p>Data such as this can help an insurance company with their financial planning. Knowing an upper quantile, such as the 0.8-quantile, would give a sense of the damage caused by the “worst” hurricanes.</p>
<p>Take a look at the sampling distributions of three estimators of the 0.8-quantile: the sample version, the MLE under a Gaussian assumption, and the MLE under a generalized Pareto distribution (GPD) assumption. The sampling distributions are obtained using bootstrapping. The Gaussian-based MLE is an example of a <em>bad</em> MLE that uses a poorly chosen assumption – its sampling distribution is much wider than that of the sample version. On the other hand, the GPD-based MLE is based on a much more realistic assumption, and shows a significant improvement to the sample version – its sampling distribution is much narrower.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># dgpd &lt;- function(x, sig, xi) 1/sig*(1 + xi*(x/sig))^(-1/(xi+1)) </span>
<span class="co"># gpd_quantile &lt;- function(x, p = 0.5) {</span>
<span class="co">#     fit &lt;- ismev::gpd.fit(x, 0, show = FALSE)</span>
<span class="co">#     sigma &lt;- fit$mle[1]</span>
<span class="co">#     xi &lt;- fit$mle[2]</span>
<span class="co">#     sigma * (p^(-xi) - 1) / xi</span>
<span class="co"># }</span>
<span class="co"># wei_quantile &lt;- function(x, p = 0.5) {</span>
<span class="co">#     nllh &lt;- function(par) -sum(dweibull(x, par[1], par[2], log = TRUE))</span>
<span class="co">#     par_hat &lt;- optim(c(1,1), nllh)$par</span>
<span class="co">#     qweibull(p, par_hat[1], par_hat[2])</span>
<span class="co"># }</span>
<span class="co"># sampling_dist &lt;- damage %&gt;% </span>
<span class="co">#     bootstraps(times = 1000) %&gt;%</span>
<span class="co">#     pull(splits) %&gt;%</span>
<span class="co">#     map(as_tibble) %&gt;% </span>
<span class="co">#     map_df(~ summarise(</span>
<span class="co">#         .x, </span>
<span class="co">#         bar = quantile(Dam, probs = 0.8), </span>
<span class="co">#         mle_gpd = gpd_quantile(Dam, 0.8),</span>
<span class="co">#         mle_gau = qnorm(0.8, mean = mean(Dam), sd = sd(Dam)))</span>
<span class="co">#     )</span>
<span class="co"># sampling_dist %&gt;% </span>
<span class="co">#     gather(key = &quot;method&quot;, value = &quot;estimate&quot;) %&gt;% </span>
<span class="co">#     ggplot(aes(estimate)) +</span>
<span class="co">#     facet_wrap(~method, nrow = 1, scales = &quot;free_x&quot;) +</span>
<span class="co">#     geom_histogram(bins = 30) +</span>
<span class="co">#     theme_bw() </span></code></pre></div>
<p><strong>Caution</strong>: A common misconception is that the two large observations in the dataset are outliers, and therefore should be removed. However, doing so would bias our understanding of hurricance damages, since these “outliers” are real occurences.</p>
<p>In other cases, an MLE does not result in much of an improvement at all.</p>
<p>Although there are far fewer cases in the univariate case compared to the regression setting where MLE gives a dramatic improvement to estimation, it’s still worth discussing when it’s most useful in the univariate setting and to ground concepts.</p>
<p>High quantile example for a PI:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># N &lt;- 10000</span>
<span class="co"># rate &lt;- 1</span>
<span class="co"># ordered &lt;- numeric(0)</span>
<span class="co"># mle &lt;- numeric(0)</span>
<span class="co"># for (i in 1:N) {</span>
<span class="co">#     x &lt;- rexp(10, rate = rate)</span>
<span class="co">#     ordered[i] &lt;- quantile(x, probs = 0.975, type = 1)</span>
<span class="co">#     mle[i] &lt;- qexp(0.975, rate = 1/mean(x))</span>
<span class="co"># }</span>
<span class="co"># tibble(ordered, mle) %&gt;% </span>
<span class="co">#     gather(value = &quot;estimate&quot;) %&gt;%</span>
<span class="co">#     ggplot(aes(estimate)) +</span>
<span class="co">#     geom_density(aes(group = key, fill = key), alpha = 0.5) +</span>
<span class="co">#     geom_vline(xintercept = qexp(0.975, rate = rate),</span>
<span class="co">#                linetype = &quot;dashed&quot;) +</span>
<span class="co">#     theme_bw()</span>
<span class="co"># sd(ordered)</span>
<span class="co"># sd(mle)</span></code></pre></div>
<p>In both cases, the sampling distribution of the MLE is better than that of the sample version – that is, more narrow and (sometimes) centered closer to the true value.</p>
<p>Is there a better estimator than the MLE? It turns out that the MLE is realistically the best that we can do – as long as the distributional assumption is not too bad of an approximation. If you’re curious to learn more, the end of this chapter fleshes this out using precise terminology.</p>
<p>If the improvement by using MLE does not seem very impressive to you, you’d be right – at least in the univariate world. To see much difference between the MLE and sample version estimators, you’d need to be estimating low-probability events with a small amount of data. In fact, estimating the mean using MLE most often results in the same estimator as the sample mean! Don’t write off the MLE just yet – it really shines in the regression setting, where it has even more benefits than just improved estimation. Tune in to Part II to learn more.</p>
<p>As an example of (1), suppose you are measuring the ratio of torso height to body height. Since your sample falls between 0 and 1, and probably does not have a weirdly shaped density, a Beta distribution would be a good assumption, since the Beta family spans non-weird densities over (0, 1). However, not knowing the data-generating process, you would not be able to justify the distribution completely (and that’s OK). As an example of (2), perhaps you are operating the port of Vancouver, BC, and based on your experience, know that vessels arrive more-or-less independently at some average rate. This is how a Poisson distribution is defined. Not only that, but the data appear to be shaped like a Poisson distribution. Then it would be justifiable to assume the data follow a Poisson distribution.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># n &lt;- 50</span>
<span class="co"># N &lt;- 1000</span>
<span class="co"># fit_mle &lt;- numeric(0)</span>
<span class="co"># fit_ls &lt;- numeric(0)</span>
<span class="co"># for (i in 1:N) {</span>
<span class="co">#     x &lt;- rnorm(n)</span>
<span class="co">#     mu &lt;- 1/(1+exp(-x))</span>
<span class="co">#     y &lt;- rbinom(n, size = 1, prob = mu)</span>
<span class="co">#     fit_mle[i] &lt;- glm(y ~ x, family = &quot;binomial&quot;)$coefficients[2]</span>
<span class="co">#     ls &lt;- function(par) sum((y - 1/(1+exp(-par[1]-par[2]*x)))^2)</span>
<span class="co">#     fit_ls[i] &lt;- optim(c(0,1), ls)$par[2]</span>
<span class="co"># }</span>
<span class="co"># tibble(fit_mle, fit_ls) %&gt;% </span>
<span class="co">#     gather(value = &quot;beta&quot;) %&gt;% </span>
<span class="co">#     ggplot(aes(beta)) +</span>
<span class="co">#     # scale_x_log10() +</span>
<span class="co">#     geom_density(aes(group = key, fill = key), alpha = 0.5)</span>
<span class="co"># sd(fit_mle)</span>
<span class="co"># sd(fit_ls)</span>
<span class="co"># IQR(fit_mle)</span>
<span class="co"># IQR(fit_ls)</span>
<span class="co"># ## More extremes show up with LS (at least with n=50):</span>
<span class="co"># sort(fit_ls) %&gt;% tail(10)</span>
<span class="co"># sort(fit_mle) %&gt;% tail(10)</span>
<span class="co"># ## Gaussian assumption</span>
<span class="co"># ##  - LS not even that good at n=100 -- bowed down. MLE is good.</span>
<span class="co"># ##  - MLE qqplot with n=50 looks about the same as LS with n=100</span>
<span class="co"># ##  - LS at n=50 is heavy tailed (seemingly).</span>
<span class="co"># qqnorm(fit_mle)</span>
<span class="co"># qqnorm(fit_ls)</span></code></pre></div>
<p>For n=50, check out an example that results in an extreme beta:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># n &lt;- 50</span>
<span class="co"># beta &lt;- 0</span>
<span class="co"># while (beta &lt; 300) {</span>
<span class="co">#     x &lt;- rnorm(n)</span>
<span class="co">#     mu &lt;- 1/(1+exp(-x))</span>
<span class="co">#     y &lt;- rbinom(n, size = 1, prob = mu)</span>
<span class="co">#     ls &lt;- function(par) sum((y - 1/(1+exp(-par[1]-par[2]*x)))^2)</span>
<span class="co">#     .optim &lt;- optim(c(0,1), ls)</span>
<span class="co">#     beta &lt;- .optim$par[2]</span>
<span class="co">#     alpha &lt;- .optim$par[1]</span>
<span class="co"># }</span>
<span class="co"># if (.optim$convergence == 0) stop(&quot;optim didn&#39;t successfully converge.&quot;)</span>
<span class="co"># mle &lt;- glm(y ~ x, family = &quot;binomial&quot;)$coefficients</span>
<span class="co"># qplot(x, y) + </span>
<span class="co">#     stat_function(fun = function(x) 1/(1+exp(-alpha-beta*x)), mapping = aes(colour = &quot;LS&quot;)) +</span>
<span class="co">#     stat_function(fun = function(x) 1/(1+exp(-mle[1]-mle[2]*x)), mapping = aes(colour = &quot;MLE&quot;))</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># # MLE is still slightly narrower, even for a Beta(2,2) distribution (which is</span>
<span class="co"># # symmetric and bell-like) -- for n=5 and n=50. Both close to Gaussian, even at</span>
<span class="co"># # n=5 (as expected).</span>
<span class="co"># shape1 &lt;- 2</span>
<span class="co"># shape2 &lt;- 2</span>
<span class="co"># foo &lt;- function(x) dbeta(x, shape1, shape2)</span>
<span class="co"># curve(foo, 0, 1)</span>
<span class="co"># n &lt;- 5</span>
<span class="co"># N &lt;- 1000</span>
<span class="co"># xbar &lt;- numeric(0)</span>
<span class="co"># mle &lt;- numeric(0)</span>
<span class="co"># for (i in 1:N) {</span>
<span class="co">#     x &lt;- rbeta(n, shape1, shape2)</span>
<span class="co">#     xbar[i] &lt;- mean(x)</span>
<span class="co">#     nllh &lt;- function(par) {</span>
<span class="co">#         if (min(par) &lt;= 0) return(Inf)</span>
<span class="co">#         -sum(dbeta(x, par[1], par[2], log = TRUE))</span>
<span class="co">#     }</span>
<span class="co">#     .optim &lt;- optim(c(shape1, shape2), nllh)</span>
<span class="co">#     par_hat &lt;- .optim$par</span>
<span class="co">#     mle[i] &lt;- par_hat[1] / sum(par_hat)</span>
<span class="co"># }</span>
<span class="co"># plot(mle - xbar) # The estimates aren&#39;t the same.</span>
<span class="co"># tibble(mle, xbar) %&gt;% </span>
<span class="co">#     gather() %&gt;% </span>
<span class="co">#     ggplot(aes(value)) +</span>
<span class="co">#     geom_density(aes(group = key, fill = key), alpha = 0.5)</span>
<span class="co"># sd(mle)</span>
<span class="co"># sd(xbar)</span>
<span class="co"># qqnorm(mle)</span>
<span class="co"># qqnorm(xbar)</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># # Univariate MLE *especially* important for heavy tailed distributions!</span>
<span class="co"># nu &lt;- 1.5</span>
<span class="co"># n &lt;- 5</span>
<span class="co"># N &lt;- 1000</span>
<span class="co"># xbar &lt;- numeric(0)</span>
<span class="co"># mle &lt;- numeric(0)</span>
<span class="co"># for (i in 1:N) {</span>
<span class="co">#     x &lt;- rt(n, df = nu)</span>
<span class="co">#     xbar[i] &lt;- mean(x)</span>
<span class="co">#     nllh &lt;- function(par) -sum(dt(x, df = par[1], ncp = par[2], log = TRUE))</span>
<span class="co">#     .optim &lt;- optim(c(nu, 0), nllh)</span>
<span class="co">#     mle[i] &lt;- .optim$par[2]</span>
<span class="co"># }</span>
<span class="co"># plot(mle - xbar) # The estimates aren&#39;t the same.</span>
<span class="co"># tibble(mle, xbar) %&gt;% </span>
<span class="co">#     gather() %&gt;% </span>
<span class="co">#     ggplot(aes(value)) +</span>
<span class="co">#     geom_density(aes(group = key, fill = key), alpha = 0.5)</span>
<span class="co"># sd(mle)</span>
<span class="co"># sd(xbar)</span>
<span class="co"># qqnorm(mle)</span>
<span class="co"># qqnorm(xbar)</span></code></pre></div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="estimation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="prediction-harnessing-the-signal.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/vincenzocoia/Interpreting-Regression/edit/master/040-Parametric_families.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
