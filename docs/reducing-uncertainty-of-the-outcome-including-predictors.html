<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 8 Reducing uncertainty of the outcome: including predictors | Interpreting Regression</title>
  <meta name="description" content="My tutorials for regression analysis, in the form of a bookdown book.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 8 Reducing uncertainty of the outcome: including predictors | Interpreting Regression" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="My tutorials for regression analysis, in the form of a bookdown book." />
  <meta name="github-repo" content="vincenzocoia/Interpreting-Regression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Reducing uncertainty of the outcome: including predictors | Interpreting Regression" />
  
  <meta name="twitter:description" content="My tutorials for regression analysis, in the form of a bookdown book." />
  

<meta name="author" content="Vincenzo Coia">


<meta name="date" content="2019-05-29">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="prediction-harnessing-the-signal.html">
<link rel="next" href="the-signal-model-functions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpreting Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#caution"><i class="fa fa-check"></i><b>1.1</b> Caution</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#purpose-of-the-book"><i class="fa fa-check"></i><b>1.2</b> Purpose of the book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#tasks-that-motivate-regression"><i class="fa fa-check"></i><b>1.3</b> Tasks that motivate Regression</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html"><i class="fa fa-check"></i><b>2</b> Regression in the context of problem solving</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#communicating-distillation-3-4"><i class="fa fa-check"></i><b>2.1</b> Communicating (Distillation 3-4)</a></li>
<li class="chapter" data-level="2.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#modelling-distillation-2-3"><i class="fa fa-check"></i><b>2.2</b> Modelling (Distillation 2-3)</a></li>
<li class="chapter" data-level="2.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#asking-useful-statistical-questions-distillation-1-2"><i class="fa fa-check"></i><b>2.3</b> Asking useful statistical questions (Distillation 1-2)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#business-objectives-examples"><i class="fa fa-check"></i><b>2.3.1</b> Business objectives: examples</a></li>
<li class="chapter" data-level="2.3.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-refining"><i class="fa fa-check"></i><b>2.3.2</b> Statistical objectives: refining</a></li>
<li class="chapter" data-level="2.3.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-examples"><i class="fa fa-check"></i><b>2.3.3</b> Statistical objectives: examples</a></li>
<li class="chapter" data-level="2.3.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-questions-are-not-the-full-picture"><i class="fa fa-check"></i><b>2.3.4</b> Statistical questions are not the full picture</a></li>
<li class="chapter" data-level="2.3.5" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-unrelated-to-supervised-learning"><i class="fa fa-check"></i><b>2.3.5</b> Statistical objectives unrelated to supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#prerequisites-to-an-analysis"><i class="fa fa-check"></i><b>2.4</b> Prerequisites to an analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="an-outcome-on-its-own.html"><a href="an-outcome-on-its-own.html"><i class="fa fa-check"></i>An outcome on its own</a></li>
<li class="chapter" data-level="3" data-path="distributions-uncertainty-is-worth-explaining.html"><a href="distributions-uncertainty-is-worth-explaining.html"><i class="fa fa-check"></i><b>3</b> Distributions: Uncertainty is worth explaining</a></li>
<li class="chapter" data-level="4" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html"><i class="fa fa-check"></i><b>4</b> Explaining an uncertain outcome: interpretable quantities</a><ul>
<li class="chapter" data-level="4.1" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#probabilistic-quantities"><i class="fa fa-check"></i><b>4.1</b> Probabilistic Quantities</a></li>
<li class="chapter" data-level="4.2" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#what-is-the-mean-anyway"><i class="fa fa-check"></i><b>4.2</b> What is the mean, anyway?</a></li>
<li class="chapter" data-level="4.3" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#quantiles"><i class="fa fa-check"></i><b>4.3</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-versions-of-interpretable-quantities.html"><a href="data-versions-of-interpretable-quantities.html"><i class="fa fa-check"></i><b>5</b> Data versions of interpretable quantities</a><ul>
<li class="chapter" data-level="5.1" data-path="data-versions-of-interpretable-quantities.html"><a href="data-versions-of-interpretable-quantities.html#estimation-of-probabilistic-quantities"><i class="fa fa-check"></i><b>5.1</b> Estimation of Probabilistic Quantities</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sampling-distributions-another-layer-of-uncertainty-added-from-estimation.html"><a href="sampling-distributions-another-layer-of-uncertainty-added-from-estimation.html"><i class="fa fa-check"></i><b>6</b> Sampling distributions: Another layer of uncertainty added from estimation</a></li>
<li class="chapter" data-level="7" data-path="improving-estimator-quality-by-parametric-distributional-assumptions-and-mle.html"><a href="improving-estimator-quality-by-parametric-distributional-assumptions-and-mle.html"><i class="fa fa-check"></i><b>7</b> Improving estimator quality by parametric distributional assumptions and MLE</a></li>
<li class="chapter" data-level="" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html"><i class="fa fa-check"></i>Prediction: harnessing the signal</a></li>
<li class="chapter" data-level="8" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html"><i class="fa fa-check"></i><b>8</b> Reducing uncertainty of the outcome: including predictors</a><ul>
<li class="chapter" data-level="8.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-terminology"><i class="fa fa-check"></i><b>8.1</b> Variable terminology</a><ul>
<li class="chapter" data-level="8.1.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-types"><i class="fa fa-check"></i><b>8.1.1</b> Variable types</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#irreducible-error"><i class="fa fa-check"></i><b>8.2</b> Irreducible Error</a></li>
<li class="chapter" data-level="8.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#in-class-exercises-irreducible-error"><i class="fa fa-check"></i><b>8.3</b> In-class Exercises: Irreducible Error</a><ul>
<li class="chapter" data-level="8.3.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-regression"><i class="fa fa-check"></i><b>8.3.1</b> Oracle regression</a></li>
<li class="chapter" data-level="8.3.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-classification"><i class="fa fa-check"></i><b>8.3.2</b> Oracle classification</a></li>
<li class="chapter" data-level="8.3.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-random-prediction"><i class="fa fa-check"></i><b>8.3.3</b> (BONUS) Random prediction</a></li>
<li class="chapter" data-level="8.3.4" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-a-more-non-standard-regression"><i class="fa fa-check"></i><b>8.3.4</b> (BONUS) A more non-standard regression</a></li>
<li class="chapter" data-level="8.3.5" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-oracle-mse"><i class="fa fa-check"></i><b>8.3.5</b> (BONUS) Oracle MSE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html"><i class="fa fa-check"></i><b>9</b> The signal: model functions</a><ul>
<li class="chapter" data-level="9.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#linear-quantile-regression"><i class="fa fa-check"></i><b>9.1</b> Linear Quantile Regression</a><ul>
<li class="chapter" data-level="9.1.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#exercise"><i class="fa fa-check"></i><b>9.1.1</b> Exercise</a></li>
<li class="chapter" data-level="9.1.2" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-crossing-quantiles"><i class="fa fa-check"></i><b>9.1.2</b> Problem: Crossing quantiles</a></li>
<li class="chapter" data-level="9.1.3" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-upper-quantiles"><i class="fa fa-check"></i><b>9.1.3</b> Problem: Upper quantiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html"><i class="fa fa-check"></i><b>10</b> Estimating parametric model functions</a><ul>
<li class="chapter" data-level="10.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#writing-the-sample-mean-as-an-optimization-problem"><i class="fa fa-check"></i><b>10.1</b> Writing the sample mean as an optimization problem</a></li>
<li class="chapter" data-level="10.2" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#evaluating-model-goodness-quantiles"><i class="fa fa-check"></i><b>10.2</b> Evaluating Model Goodness: Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><i class="fa fa-check"></i><b>11</b> Estimating assumption-free: the world of supervised learning techniques</a><ul>
<li class="chapter" data-level="11.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#what-machine-learning-is"><i class="fa fa-check"></i><b>11.1</b> What machine learning is</a></li>
<li class="chapter" data-level="11.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#types-of-supervised-learning"><i class="fa fa-check"></i><b>11.2</b> Types of Supervised Learning</a></li>
<li class="chapter" data-level="11.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#local-regression"><i class="fa fa-check"></i><b>11.3</b> Local Regression</a><ul>
<li class="chapter" data-level="11.3.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#knn"><i class="fa fa-check"></i><b>11.3.1</b> kNN</a></li>
<li class="chapter" data-level="11.3.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess"><i class="fa fa-check"></i><b>11.3.2</b> loess</a></li>
<li class="chapter" data-level="11.3.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#in-class-exercises"><i class="fa fa-check"></i><b>11.3.3</b> In-Class Exercises</a></li>
<li class="chapter" data-level="11.3.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#hyperparameters-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>11.3.4</b> Hyperparameters and the bias/variance tradeoff</a></li>
<li class="chapter" data-level="11.3.5" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#extensions-to-knn-and-loess"><i class="fa fa-check"></i><b>11.3.5</b> Extensions to kNN and loess</a></li>
<li class="chapter" data-level="11.3.6" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#model-assumptions-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>11.3.6</b> Model assumptions and the bias/variance tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#splines-and-loess-regression"><i class="fa fa-check"></i><b>11.4</b> Splines and Loess Regression</a><ul>
<li class="chapter" data-level="11.4.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess-1"><i class="fa fa-check"></i><b>11.4.1</b> Loess</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html"><i class="fa fa-check"></i><b>12</b> Overfitting: The problem with adding too many parameters</a><ul>
<li class="chapter" data-level="12.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#classification-exercise-do-together"><i class="fa fa-check"></i><b>12.1</b> Classification Exercise: Do Together</a></li>
<li class="chapter" data-level="12.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#training-error-vs.generalization-error"><i class="fa fa-check"></i><b>12.2</b> Training Error vs. Generalization Error</a></li>
<li class="chapter" data-level="12.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-complexity"><i class="fa fa-check"></i><b>12.3</b> Model complexity</a><ul>
<li class="chapter" data-level="12.3.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#activity"><i class="fa fa-check"></i><b>12.3.1</b> Activity</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducible-error"><i class="fa fa-check"></i><b>12.4</b> Reducible Error</a><ul>
<li class="chapter" data-level="12.4.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#what-is-it"><i class="fa fa-check"></i><b>12.4.1</b> What is it?</a></li>
<li class="chapter" data-level="12.4.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#example"><i class="fa fa-check"></i><b>12.4.2</b> Example</a></li>
<li class="chapter" data-level="12.4.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#bias-and-variance"><i class="fa fa-check"></i><b>12.4.3</b> Bias and Variance</a></li>
<li class="chapter" data-level="12.4.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducing-reducible-error"><i class="fa fa-check"></i><b>12.4.4</b> Reducing reducible error</a></li>
<li class="chapter" data-level="12.4.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#error-decomposition"><i class="fa fa-check"></i><b>12.4.5</b> Error decomposition</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection"><i class="fa fa-check"></i><b>12.5</b> Model Selection</a><ul>
<li class="chapter" data-level="12.5.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#exercise-cv"><i class="fa fa-check"></i><b>12.5.1</b> Exercise: CV</a></li>
<li class="chapter" data-level="12.5.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#out-of-sample-error"><i class="fa fa-check"></i><b>12.5.2</b> Out-of-sample Error</a></li>
<li class="chapter" data-level="12.5.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#alternative-measures-of-model-goodness"><i class="fa fa-check"></i><b>12.5.3</b> Alternative measures of model goodness</a></li>
<li class="chapter" data-level="12.5.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-and-model-selection-setup"><i class="fa fa-check"></i><b>12.5.4</b> Feature and model selection: setup</a></li>
<li class="chapter" data-level="12.5.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection-1"><i class="fa fa-check"></i><b>12.5.5</b> Model selection</a></li>
<li class="chapter" data-level="12.5.6" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-predictor-selection"><i class="fa fa-check"></i><b>12.5.6</b> Feature (predictor) selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="describing-relationships.html"><a href="describing-relationships.html"><i class="fa fa-check"></i>Describing Relationships</a></li>
<li class="chapter" data-level="13" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html"><i class="fa fa-check"></i><b>13</b> There’s meaning in parameters</a></li>
<li class="chapter" data-level="14" data-path="the-meaning-of-interaction.html"><a href="the-meaning-of-interaction.html"><i class="fa fa-check"></i><b>14</b> The meaning of interaction</a></li>
<li class="chapter" data-level="15" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html"><i class="fa fa-check"></i><b>15</b> Scales and the restricted range problem</a></li>
<li class="chapter" data-level="16" data-path="improving-estimation-through-distributional-assumptions.html"><a href="improving-estimation-through-distributional-assumptions.html"><i class="fa fa-check"></i><b>16</b> Improving estimation through distributional assumptions</a></li>
<li class="chapter" data-level="17" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html"><i class="fa fa-check"></i><b>17</b> When we only want interpretation on some predictors</a></li>
<li class="chapter" data-level="" data-path="special-cases.html"><a href="special-cases.html"><i class="fa fa-check"></i>Special cases</a></li>
<li class="chapter" data-level="18" data-path="regression-when-data-are-censored-survival-analysis.html"><a href="regression-when-data-are-censored-survival-analysis.html"><i class="fa fa-check"></i><b>18</b> Regression when data are censored: survival analysis</a></li>
<li class="chapter" data-level="19" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html"><i class="fa fa-check"></i><b>19</b> Regression in the presence of outliers: robust regression</a></li>
<li class="chapter" data-level="20" data-path="regression-in-the-presence-of-extremes-extreme-value-regression.html"><a href="regression-in-the-presence-of-extremes-extreme-value-regression.html"><i class="fa fa-check"></i><b>20</b> Regression in the presence of extremes: extreme value regression</a></li>
<li class="chapter" data-level="21" data-path="regression-when-data-are-ordinal.html"><a href="regression-when-data-are-ordinal.html"><i class="fa fa-check"></i><b>21</b> Regression when data are ordinal</a></li>
<li class="chapter" data-level="22" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html"><i class="fa fa-check"></i><b>22</b> Regression when data are missing: multiple imputation</a></li>
<li class="chapter" data-level="23" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html"><i class="fa fa-check"></i><b>23</b> Regression under many groups: mixed effects models</a></li>
<li class="chapter" data-level="24" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html"><i class="fa fa-check"></i><b>24</b> Regression on an entire distribution: Probabilistic Forecasting</a><ul>
<li class="chapter" data-level="24.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasting-what-it-is"><i class="fa fa-check"></i><b>24.1</b> Probabilistic Forecasting: What it is</a></li>
<li class="chapter" data-level="24.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#review-univariate-distribution-estimates"><i class="fa fa-check"></i><b>24.2</b> Review: Univariate distribution estimates</a><ul>
<li class="chapter" data-level="24.2.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#continuous-response"><i class="fa fa-check"></i><b>24.2.1</b> Continuous response</a></li>
<li class="chapter" data-level="24.2.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discrete-response"><i class="fa fa-check"></i><b>24.2.2</b> Discrete Response</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasts-subset-based-learning-methods"><i class="fa fa-check"></i><b>24.3</b> Probabilistic Forecasts: subset-based learning methods</a><ul>
<li class="chapter" data-level="24.3.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#the-techniques"><i class="fa fa-check"></i><b>24.3.1</b> The techniques</a></li>
<li class="chapter" data-level="24.3.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#exercise-1"><i class="fa fa-check"></i><b>24.3.2</b> Exercise</a></li>
<li class="chapter" data-level="24.3.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>24.3.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="24.3.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#evaluating-model-goodness"><i class="fa fa-check"></i><b>24.3.4</b> Evaluating Model Goodness</a></li>
</ul></li>
<li class="chapter" data-level="24.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discussion-points"><i class="fa fa-check"></i><b>24.4</b> Discussion Points</a></li>
<li class="chapter" data-level="24.5" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#when-are-they-not-useful"><i class="fa fa-check"></i><b>24.5</b> When are they not useful?</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/vincenzocoia/Interpreting-Regression" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpreting Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="reducing-uncertainty-of-the-outcome-including-predictors" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Reducing uncertainty of the outcome: including predictors</h1>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<div id="variable-terminology" class="section level2">
<h2><span class="header-section-number">8.1</span> Variable terminology</h2>
<p>In supervised learning:</p>
<ul>
<li>The output is a random variable, typically denoted <span class="math inline">\(Y\)</span>.</li>
<li>The input(s) variables (which may or may not be random), if there are <span class="math inline">\(p\)</span> of them, are typically denoted <span class="math inline">\(X_1\)</span>, …, <span class="math inline">\(X_p\)</span> – or just <span class="math inline">\(X\)</span> if there’s one.</li>
</ul>
<p>There are many names for the input and output variables. Here are some (there are more, undoubtedly):</p>
<ul>
<li><strong>Output</strong>: response, dependent variable.</li>
<li><strong>Input</strong>: predictors, covariates, features, independent variables, explanatory variables, regressors.</li>
</ul>
<p>In BAIT 509, we will use the terminology <em>predictors</em> and <em>response</em>.</p>
<div id="variable-types" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Variable types</h3>
<p>Terminology surrounding variable types can be confusing, so it’s worth going over it. Here are some non-technical definitions.</p>
<ul>
<li>A <strong>numeric</strong> variable is one that has a quantity associated with it, such as age or height. Of these, a numeric variable can be one of two things:</li>
<li>A <strong>categorical</strong> variable, as the name suggests, is a variable that can be one of many categories. For example, type of fruit; success or failure.</li>
</ul>
</div>
</div>
<div id="irreducible-error" class="section level2">
<h2><span class="header-section-number">8.2</span> Irreducible Error</h2>
<p>The concept of <strong>irreducible error</strong> is paramount to supervised learning. Next time, we’ll look at the concept of <em>reducible</em> error.</p>
<p>When building a supervised learning model (like linear regression), we can never build a perfect forecaster – even if we have infinite data!</p>
<p>Let’s explore this notion. When we hypothetically have an infinite amount of data to train a model with, what we actually have is the <em>probability distribution</em> of <span class="math inline">\(Y\)</span> given any value of the predictors. The uncertainty in this probability distribution is the <strong>irreducible error</strong>.</p>
<p><strong>Example</strong>: Let’s say <span class="math inline">\((X,Y)\)</span> follows a (known) bivariate Normal distribution. Then, for any input of <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span> has a <em>distribution</em>. Here are some examples of this distribution for a few values of the predictor variable (these are called <em>conditional</em> distributions, because they’re conditional on observing particular values of the predictors).</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-2-1.png" width="576" /></p>
<p>This means we cannot know what <span class="math inline">\(Y\)</span> will be, no matter what! What’s one to do?</p>
<ul>
<li>In <strong>regression</strong> (i.e., when <span class="math inline">\(Y\)</span> is numeric, as above), the go-to standard is to predict the <em>mean</em> as our best guess.
<ul>
<li>We typically measure error with the <strong>mean squared error</strong> = average of (observed-predicted)^2.</li>
</ul></li>
<li>In <strong>classification</strong>, the conditional distributions are categorical variables, so the go-to standard is to predict the <em>mode</em> as our best guess (i.e., the category having the highest probability).
<ul>
<li>A typical measurement of error is the <strong>error rate</strong> = proportion of incorrect predictions.</li>
<li>A more “complete” picture of error is the <strong>entropy</strong>, or equivalently, the <strong>information measure</strong>.</li>
</ul></li>
</ul>
<p>In Class Meeting 07, we’ll look at different options besides the mean and the mode.</p>
<p>An important concept is that <em>predictors give us more information about the response</em>, leading to a more certain distribution. In the above example, let’s try to make a prediction when we don’t have knowledge of predictors. Here’s what the distribution of the response looks like:</p>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-3-1.png" width="576" /></p>
<p>This is much more uncertain than in the case where we have predictors!</p>
</div>
<div id="in-class-exercises-irreducible-error" class="section level2">
<h2><span class="header-section-number">8.3</span> In-class Exercises: Irreducible Error</h2>
<p><strong>NOT REQUIRED FOR PARTICIPATION</strong></p>
<div id="oracle-regression" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Oracle regression</h3>
<p>Suppose you have two independent predictors, <span class="math inline">\(X_1, X_2 \sim N(0,1)\)</span>, and the conditional distribution of <span class="math inline">\(Y\)</span> is <span class="math display">\[ Y \mid (X_1=x_1, X_2=x_2) \sim N(5-x_1+2x_2, 1). \]</span> From this, it follows that:</p>
<ul>
<li>The conditional distribution of <span class="math inline">\(Y\)</span> given <em>only</em> <span class="math inline">\(X_1\)</span> is <span class="math display">\[ Y \mid X_1=x_1 \sim N(5-x_1, 5). \]</span></li>
<li>The conditional distribution of <span class="math inline">\(Y\)</span> given <em>only</em> <span class="math inline">\(X_2\)</span> is <span class="math display">\[ Y \mid X_2=x_2 \sim N(5+2x_2, 2). \]</span></li>
<li>The (marginal) distribution of <span class="math inline">\(Y\)</span> (not given any of the predictors) is <span class="math display">\[ Y \sim N(5, 6). \]</span></li>
</ul>
<p>The following R function generates data from the joint distribution of <span class="math inline">\((X_1, X_2, Y)\)</span>. It takes a single positive integer as an input, representing the sample size, and returns a <code>tibble</code> (a fancy version of a data frame) with columns named <code>x1</code>, <code>x2</code>, and <code>y</code>, corresponding to the random vector <span class="math inline">\((X_1, X_2, Y)\)</span>, with realizations given in the rows.</p>
<pre><code>genreg &lt;- function(n){
    x1 &lt;- rnorm(n)
    x2 &lt;- rnorm(n)
    eps &lt;- rnorm(n)
    y &lt;- 5-x1+2*x2+eps
    tibble(x1=x1, x2=x2, y=y)
}</code></pre>
<ol style="list-style-type: decimal">
<li>Generate data – as much as you’d like.</li>
</ol>
<pre><code>dat &lt;- genreg(1000)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>For now, ignore the <span class="math inline">\(Y\)</span> values. Use the means from the distributions listed above to predict <span class="math inline">\(Y\)</span> under four circumstances:
<ol style="list-style-type: decimal">
<li>Using both the values of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>.</li>
<li>Using only the values of <span class="math inline">\(X_1\)</span>.</li>
<li>Using only the values of <span class="math inline">\(X_2\)</span>.</li>
<li>Using neither the values of <span class="math inline">\(X_1\)</span> nor <span class="math inline">\(X_2\)</span>. (Your predictions in this case will be the same every time – what is that number?)</li>
</ol></li>
</ol>
<pre><code>dat &lt;- mutate(dat,
       yhat = FILL_THIS_IN,
       yhat1 = FILL_THIS_IN,
       yhat2 = FILL_THIS_IN,
       yhat12 = FILL_THIS_IN)</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Now use the actual outcomes of <span class="math inline">\(Y\)</span> to calculate the mean squared error (MSE) for each of the four situations.
<ul>
<li>Try re-running the simulation with a new batch of data. Do your MSE’s change much? If so, choose a larger sample so that these numbers are more stable.</li>
</ul></li>
</ol>
<pre><code>(mse &lt;- mean((dat$FILL_THIS_IN - dat$y)^2))
(mse1 &lt;- mean((dat$FILL_THIS_IN - dat$y)^2))
(mse2 &lt;- mean((dat$FILL_THIS_IN - dat$y)^2))
(mse12 &lt;- mean((dat$FILL_THIS_IN - dat$y)^2))
knitr::kable(tribble(
    ~ Case, ~ MSE,
    &quot;No predictors&quot;, mse,
    &quot;Only X1&quot;, mse1,
    &quot;Only X2&quot;, mse2,
    &quot;Both X1 and X2&quot;, mse12
))</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Order the situations from “best forecaster” to “worst forecaster”. Why do we see this order?</li>
</ol>
</div>
<div id="oracle-classification" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Oracle classification</h3>
<p>Consider a categorical response that can take on one of three categories: <em>A</em>, <em>B</em>, or <em>C</em>. The conditional probabilities are: <span class="math display">\[ P(Y=A \mid X=x) = 0.2, \]</span> <span class="math display">\[ P(Y=B \mid X=x) = 0.8/(1+e^{-x}), \]</span></p>
<p>To help you visualize this, here is a plot of <span class="math inline">\(P(Y=B \mid X=x)\)</span> vs <span class="math inline">\(x\)</span> (notice that it is bounded above by 0.8, and below by 0).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">tibble</span>(<span class="dt">x=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">7</span>, <span class="dv">7</span>)), <span class="kw">aes</span>(x)) <span class="op">+</span>
<span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun=</span><span class="cf">function</span>(x) <span class="fl">0.8</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span><span class="kw">exp</span>(<span class="op">-</span>x))) <span class="op">+</span>
<span class="st">    </span><span class="kw">ylim</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_hline</span>(<span class="dt">yintercept=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.8</span>), <span class="dt">linetype=</span><span class="st">&quot;dashed&quot;</span>, <span class="dt">alpha=</span><span class="fl">0.5</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">y=</span><span class="st">&quot;P(Y=B|X=x)&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Here’s an R function to generate data for you, where <span class="math inline">\(X\sim N(0,1)\)</span>. As before, it accepts a positive integer as its input, representing the sample size, and returns a tibble with column names <code>x</code> and <code>y</code> corresponding to the predictor and response.</p>
<pre><code>gencla &lt;- function(n) {
    x &lt;- rnorm(n) 
    pB &lt;- 0.8/(1+exp(-x))
    y &lt;- map_chr(pB, function(t) 
            sample(LETTERS[1:3], size=1, replace=TRUE,
                   prob=c(0.2, t, 1-t-0.2)))
    tibble(x=x, y=y)
}</code></pre>
<ol style="list-style-type: decimal">
<li>Calculate the probabilities of each category when <span class="math inline">\(X=1\)</span>. What about when <span class="math inline">\(X=-2\)</span>? With this information, what would you classify <span class="math inline">\(Y\)</span> as in both cases?
<ul>
<li>BONUS: Plot these two conditional distributions.</li>
</ul></li>
</ol>
<pre><code>## X=1:
(pB &lt;- FILL_THIS_IN)
(pA &lt;- FILL_THIS_IN)
(pC &lt;- FILL_THIS_IN)
ggplot(tibble(p=c(pA,pB,pC), y=LETTERS[1:3]), aes(y, p)) +
    geom_col() +
    theme_bw() +
    labs(y=&quot;Probabilities&quot;, title=&quot;X=1&quot;)
## X=-2
(pB &lt;- FILL_THIS_IN)
(pA &lt;- FILL_THIS_IN)
(pC &lt;- FILL_THIS_IN)
ggplot(tibble(p=c(pA,pB,pC), y=LETTERS[1:3]), aes(y, p)) +
    geom_col() +
    theme_bw() +
    labs(&quot;Probabilities&quot;, title=&quot;X=-2&quot;)</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>In general, when would you classify <span class="math inline">\(Y\)</span> as <em>A</em>? <em>B</em>? <em>C</em>?</li>
</ol>
</div>
<div id="bonus-random-prediction" class="section level3">
<h3><span class="header-section-number">8.3.3</span> (BONUS) Random prediction</h3>
<p>You might think that, if we know the conditional distribution of <span class="math inline">\(Y\)</span> given some predictors, why not take a random draw from that distribution as our prediction? After all, this would be simulating nature.</p>
<p>The problem is, this prediction doesn’t do well.</p>
<p>Re-do the regression exercise above (feel free to only do Case 1 to prove the point), but this time, instead of using the mean as a prediction, use a random draw from the conditional distributions. Calculate the MSE. How much worse is it? How does this error compare to the original Case 1-4 errors?</p>
</div>
<div id="bonus-a-more-non-standard-regression" class="section level3">
<h3><span class="header-section-number">8.3.4</span> (BONUS) A more non-standard regression</h3>
<p>The regression example given above is your perfect, everything-is-linear-and-Normal world. Let’s see an example of a joint distribution of <span class="math inline">\((X,Y)\)</span> that’s <em>not</em> Normal.</p>
<p>The joint distribution in question can be respresented as follows: <span class="math display">\[ Y|X=x \sim \text{Beta}(e^{-x}, 1/x), \]</span> <span class="math display">\[ X \sim \text{Exp}(1). \]</span></p>
<p>Write a formula that gives a prediction of <span class="math inline">\(Y\)</span> from <span class="math inline">\(X\)</span> (you might have to look up the formula for the mean of a Beta random variable). Generate data, and evaluate the MSE. Plot the data, and the conditional mean as a function of <span class="math inline">\(x\)</span> overtop.</p>
</div>
<div id="bonus-oracle-mse" class="section level3">
<h3><span class="header-section-number">8.3.5</span> (BONUS) Oracle MSE</h3>
<p>What statistical quantity does the mean squared error (MSE) reduce to when we know the true distribution of the data? Hint: if each conditional distribution has a certain variance, what then is the MSE?</p>
<p>What is the error rate in the classification setting?</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="prediction-harnessing-the-signal.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="the-signal-model-functions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/vincenzocoia/Interpreting-Regression/edit/master/060-Reducing_uncertainty_of_the_outcome.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
