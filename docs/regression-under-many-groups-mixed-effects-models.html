<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 23 Regression under many groups: mixed effects models | Interpreting Regression</title>
  <meta name="description" content="A book about the why of regression to help you make decisions about your analysis.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 23 Regression under many groups: mixed effects models | Interpreting Regression" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book about the why of regression to help you make decisions about your analysis." />
  <meta name="github-repo" content="vincenzocoia/Interpreting-Regression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 23 Regression under many groups: mixed effects models | Interpreting Regression" />
  
  <meta name="twitter:description" content="A book about the why of regression to help you make decisions about your analysis." />
  

<meta name="author" content="Vincenzo Coia">


<meta name="date" content="2019-08-09">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regression-when-data-are-missing-multiple-imputation.html">
<link rel="next" href="regression-on-an-entire-distribution-probabilistic-forecasting.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpreting Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#caution"><i class="fa fa-check"></i><b>1.1</b> Caution</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#purpose-of-the-book"><i class="fa fa-check"></i><b>1.2</b> Purpose of the book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#tasks-that-motivate-regression"><i class="fa fa-check"></i><b>1.3</b> Tasks that motivate Regression</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html"><i class="fa fa-check"></i><b>2</b> Regression in the context of problem solving</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#communicating-distillation-3-4"><i class="fa fa-check"></i><b>2.1</b> Communicating (Distillation 3-4)</a></li>
<li class="chapter" data-level="2.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#modelling-distillation-2-3"><i class="fa fa-check"></i><b>2.2</b> Modelling (Distillation 2-3)</a></li>
<li class="chapter" data-level="2.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#asking-useful-statistical-questions-distillation-1-2"><i class="fa fa-check"></i><b>2.3</b> Asking useful statistical questions (Distillation 1-2)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#business-objectives-examples"><i class="fa fa-check"></i><b>2.3.1</b> Business objectives: examples</a></li>
<li class="chapter" data-level="2.3.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-refining"><i class="fa fa-check"></i><b>2.3.2</b> Statistical objectives: refining</a></li>
<li class="chapter" data-level="2.3.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-examples"><i class="fa fa-check"></i><b>2.3.3</b> Statistical objectives: examples</a></li>
<li class="chapter" data-level="2.3.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-questions-are-not-the-full-picture"><i class="fa fa-check"></i><b>2.3.4</b> Statistical questions are not the full picture</a></li>
<li class="chapter" data-level="2.3.5" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-unrelated-to-supervised-learning"><i class="fa fa-check"></i><b>2.3.5</b> Statistical objectives unrelated to supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#prerequisites-to-an-analysis"><i class="fa fa-check"></i><b>2.4</b> Prerequisites to an analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="an-outcome-on-its-own.html"><a href="an-outcome-on-its-own.html"><i class="fa fa-check"></i>An outcome on its own</a></li>
<li class="chapter" data-level="3" data-path="distributions-uncertainty-is-worth-explaining.html"><a href="distributions-uncertainty-is-worth-explaining.html"><i class="fa fa-check"></i><b>3</b> Distributions: Uncertainty is worth explaining</a></li>
<li class="chapter" data-level="4" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html"><i class="fa fa-check"></i><b>4</b> Explaining an uncertain outcome: interpretable quantities</a><ul>
<li class="chapter" data-level="4.1" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#probabilistic-quantities"><i class="fa fa-check"></i><b>4.1</b> Probabilistic Quantities</a></li>
<li class="chapter" data-level="4.2" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#what-is-the-mean-anyway"><i class="fa fa-check"></i><b>4.2</b> What is the mean, anyway?</a></li>
<li class="chapter" data-level="4.3" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#quantiles"><i class="fa fa-check"></i><b>4.3</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>5</b> Estimation</a><ul>
<li class="chapter" data-level="5.1" data-path="estimation.html"><a href="estimation.html#estimation-of-probabilistic-quantities"><i class="fa fa-check"></i><b>5.1</b> Estimation of Probabilistic Quantities</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html"><i class="fa fa-check"></i><b>6</b> Parametric Families of Distributions</a><ul>
<li class="chapter" data-level="6.1" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#parametric-families-of-distributions-1"><i class="fa fa-check"></i><b>6.1</b> Parametric Families of Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#analyses-under-a-distributional-assumption"><i class="fa fa-check"></i><b>6.2</b> Analyses under a Distributional Assumption</a><ul>
<li class="chapter" data-level="6.2.1" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>6.2.1</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="6.2.2" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#usefulness-in-practice"><i class="fa fa-check"></i><b>6.2.2</b> Usefulness in Practice</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html"><i class="fa fa-check"></i>Prediction: harnessing the signal</a></li>
<li class="chapter" data-level="7" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html"><i class="fa fa-check"></i><b>7</b> Reducing uncertainty of the outcome: including predictors</a><ul>
<li class="chapter" data-level="7.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-terminology"><i class="fa fa-check"></i><b>7.1</b> Variable terminology</a><ul>
<li class="chapter" data-level="7.1.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-types"><i class="fa fa-check"></i><b>7.1.1</b> Variable types</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#irreducible-error"><i class="fa fa-check"></i><b>7.2</b> Irreducible Error</a></li>
<li class="chapter" data-level="7.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#in-class-exercises-irreducible-error"><i class="fa fa-check"></i><b>7.3</b> In-class Exercises: Irreducible Error</a><ul>
<li class="chapter" data-level="7.3.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-regression"><i class="fa fa-check"></i><b>7.3.1</b> Oracle regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-classification"><i class="fa fa-check"></i><b>7.3.2</b> Oracle classification</a></li>
<li class="chapter" data-level="7.3.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-random-prediction"><i class="fa fa-check"></i><b>7.3.3</b> (BONUS) Random prediction</a></li>
<li class="chapter" data-level="7.3.4" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-a-more-non-standard-regression"><i class="fa fa-check"></i><b>7.3.4</b> (BONUS) A more non-standard regression</a></li>
<li class="chapter" data-level="7.3.5" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-oracle-mse"><i class="fa fa-check"></i><b>7.3.5</b> (BONUS) Oracle MSE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html"><i class="fa fa-check"></i><b>8</b> The signal: model functions</a><ul>
<li class="chapter" data-level="8.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#linear-quantile-regression"><i class="fa fa-check"></i><b>8.1</b> Linear Quantile Regression</a><ul>
<li class="chapter" data-level="8.1.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#exercise"><i class="fa fa-check"></i><b>8.1.1</b> Exercise</a></li>
<li class="chapter" data-level="8.1.2" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-crossing-quantiles"><i class="fa fa-check"></i><b>8.1.2</b> Problem: Crossing quantiles</a></li>
<li class="chapter" data-level="8.1.3" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-upper-quantiles"><i class="fa fa-check"></i><b>8.1.3</b> Problem: Upper quantiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html"><i class="fa fa-check"></i><b>9</b> The Model-Fitting Paradigm in R</a><ul>
<li class="chapter" data-level="9.1" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html#broom-package"><i class="fa fa-check"></i><b>9.1</b> <code>broom</code> package</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html"><i class="fa fa-check"></i><b>10</b> Estimating parametric model functions</a><ul>
<li class="chapter" data-level="10.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#writing-the-sample-mean-as-an-optimization-problem"><i class="fa fa-check"></i><b>10.1</b> Writing the sample mean as an optimization problem</a></li>
<li class="chapter" data-level="10.2" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#evaluating-model-goodness-quantiles"><i class="fa fa-check"></i><b>10.2</b> Evaluating Model Goodness: Quantiles</a></li>
<li class="chapter" data-level="10.3" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#simple-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="10.3.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#model-specification"><i class="fa fa-check"></i><b>10.3.1</b> Model Specification</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#linear-models-in-general"><i class="fa fa-check"></i><b>10.4</b> Linear models in general</a></li>
<li class="chapter" data-level="10.5" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#reference-treatment-parameterization"><i class="fa fa-check"></i><b>10.5</b> reference-treatment parameterization</a><ul>
<li class="chapter" data-level="10.5.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#more-than-one-category-lab-2"><i class="fa fa-check"></i><b>10.5.1</b> More than one category (Lab 2)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><i class="fa fa-check"></i><b>11</b> Estimating assumption-free: the world of supervised learning techniques</a><ul>
<li class="chapter" data-level="11.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#what-machine-learning-is"><i class="fa fa-check"></i><b>11.1</b> What machine learning is</a></li>
<li class="chapter" data-level="11.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#types-of-supervised-learning"><i class="fa fa-check"></i><b>11.2</b> Types of Supervised Learning</a></li>
<li class="chapter" data-level="11.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#local-regression"><i class="fa fa-check"></i><b>11.3</b> Local Regression</a><ul>
<li class="chapter" data-level="11.3.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#knn"><i class="fa fa-check"></i><b>11.3.1</b> kNN</a></li>
<li class="chapter" data-level="11.3.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess"><i class="fa fa-check"></i><b>11.3.2</b> loess</a></li>
<li class="chapter" data-level="11.3.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#in-class-exercises"><i class="fa fa-check"></i><b>11.3.3</b> In-Class Exercises</a></li>
<li class="chapter" data-level="11.3.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#hyperparameters-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>11.3.4</b> Hyperparameters and the bias/variance tradeoff</a></li>
<li class="chapter" data-level="11.3.5" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#extensions-to-knn-and-loess"><i class="fa fa-check"></i><b>11.3.5</b> Extensions to kNN and loess</a></li>
<li class="chapter" data-level="11.3.6" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#model-assumptions-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>11.3.6</b> Model assumptions and the bias/variance tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#splines-and-loess-regression"><i class="fa fa-check"></i><b>11.4</b> Splines and Loess Regression</a><ul>
<li class="chapter" data-level="11.4.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess-1"><i class="fa fa-check"></i><b>11.4.1</b> Loess</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html"><i class="fa fa-check"></i><b>12</b> Overfitting: The problem with adding too many parameters</a><ul>
<li class="chapter" data-level="12.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#classification-exercise-do-together"><i class="fa fa-check"></i><b>12.1</b> Classification Exercise: Do Together</a></li>
<li class="chapter" data-level="12.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#training-error-vs.generalization-error"><i class="fa fa-check"></i><b>12.2</b> Training Error vs. Generalization Error</a></li>
<li class="chapter" data-level="12.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-complexity"><i class="fa fa-check"></i><b>12.3</b> Model complexity</a><ul>
<li class="chapter" data-level="12.3.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#activity"><i class="fa fa-check"></i><b>12.3.1</b> Activity</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducible-error"><i class="fa fa-check"></i><b>12.4</b> Reducible Error</a><ul>
<li class="chapter" data-level="12.4.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#what-is-it"><i class="fa fa-check"></i><b>12.4.1</b> What is it?</a></li>
<li class="chapter" data-level="12.4.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#example"><i class="fa fa-check"></i><b>12.4.2</b> Example</a></li>
<li class="chapter" data-level="12.4.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#bias-and-variance"><i class="fa fa-check"></i><b>12.4.3</b> Bias and Variance</a></li>
<li class="chapter" data-level="12.4.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducing-reducible-error"><i class="fa fa-check"></i><b>12.4.4</b> Reducing reducible error</a></li>
<li class="chapter" data-level="12.4.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#error-decomposition"><i class="fa fa-check"></i><b>12.4.5</b> Error decomposition</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection"><i class="fa fa-check"></i><b>12.5</b> Model Selection</a><ul>
<li class="chapter" data-level="12.5.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#exercise-cv"><i class="fa fa-check"></i><b>12.5.1</b> Exercise: CV</a></li>
<li class="chapter" data-level="12.5.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#out-of-sample-error"><i class="fa fa-check"></i><b>12.5.2</b> Out-of-sample Error</a></li>
<li class="chapter" data-level="12.5.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#alternative-measures-of-model-goodness"><i class="fa fa-check"></i><b>12.5.3</b> Alternative measures of model goodness</a></li>
<li class="chapter" data-level="12.5.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-and-model-selection-setup"><i class="fa fa-check"></i><b>12.5.4</b> Feature and model selection: setup</a></li>
<li class="chapter" data-level="12.5.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection-1"><i class="fa fa-check"></i><b>12.5.5</b> Model selection</a></li>
<li class="chapter" data-level="12.5.6" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-predictor-selection"><i class="fa fa-check"></i><b>12.5.6</b> Feature (predictor) selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="describing-relationships.html"><a href="describing-relationships.html"><i class="fa fa-check"></i>Describing Relationships</a></li>
<li class="chapter" data-level="13" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html"><i class="fa fa-check"></i><b>13</b> There’s meaning in parameters</a><ul>
<li class="chapter" data-level="13.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#the-types-of-parametric-assumptions"><i class="fa fa-check"></i><b>13.1</b> The types of parametric assumptions</a><ul>
<li class="chapter" data-level="13.1.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#when-defining-a-model-function."><i class="fa fa-check"></i><b>13.1.1</b> 1. When defining a <strong>model function</strong>.</a></li>
<li class="chapter" data-level="13.1.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#when-defining-probability-distributions."><i class="fa fa-check"></i><b>13.1.2</b> 2. When defining <strong>probability distributions</strong>.</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#the-value-of-making-parametric-assumptions"><i class="fa fa-check"></i><b>13.2</b> The value of making parametric assumptions</a><ul>
<li class="chapter" data-level="13.2.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#value-1-reduced-error"><i class="fa fa-check"></i><b>13.2.1</b> Value #1: Reduced Error</a></li>
<li class="chapter" data-level="13.2.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#value-2-interpretation"><i class="fa fa-check"></i><b>13.2.2</b> Value #2: Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#anova"><i class="fa fa-check"></i><b>13.3</b> ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="the-meaning-of-interaction.html"><a href="the-meaning-of-interaction.html"><i class="fa fa-check"></i><b>14</b> The meaning of interaction</a></li>
<li class="chapter" data-level="15" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html"><i class="fa fa-check"></i><b>15</b> Scales and the restricted range problem</a><ul>
<li class="chapter" data-level="15.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#problems"><i class="fa fa-check"></i><b>15.1</b> Problems</a></li>
<li class="chapter" data-level="15.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solutions"><i class="fa fa-check"></i><b>15.2</b> Solutions</a><ul>
<li class="chapter" data-level="15.2.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-1-transformations"><i class="fa fa-check"></i><b>15.2.1</b> Solution 1: Transformations</a></li>
<li class="chapter" data-level="15.2.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-2-link-functions"><i class="fa fa-check"></i><b>15.2.2</b> Solution 2: Link Functions</a></li>
<li class="chapter" data-level="15.2.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-3-scientifically-backed-functions"><i class="fa fa-check"></i><b>15.2.3</b> Solution 3: Scientifically-backed functions</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#glms-in-r"><i class="fa fa-check"></i><b>15.3</b> GLM’s in R</a><ul>
<li class="chapter" data-level="15.3.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#broomaugment"><i class="fa fa-check"></i><b>15.3.1</b> <code>broom::augment()</code></a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#options-for-logistic-regression"><i class="fa fa-check"></i><b>15.4</b> Options for Logistic Regression</a><ul>
<li class="chapter" data-level="15.4.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#models"><i class="fa fa-check"></i><b>15.4.1</b> Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="improving-estimation-through-distributional-assumptions.html"><a href="improving-estimation-through-distributional-assumptions.html"><i class="fa fa-check"></i><b>16</b> Improving estimation through distributional assumptions</a></li>
<li class="chapter" data-level="17" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html"><i class="fa fa-check"></i><b>17</b> When we only want interpretation on some predictors</a><ul>
<li class="chapter" data-level="17.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability-in-gams"><i class="fa fa-check"></i><b>17.1</b> Non-identifiability in GAMS</a><ul>
<li class="chapter" data-level="17.1.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability"><i class="fa fa-check"></i><b>17.1.1</b> Non-identifiability</a></li>
<li class="chapter" data-level="17.1.2" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#question-1b"><i class="fa fa-check"></i><b>17.1.2</b> Question 1b</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="special-cases.html"><a href="special-cases.html"><i class="fa fa-check"></i>Special cases</a></li>
<li class="chapter" data-level="18" data-path="regression-when-data-are-censored-survival-analysis.html"><a href="regression-when-data-are-censored-survival-analysis.html"><i class="fa fa-check"></i><b>18</b> Regression when data are censored: survival analysis</a></li>
<li class="chapter" data-level="19" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html"><i class="fa fa-check"></i><b>19</b> Regression in the presence of outliers: robust regression</a><ul>
<li class="chapter" data-level="19.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#robust-regression-in-r"><i class="fa fa-check"></i><b>19.1</b> Robust Regression in R</a><ul>
<li class="chapter" data-level="19.1.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#heavy-tailed-regression"><i class="fa fa-check"></i><b>19.1.1</b> Heavy Tailed Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="regression-in-the-presence-of-extremes-extreme-value-regression.html"><a href="regression-in-the-presence-of-extremes-extreme-value-regression.html"><i class="fa fa-check"></i><b>20</b> Regression in the presence of extremes: extreme value regression</a></li>
<li class="chapter" data-level="21" data-path="regression-when-data-are-ordinal.html"><a href="regression-when-data-are-ordinal.html"><i class="fa fa-check"></i><b>21</b> Regression when data are ordinal</a></li>
<li class="chapter" data-level="22" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html"><i class="fa fa-check"></i><b>22</b> Regression when data are missing: multiple imputation</a><ul>
<li class="chapter" data-level="22.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#mean-imputation"><i class="fa fa-check"></i><b>22.1</b> Mean Imputation</a></li>
<li class="chapter" data-level="22.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation"><i class="fa fa-check"></i><b>22.2</b> Multiple Imputation</a><ul>
<li class="chapter" data-level="22.2.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#patterns"><i class="fa fa-check"></i><b>22.2.1</b> Patterns</a></li>
<li class="chapter" data-level="22.2.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation-1"><i class="fa fa-check"></i><b>22.2.2</b> Multiple Imputation</a></li>
<li class="chapter" data-level="22.2.3" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#pooling"><i class="fa fa-check"></i><b>22.2.3</b> Pooling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html"><i class="fa fa-check"></i><b>23</b> Regression under many groups: mixed effects models</a><ul>
<li class="chapter" data-level="23.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#motivation-for-lme"><i class="fa fa-check"></i><b>23.1</b> Motivation for LME</a><ul>
<li class="chapter" data-level="23.1.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#definition"><i class="fa fa-check"></i><b>23.1.1</b> Definition</a></li>
<li class="chapter" data-level="23.1.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#r-tools-for-fitting"><i class="fa fa-check"></i><b>23.1.2</b> R Tools for Fitting</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#mixed-effects-models-in-r-tutorial"><i class="fa fa-check"></i><b>23.2</b> Mixed Effects Models in R: tutorial</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html"><i class="fa fa-check"></i><b>24</b> Regression on an entire distribution: Probabilistic Forecasting</a><ul>
<li class="chapter" data-level="24.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasting-what-it-is"><i class="fa fa-check"></i><b>24.1</b> Probabilistic Forecasting: What it is</a></li>
<li class="chapter" data-level="24.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#review-univariate-distribution-estimates"><i class="fa fa-check"></i><b>24.2</b> Review: Univariate distribution estimates</a><ul>
<li class="chapter" data-level="24.2.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#continuous-response"><i class="fa fa-check"></i><b>24.2.1</b> Continuous response</a></li>
<li class="chapter" data-level="24.2.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discrete-response"><i class="fa fa-check"></i><b>24.2.2</b> Discrete Response</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasts-subset-based-learning-methods"><i class="fa fa-check"></i><b>24.3</b> Probabilistic Forecasts: subset-based learning methods</a><ul>
<li class="chapter" data-level="24.3.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#the-techniques"><i class="fa fa-check"></i><b>24.3.1</b> The techniques</a></li>
<li class="chapter" data-level="24.3.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#exercise-1"><i class="fa fa-check"></i><b>24.3.2</b> Exercise</a></li>
<li class="chapter" data-level="24.3.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>24.3.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="24.3.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#evaluating-model-goodness"><i class="fa fa-check"></i><b>24.3.4</b> Evaluating Model Goodness</a></li>
</ul></li>
<li class="chapter" data-level="24.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discussion-points"><i class="fa fa-check"></i><b>24.4</b> Discussion Points</a></li>
<li class="chapter" data-level="24.5" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#when-are-they-not-useful"><i class="fa fa-check"></i><b>24.5</b> When are they not useful?</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html"><i class="fa fa-check"></i><b>25</b> Regression when order matters: time series and spatial analysis</a><ul>
<li class="chapter" data-level="25.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#timeseries-in-base-r"><i class="fa fa-check"></i><b>25.1</b> Timeseries in (base) R</a></li>
<li class="chapter" data-level="25.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#spatial-example"><i class="fa fa-check"></i><b>25.2</b> Spatial Example</a></li>
<li class="chapter" data-level="25.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#a-model-for-river-rock-size"><i class="fa fa-check"></i><b>25.3</b> A Model for River Rock Size</a><ul>
<li class="chapter" data-level="25.3.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#average-rock-size"><i class="fa fa-check"></i><b>25.3.1</b> 1. Average rock size:</a></li>
<li class="chapter" data-level="25.3.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-rock-size"><i class="fa fa-check"></i><b>25.3.2</b> 2. Mean rock size:</a></li>
<li class="chapter" data-level="25.3.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#downstream-fining-curve"><i class="fa fa-check"></i><b>25.3.3</b> 3. Downstream fining curve:</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objectives"><i class="fa fa-check"></i><b>25.4</b> Statistical Objectives</a><ul>
<li class="chapter" data-level="25.4.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#preliminaries-variance-and-correlation"><i class="fa fa-check"></i><b>25.4.1</b> Preliminaries: Variance and Correlation</a></li>
</ul></li>
<li class="chapter" data-level="25.5" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#three-concepts"><i class="fa fa-check"></i><b>25.5</b> Three Concepts</a><ul>
<li class="chapter" data-level="25.5.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#error-variance-sigma_e2leftxright"><i class="fa fa-check"></i><b>25.5.1</b> Error Variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span></a></li>
<li class="chapter" data-level="25.5.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-variance-sigma_m2"><i class="fa fa-check"></i><b>25.5.2</b> Mean Variance <span class="math inline">\(\sigma_{M}^{2}\)</span></a></li>
<li class="chapter" data-level="25.5.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-correlation-rholeftdright"><i class="fa fa-check"></i><b>25.5.3</b> Mean Correlation <span class="math inline">\(\rho\left(d\right)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="25.6" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#estimation-1"><i class="fa fa-check"></i><b>25.6</b> Estimation</a><ul>
<li class="chapter" data-level="25.6.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#constant-error-variance"><i class="fa fa-check"></i><b>25.6.1</b> Constant Error Variance</a></li>
<li class="chapter" data-level="25.6.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#non-constant-error-variance"><i class="fa fa-check"></i><b>25.6.2</b> Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="25.7" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-1-downstream-fining-curve"><i class="fa fa-check"></i><b>25.7</b> Statistical Objective 1: Downstream Fining Curve</a><ul>
<li class="chapter" data-level="25.7.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#regression-form"><i class="fa fa-check"></i><b>25.7.1</b> Regression Form</a></li>
</ul></li>
<li class="chapter" data-level="25.8" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-2-river-profile"><i class="fa fa-check"></i><b>25.8</b> Statistical Objective 2: River Profile</a><ul>
<li class="chapter" data-level="25.8.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#simple-kriging"><i class="fa fa-check"></i><b>25.8.1</b> Simple Kriging</a></li>
<li class="chapter" data-level="25.8.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#universal-kriging"><i class="fa fa-check"></i><b>25.8.2</b> Universal Kriging</a></li>
<li class="chapter" data-level="25.8.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#kriging-under-non-constant-error-variance"><i class="fa fa-check"></i><b>25.8.3</b> Kriging under Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="25.9" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#confidence-intervals-of-the-river-profile"><i class="fa fa-check"></i><b>25.9</b> Confidence Intervals of the River Profile</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/vincenzocoia/Interpreting-Regression" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpreting Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression-under-many-groups-mixed-effects-models" class="section level1">
<h1><span class="header-section-number">Chapter 23</span> Regression under many groups: mixed effects models</h1>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">suppressPackageStartupMessages</span>(<span class="kw">library</span>(tidyverse))</code></pre></div>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;tibble&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;purrr&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;dplyr&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;stringr&#39; was built under R version 3.5.2</code></pre>
<div id="motivation-for-lme" class="section level2">
<h2><span class="header-section-number">23.1</span> Motivation for LME</h2>
<p>Let’s take a look at the <code>esoph</code> data set, to see how the number of controls <code>ncontrols</code> affects the number of cases <code>ncases</code> of cancer for each age group <code>agegp</code>. Here’s what the data look like (with a tad bit of vertical jitter):</p>
<p><img src="210-Regression_under_many_groups_files/figure-html/unnamed-chunk-2-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>It seems each age group has a different relationship. Should we then fit regression lines for each group separately? Here’s what we get, if we do:</p>
<p><img src="210-Regression_under_many_groups_files/figure-html/unnamed-chunk-3-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>But, each group has so few observations, making the regression less powerful:</p>
<pre><code>## # A tibble: 6 x 2
##   agegp     n
##   &lt;ord&gt; &lt;int&gt;
## 1 25-34    15
## 2 35-44    15
## 3 45-54    16
## 4 55-64    16
## 5 65-74    15
## 6 75+      11</code></pre>
<p><strong>Question</strong>: can we borrow information across groups to strengthen regression, while still allowing each group to have its own regression line?</p>
<p>Here’s another scenario: suppose we want to know the effect of <code>ncontrols</code> on the average person. Then, we would only include one common slope parameter for all individuals. Even if each individual “has their own unique slope”, this model is still sensible because the common slope can be interpreted as the <em>average effect</em>. The problem with this model is that the typical estimates of standard error on our regression coefficients will be artificially small due to correlation in the data induced by the grouping.</p>
<p>Here is a simulation that compares the “actual” SE (or at least an approximation of it) and the SE reported by <code>lm</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># library(tidyverse)</span>
<span class="co"># library(broom)</span>
<span class="co"># set.seed(1000)</span>
<span class="co"># ## Number of groups</span>
<span class="co"># g &lt;- 10</span>
<span class="co"># ## Number of observations per group</span>
<span class="co"># ng &lt;- 10</span>
<span class="co"># ## Initiate slope and SE estimates</span>
<span class="co"># beta1hat &lt;- numeric(0)</span>
<span class="co"># se &lt;- numeric(0)</span>
<span class="co"># for (i in 1:1000) {</span>
<span class="co">#   ## Generate intercept and slope from a joint Normal distribution</span>
<span class="co">#   beta0 &lt;- rnorm(g)</span>
<span class="co">#   beta1 &lt;- 1 + beta0 + rnorm(g)</span>
<span class="co">#   ## Generate iid data from within each group</span>
<span class="co">#   esoph &lt;- tibble(group=LETTERS[1:g], beta0, beta1) %&gt;%</span>
<span class="co">#     mutate(x = map(beta0, ~ rnorm(ng))) %&gt;%</span>
<span class="co">#     unnest() %&gt;%</span>
<span class="co">#     group_by(group) %&gt;%</span>
<span class="co">#     mutate(eps = rnorm(length(x)),</span>
<span class="co">#            y = beta0 + beta1 * x + eps)</span>
<span class="co">#   ## Fit a linear regression, forcing a common slope</span>
<span class="co">#   fit &lt;- lm(y ~ x + group, data=esoph) %&gt;%</span>
<span class="co">#     tidy()</span>
<span class="co">#   beta1hat[i] &lt;- fit$estimate[2]</span>
<span class="co">#   se[i] &lt;- fit$std.error[2]</span>
<span class="co"># }</span>
<span class="co"># ## Actual SE:</span>
<span class="co"># sd(beta1hat)</span>
<span class="co"># ## SE given from the lm fit:</span>
<span class="co"># mean(se)</span>
<span class="co"># </span>
<span class="co"># ## Here&#39;s a plot of the last sample generated:</span>
<span class="co"># ggplot(esoph, aes(x, y)) +</span>
<span class="co">#   geom_point(aes(colour=group), alpha=0.5) +</span>
<span class="co">#   theme_bw()</span></code></pre></div>
<p><strong>Question</strong>: How can we account for the dependence in the data?</p>
<p>Both questions can be addressed using a <em>Linear Mixed Effects</em> (LME) model. An LME model is just a linear regression model for each group, with different slopes and intercepts, but the collection of slopes and intercepts <em>is assumed to come from some normal distribution</em>.</p>
<div id="definition" class="section level3">
<h3><span class="header-section-number">23.1.1</span> Definition</h3>
<p>With one predictor (<span class="math inline">\(X\)</span>), we can write an LME as follows: <span class="math display">\[ Y = \left(\beta_0 + b_0\right) + \left(\beta_1 + b_1\right) X + \varepsilon,  \]</span> where the error term <span class="math inline">\(\varepsilon\)</span> has mean zero, and the <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> terms are normally distributed having a mean of zero, and some unknown variances and correlation. The <span class="math inline">\(\beta\)</span> terms are called the <em>fixed effects</em>, and the <span class="math inline">\(b\)</span> terms are called the <em>random effects</em>. Since the model has both types of effects, it’s said to be a <em>mixed</em> model – hence the name of “LME”.</p>
<p>Note that we don’t have to make <em>both</em> the slope and intercept random. For example, we can remove the <span class="math inline">\(b_0\)</span> term, which would mean that each group is forced to have the same (fixed) intercept <span class="math inline">\(\beta_0\)</span>. Also, we can add more predictors (<span class="math inline">\(X\)</span> variables).</p>
</div>
<div id="r-tools-for-fitting" class="section level3">
<h3><span class="header-section-number">23.1.2</span> R Tools for Fitting</h3>
<p>Two R packages exist for working with mixed effects models: <code>lme4</code> and <code>nlme</code>. We’ll be using the <code>lme4</code> package (check out <a href="http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models">this</a> discussion on Cross Valiesophed for a comparison of the two packages).</p>
<p>Let’s fit the model. We need to indicate a formula first in the <code>lmer</code> function, and indicate the data set we’re using.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span>lme4::<span class="kw">lmer</span>(ncases ~<span class="st"> </span>ncontrols +<span class="st"> </span>(ncontrols |<span class="st"> </span>agegp), 
            <span class="dt">data=</span>esoph)</code></pre></div>
<p>Let’s take a closer look at the <em>formula</em>, which in this case is <code>ncases ~ ncontrols + (ncontrols | agegp)</code>.</p>
<p>On the left of the <code>~</code> is the response variable, as usual (just like for <code>lm</code>). On the right, we need to specify both the fixed and random effects. The fixed effects part is the same as usual: <code>ncontrols</code> indicates the explanatory variables that get a fixed effect. Then, we need to indicate which explanatory variables get a random effect. The random effects can be indicated in parentheses, separated by <code>+</code>, followed by a <code>|</code>, after which the variable(s) that you wish to group by are indicated. So <code>|</code> can be interpreted as “grouped by”.</p>
<p>Now let’s look at the model output:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fit)</code></pre></div>
<pre><code>## Linear mixed model fit by REML [&#39;lmerMod&#39;]
## Formula: ncases ~ ncontrols + (ncontrols | agegp)
##    Data: esoph
## 
## REML criterion at convergence: 388.6
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.6510 -0.3710 -0.1301  0.3683  4.8056 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  agegp    (Intercept) 1.694453 1.30171      
##           ncontrols   0.005729 0.07569  0.26
##  Residual             3.732899 1.93207      
## Number of obs: 88, groups:  agegp, 6
## 
## Fixed effects:
##             Estimate Std. Error t value
## (Intercept)  1.63379    0.59994   2.723
## ncontrols    0.04971    0.03676   1.352
## 
## Correlation of Fixed Effects:
##           (Intr)
## ncontrols 0.038</code></pre>
<p>The random and fixed effects are indicated here.</p>
<ul>
<li>Under the “Random effects:” section, we have the variance of each random effect, and the lower part of the correlation matrix of these random effects.</li>
<li>Under the “Fixed effects:” section, we have the estimates of the fixed effects, as well as the uncertainty in the estimate (indicated by the Std. Error).</li>
</ul>
<p>We can extract the collection of slopes and intercepts for each group using the <code>coef</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(par_coll &lt;-<span class="st"> </span><span class="kw">coef</span>(fit)[[<span class="dv">1</span>]])</code></pre></div>
<pre><code>##       (Intercept)    ncontrols
## 25-34   0.2674067 -0.002914520
## 35-44   0.7227280 -0.001127293
## 45-54   2.2834139  0.036587885
## 55-64   3.5108403  0.064242966
## 65-74   1.8699415  0.171918181
## 75+     1.1484332  0.029581764</code></pre>
<p>Let’s put these regression lines on the plot:</p>
<pre><code>## Warning: Column `agegp` joining character vector and factor, coercing into
## character vector</code></pre>
<pre><code>##    agegp (Intercept)  ncontrols.x     alcgp    tobgp ncases ncontrols.y
## 1  25-34   0.2674067 -0.002914520 0-39g/day 0-9g/day      0          40
## 2  25-34   0.2674067 -0.002914520 0-39g/day    10-19      0          10
## 3  25-34   0.2674067 -0.002914520 0-39g/day    20-29      0           6
## 4  25-34   0.2674067 -0.002914520 0-39g/day      30+      0           5
## 5  25-34   0.2674067 -0.002914520     40-79 0-9g/day      0          27
## 6  25-34   0.2674067 -0.002914520     40-79    10-19      0           7
## 7  25-34   0.2674067 -0.002914520     40-79    20-29      0           4
## 8  25-34   0.2674067 -0.002914520     40-79      30+      0           7
## 9  25-34   0.2674067 -0.002914520    80-119 0-9g/day      0           2
## 10 25-34   0.2674067 -0.002914520    80-119    10-19      0           1
## 11 25-34   0.2674067 -0.002914520    80-119      30+      0           2
## 12 25-34   0.2674067 -0.002914520      120+ 0-9g/day      0           1
## 13 25-34   0.2674067 -0.002914520      120+    10-19      1           1
## 14 25-34   0.2674067 -0.002914520      120+    20-29      0           1
## 15 25-34   0.2674067 -0.002914520      120+      30+      0           2
## 16 35-44   0.7227280 -0.001127293 0-39g/day 0-9g/day      0          60
## 17 35-44   0.7227280 -0.001127293 0-39g/day    10-19      1          14
## 18 35-44   0.7227280 -0.001127293 0-39g/day    20-29      0           7
## 19 35-44   0.7227280 -0.001127293 0-39g/day      30+      0           8
## 20 35-44   0.7227280 -0.001127293     40-79 0-9g/day      0          35
## 21 35-44   0.7227280 -0.001127293     40-79    10-19      3          23
## 22 35-44   0.7227280 -0.001127293     40-79    20-29      1          14
## 23 35-44   0.7227280 -0.001127293     40-79      30+      0           8
## 24 35-44   0.7227280 -0.001127293    80-119 0-9g/day      0          11
## 25 35-44   0.7227280 -0.001127293    80-119    10-19      0           6
## 26 35-44   0.7227280 -0.001127293    80-119    20-29      0           2
## 27 35-44   0.7227280 -0.001127293    80-119      30+      0           1
## 28 35-44   0.7227280 -0.001127293      120+ 0-9g/day      2           3
## 29 35-44   0.7227280 -0.001127293      120+    10-19      0           3
## 30 35-44   0.7227280 -0.001127293      120+    20-29      2           4
## 31 45-54   2.2834139  0.036587885 0-39g/day 0-9g/day      1          46
## 32 45-54   2.2834139  0.036587885 0-39g/day    10-19      0          18
## 33 45-54   2.2834139  0.036587885 0-39g/day    20-29      0          10
## 34 45-54   2.2834139  0.036587885 0-39g/day      30+      0           4
## 35 45-54   2.2834139  0.036587885     40-79 0-9g/day      6          38
## 36 45-54   2.2834139  0.036587885     40-79    10-19      4          21
## 37 45-54   2.2834139  0.036587885     40-79    20-29      5          15
## 38 45-54   2.2834139  0.036587885     40-79      30+      5           7
## 39 45-54   2.2834139  0.036587885    80-119 0-9g/day      3          16
## 40 45-54   2.2834139  0.036587885    80-119    10-19      6          14
## 41 45-54   2.2834139  0.036587885    80-119    20-29      1           5
## 42 45-54   2.2834139  0.036587885    80-119      30+      2           4
## 43 45-54   2.2834139  0.036587885      120+ 0-9g/day      4           4
## 44 45-54   2.2834139  0.036587885      120+    10-19      3           4
## 45 45-54   2.2834139  0.036587885      120+    20-29      2           3
## 46 45-54   2.2834139  0.036587885      120+      30+      4           4
## 47 55-64   3.5108403  0.064242966 0-39g/day 0-9g/day      2          49
## 48 55-64   3.5108403  0.064242966 0-39g/day    10-19      3          22
## 49 55-64   3.5108403  0.064242966 0-39g/day    20-29      3          12
## 50 55-64   3.5108403  0.064242966 0-39g/day      30+      4           6
## 51 55-64   3.5108403  0.064242966     40-79 0-9g/day      9          40
## 52 55-64   3.5108403  0.064242966     40-79    10-19      6          21
## 53 55-64   3.5108403  0.064242966     40-79    20-29      4          17
## 54 55-64   3.5108403  0.064242966     40-79      30+      3           6
## 55 55-64   3.5108403  0.064242966    80-119 0-9g/day      9          18
## 56 55-64   3.5108403  0.064242966    80-119    10-19      8          15
## 57 55-64   3.5108403  0.064242966    80-119    20-29      3           6
## 58 55-64   3.5108403  0.064242966    80-119      30+      4           4
## 59 55-64   3.5108403  0.064242966      120+ 0-9g/day      5          10
## 60 55-64   3.5108403  0.064242966      120+    10-19      6           7
## 61 55-64   3.5108403  0.064242966      120+    20-29      2           3
## 62 55-64   3.5108403  0.064242966      120+      30+      5           6
## 63 65-74   1.8699415  0.171918181 0-39g/day 0-9g/day      5          48
## 64 65-74   1.8699415  0.171918181 0-39g/day    10-19      4          14
## 65 65-74   1.8699415  0.171918181 0-39g/day    20-29      2           7
## 66 65-74   1.8699415  0.171918181 0-39g/day      30+      0           2
## 67 65-74   1.8699415  0.171918181     40-79 0-9g/day     17          34
## 68 65-74   1.8699415  0.171918181     40-79    10-19      3          10
## 69 65-74   1.8699415  0.171918181     40-79    20-29      5           9
## 70 65-74   1.8699415  0.171918181    80-119 0-9g/day      6          13
## 71 65-74   1.8699415  0.171918181    80-119    10-19      4          12
## 72 65-74   1.8699415  0.171918181    80-119    20-29      2           3
## 73 65-74   1.8699415  0.171918181    80-119      30+      1           1
## 74 65-74   1.8699415  0.171918181      120+ 0-9g/day      3           4
## 75 65-74   1.8699415  0.171918181      120+    10-19      1           2
## 76 65-74   1.8699415  0.171918181      120+    20-29      1           1
## 77 65-74   1.8699415  0.171918181      120+      30+      1           1
## 78   75+   1.1484332  0.029581764 0-39g/day 0-9g/day      1          18
## 79   75+   1.1484332  0.029581764 0-39g/day    10-19      2           6
## 80   75+   1.1484332  0.029581764 0-39g/day      30+      1           3
## 81   75+   1.1484332  0.029581764     40-79 0-9g/day      2           5
## 82   75+   1.1484332  0.029581764     40-79    10-19      1           3
## 83   75+   1.1484332  0.029581764     40-79    20-29      0           3
## 84   75+   1.1484332  0.029581764     40-79      30+      1           1
## 85   75+   1.1484332  0.029581764    80-119 0-9g/day      1           1
## 86   75+   1.1484332  0.029581764    80-119    10-19      1           1
## 87   75+   1.1484332  0.029581764      120+ 0-9g/day      2           2
## 88   75+   1.1484332  0.029581764      120+    10-19      1           1</code></pre>
<p><img src="210-Regression_under_many_groups_files/figure-html/unnamed-chunk-9-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>So, each group still gets its own regression line, but tying the parameters together with a normal distribution gives us a more powerful regression.</p>
</div>
</div>
<div id="mixed-effects-models-in-r-tutorial" class="section level2">
<h2><span class="header-section-number">23.2</span> Mixed Effects Models in R: tutorial</h2>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<p>Two R packages exist for working with mixed effects models: <code>lme4</code> and <code>nlme</code>. We’ll be using the <code>lme4</code> package (check out <a href="http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models">this</a> discussion on Cross Valiesophed for a comparison of the two packages).</p>
<p>In Lab 1, we compared linear regression (function <code>lm</code>) with GLM’s (function <code>glm</code>). In Lab 2, we consider adding a random effect to either of these:</p>
<ul>
<li>A linear model with random effects is a <em>Linear Mixed-Effects Model</em>, and is fit using the <code>lmer</code> function.</li>
<li>A generalized linear model with random effects is a <em>Generalized Linear Mixed-Effects Model</em>, and is fit using the <code>glmer</code> function.</li>
</ul>
<p>We’ll work with the <code>esoph</code> data set, to see how the number of controls <code>ncontrols</code> affects the number of cases <code>ncases</code> based on age group <code>agegp</code>. Here’s what the data look like (with a tad bit of vertical jitter):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p &lt;-<span class="st"> </span><span class="kw">ggplot</span>(esoph, <span class="kw">aes</span>(ncontrols, ncases)) +
<span class="st">    </span><span class="kw">geom_jitter</span>(<span class="kw">aes</span>(<span class="dt">colour=</span>agegp), <span class="dt">height=</span><span class="fl">0.25</span>)
p</code></pre></div>
<p><img src="210-Regression_under_many_groups_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Since the response is a count variable, we’ll go ahead with a Poisson regression – a Generalized Linear Mixed-Effects Model. The model is <span class="math display">\[ Y_{ij} \mid X_{ij} = x_{ij} \sim \text{Poisson}\left(\lambda_{ij}\right) \]</span> for each observation <span class="math inline">\(i\)</span> on the <span class="math inline">\(j\)</span>’th age group, where <span class="math inline">\(Y_{ij}\)</span> is the number of cases, <span class="math inline">\(X_{ij}\)</span> is the number of controls, and <span class="math inline">\(\lambda_{ij}\)</span> is the conditional mean of <span class="math inline">\(Y_{ij}.\)</span> We model <span class="math inline">\(\lambda_{ij}\)</span> as <span class="math display">\[ \log\left(\lambda_{ij}\right) = \left(\beta_0 + b_{0j}\right) + \left(\beta_1 + b_{1j}\right) x_{ij}, \]</span> where <span class="math inline">\(b_{0j}\)</span> and <span class="math inline">\(b_{1j}\)</span> are joint (bivariate) normally distributed with zero mean.</p>
<p>What does this model mean? First, it means that the mean is exponential in the explanatory variable, since we chose a <span class="math inline">\(\log\)</span> link function. Second, each age group (<span class="math inline">\(j\)</span>) gets its own mean curve, via its own linear predictor. But we’re saying that these linear predictors are related: the collection of slopes and intercepts across age groups are centered around <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> (respectively, called the <em>fixed effects</em>), and the slope and intercept of each age group departs from this center according to some Gaussian random noise (the <span class="math inline">\(b\)</span> terms, called the <em>random effects</em>).</p>
<p>Let’s fit the model. Then we’ll go through the formula, and the output.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit &lt;-<span class="st"> </span>lme4::<span class="kw">glmer</span>(ncases ~<span class="st"> </span>ncontrols +<span class="st"> </span>(<span class="dv">1</span> +<span class="st"> </span>ncontrols |<span class="st"> </span>agegp), 
                   <span class="dt">data=</span>esoph, 
                   <span class="dt">family=</span>poisson)
<span class="kw">summary</span>(fit)</code></pre></div>
<pre><code>## Generalized linear mixed model fit by maximum likelihood (Laplace
##   Approximation) [glmerMod]
##  Family: poisson  ( log )
## Formula: ncases ~ ncontrols + (1 + ncontrols | agegp)
##    Data: esoph
## 
##      AIC      BIC   logLik deviance df.resid 
##    315.1    327.5   -152.5    305.1       83 
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -1.8527 -0.7919 -0.3286  0.4776  3.8037 
## 
## Random effects:
##  Groups Name        Variance  Std.Dev. Corr
##  agegp  (Intercept) 1.2343732 1.11102      
##         ncontrols   0.0003231 0.01797  0.66
## Number of obs: 88, groups:  agegp, 6
## 
## Fixed effects:
##             Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) 0.135467   0.483821   0.280    0.779
## ncontrols   0.006613   0.013945   0.474    0.635
## 
## Correlation of Fixed Effects:
##           (Intr)
## ncontrols 0.174</code></pre>
<p>To specify the formula, the fixed effects part is the same as usual: <code>ncases ~ ncontrols</code> gives you <code>ncases = beta0 + beta1 * ncontrols</code>. Note that the intercept is put in there by default. Then, we need to indicate which explanatory variables are getting the random effects – including the intercept this time (with a 1), if you want it (in this case, we do). The random effects can be indicated in parentheses, separated by <code>+</code>, followed by a <code>|</code>, after which the variable(s) that you wish to group by are indicated. So <code>|</code> can be interpreted as “grouped by”.</p>
<p>The output of the model fit is similar to what you’ve seen before (in <code>glm</code> for example), but the “random effects” part is new. That gives us the estimates of the joint normal distribution of the random effects – through the variances, and correlation matrix to the right (only the lower-diagonal of the correlation matrix is given, because that matrix is symmetric anyway).</p>
<p>Let’s see what the intercepts and slopes for each age group are, and let’s plot the estimated mean curves:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(coef_fit &lt;-<span class="st"> </span><span class="kw">coef</span>(fit)$agegp)</code></pre></div>
<pre><code>##       (Intercept)    ncontrols
## 25-34  -1.7354568 -0.015913381
## 35-44  -0.4511795 -0.001500806
## 45-54   0.8834953  0.010877993
## 55-64   1.3497975  0.011718494
## 65-74   0.8568544  0.031875270
## 75+     0.1396592  0.006661840</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">## Colours with stat_function are not nice to deal with. Do manually.
p +<span class="st"> </span><span class="kw">stat_function</span>(<span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;25-34&quot;</span>), <span class="dt">fun =</span> function(x) <span class="kw">exp</span>(coef_fit[<span class="dv">1</span>,<span class="dv">1</span>] +<span class="st"> </span>coef_fit[<span class="dv">1</span>,<span class="dv">2</span>]*x)) +
<span class="st">    </span><span class="kw">stat_function</span>(<span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;35-44&quot;</span>), <span class="dt">fun =</span> function(x) <span class="kw">exp</span>(coef_fit[<span class="dv">2</span>,<span class="dv">1</span>] +<span class="st"> </span>coef_fit[<span class="dv">2</span>,<span class="dv">2</span>]*x)) +
<span class="st">    </span><span class="kw">stat_function</span>(<span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;45-54&quot;</span>), <span class="dt">fun =</span> function(x) <span class="kw">exp</span>(coef_fit[<span class="dv">3</span>,<span class="dv">1</span>] +<span class="st"> </span>coef_fit[<span class="dv">3</span>,<span class="dv">2</span>]*x)) +
<span class="st">    </span><span class="kw">stat_function</span>(<span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;55-64&quot;</span>), <span class="dt">fun =</span> function(x) <span class="kw">exp</span>(coef_fit[<span class="dv">4</span>,<span class="dv">1</span>] +<span class="st"> </span>coef_fit[<span class="dv">4</span>,<span class="dv">2</span>]*x)) +
<span class="st">    </span><span class="kw">stat_function</span>(<span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;65-74&quot;</span>), <span class="dt">fun =</span> function(x) <span class="kw">exp</span>(coef_fit[<span class="dv">5</span>,<span class="dv">1</span>] +<span class="st"> </span>coef_fit[<span class="dv">5</span>,<span class="dv">2</span>]*x)) +
<span class="st">    </span><span class="kw">stat_function</span>(<span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;75+&quot;</span>),   <span class="dt">fun =</span> function(x) <span class="kw">exp</span>(coef_fit[<span class="dv">6</span>,<span class="dv">1</span>] +<span class="st"> </span>coef_fit[<span class="dv">6</span>,<span class="dv">2</span>]*x))</code></pre></div>
<p><img src="210-Regression_under_many_groups_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>A (response-) residual plot is somewhat sensible to look at here:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit)</code></pre></div>
<p><img src="210-Regression_under_many_groups_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>Looks fairly centered at zero, so the shape of the mean curves are satisfactory.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-when-data-are-missing-multiple-imputation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="regression-on-an-entire-distribution-probabilistic-forecasting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/vincenzocoia/Interpreting-Regression/edit/master/210-Regression_under_many_groups.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Interpreting-Regression.pdf", "Interpreting-Regression.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
