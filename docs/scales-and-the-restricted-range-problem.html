<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 16 Scales and the restricted range problem | Interpreting Regression</title>
  <meta name="description" content="My tutorials for regression analysis, in the form of a bookdown book.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 16 Scales and the restricted range problem | Interpreting Regression" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="My tutorials for regression analysis, in the form of a bookdown book." />
  <meta name="github-repo" content="vincenzocoia/Interpreting-Regression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 16 Scales and the restricted range problem | Interpreting Regression" />
  
  <meta name="twitter:description" content="My tutorials for regression analysis, in the form of a bookdown book." />
  

<meta name="author" content="Vincenzo Coia">


<meta name="date" content="2019-05-29">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="the-meaning-of-interaction.html">
<link rel="next" href="improving-estimation-through-distributional-assumptions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpreting Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#caution"><i class="fa fa-check"></i><b>1.1</b> Caution</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#purpose-of-the-book"><i class="fa fa-check"></i><b>1.2</b> Purpose of the book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#tasks-that-motivate-regression"><i class="fa fa-check"></i><b>1.3</b> Tasks that motivate Regression</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html"><i class="fa fa-check"></i><b>2</b> Regression in the context of problem solving</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#communicating-distillation-3-4"><i class="fa fa-check"></i><b>2.1</b> Communicating (Distillation 3-4)</a></li>
<li class="chapter" data-level="2.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#modelling-distillation-2-3"><i class="fa fa-check"></i><b>2.2</b> Modelling (Distillation 2-3)</a></li>
<li class="chapter" data-level="2.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#asking-useful-statistical-questions-distillation-1-2"><i class="fa fa-check"></i><b>2.3</b> Asking useful statistical questions (Distillation 1-2)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#business-objectives-examples"><i class="fa fa-check"></i><b>2.3.1</b> Business objectives: examples</a></li>
<li class="chapter" data-level="2.3.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-refining"><i class="fa fa-check"></i><b>2.3.2</b> Statistical objectives: refining</a></li>
<li class="chapter" data-level="2.3.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-examples"><i class="fa fa-check"></i><b>2.3.3</b> Statistical objectives: examples</a></li>
<li class="chapter" data-level="2.3.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-questions-are-not-the-full-picture"><i class="fa fa-check"></i><b>2.3.4</b> Statistical questions are not the full picture</a></li>
<li class="chapter" data-level="2.3.5" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-unrelated-to-supervised-learning"><i class="fa fa-check"></i><b>2.3.5</b> Statistical objectives unrelated to supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#prerequisites-to-an-analysis"><i class="fa fa-check"></i><b>2.4</b> Prerequisites to an analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="an-outcome-on-its-own.html"><a href="an-outcome-on-its-own.html"><i class="fa fa-check"></i>An outcome on its own</a></li>
<li class="chapter" data-level="3" data-path="distributions-uncertainty-is-worth-explaining.html"><a href="distributions-uncertainty-is-worth-explaining.html"><i class="fa fa-check"></i><b>3</b> Distributions: Uncertainty is worth explaining</a></li>
<li class="chapter" data-level="4" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html"><i class="fa fa-check"></i><b>4</b> Explaining an uncertain outcome: interpretable quantities</a><ul>
<li class="chapter" data-level="4.1" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#probabilistic-quantities"><i class="fa fa-check"></i><b>4.1</b> Probabilistic Quantities</a></li>
<li class="chapter" data-level="4.2" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#what-is-the-mean-anyway"><i class="fa fa-check"></i><b>4.2</b> What is the mean, anyway?</a></li>
<li class="chapter" data-level="4.3" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#quantiles"><i class="fa fa-check"></i><b>4.3</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-versions-of-interpretable-quantities.html"><a href="data-versions-of-interpretable-quantities.html"><i class="fa fa-check"></i><b>5</b> Data versions of interpretable quantities</a><ul>
<li class="chapter" data-level="5.1" data-path="data-versions-of-interpretable-quantities.html"><a href="data-versions-of-interpretable-quantities.html#estimation-of-probabilistic-quantities"><i class="fa fa-check"></i><b>5.1</b> Estimation of Probabilistic Quantities</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="sampling-distributions-another-layer-of-uncertainty-added-from-estimation.html"><a href="sampling-distributions-another-layer-of-uncertainty-added-from-estimation.html"><i class="fa fa-check"></i><b>6</b> Sampling distributions: Another layer of uncertainty added from estimation</a></li>
<li class="chapter" data-level="7" data-path="improving-estimator-quality-by-parametric-distributional-assumptions-and-mle.html"><a href="improving-estimator-quality-by-parametric-distributional-assumptions-and-mle.html"><i class="fa fa-check"></i><b>7</b> Improving estimator quality by parametric distributional assumptions and MLE</a></li>
<li class="chapter" data-level="" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html"><i class="fa fa-check"></i>Prediction: harnessing the signal</a></li>
<li class="chapter" data-level="8" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html"><i class="fa fa-check"></i><b>8</b> Reducing uncertainty of the outcome: including predictors</a><ul>
<li class="chapter" data-level="8.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-terminology"><i class="fa fa-check"></i><b>8.1</b> Variable terminology</a><ul>
<li class="chapter" data-level="8.1.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-types"><i class="fa fa-check"></i><b>8.1.1</b> Variable types</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#irreducible-error"><i class="fa fa-check"></i><b>8.2</b> Irreducible Error</a></li>
<li class="chapter" data-level="8.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#in-class-exercises-irreducible-error"><i class="fa fa-check"></i><b>8.3</b> In-class Exercises: Irreducible Error</a><ul>
<li class="chapter" data-level="8.3.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-regression"><i class="fa fa-check"></i><b>8.3.1</b> Oracle regression</a></li>
<li class="chapter" data-level="8.3.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-classification"><i class="fa fa-check"></i><b>8.3.2</b> Oracle classification</a></li>
<li class="chapter" data-level="8.3.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-random-prediction"><i class="fa fa-check"></i><b>8.3.3</b> (BONUS) Random prediction</a></li>
<li class="chapter" data-level="8.3.4" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-a-more-non-standard-regression"><i class="fa fa-check"></i><b>8.3.4</b> (BONUS) A more non-standard regression</a></li>
<li class="chapter" data-level="8.3.5" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-oracle-mse"><i class="fa fa-check"></i><b>8.3.5</b> (BONUS) Oracle MSE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html"><i class="fa fa-check"></i><b>9</b> The signal: model functions</a><ul>
<li class="chapter" data-level="9.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#linear-quantile-regression"><i class="fa fa-check"></i><b>9.1</b> Linear Quantile Regression</a><ul>
<li class="chapter" data-level="9.1.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#exercise"><i class="fa fa-check"></i><b>9.1.1</b> Exercise</a></li>
<li class="chapter" data-level="9.1.2" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-crossing-quantiles"><i class="fa fa-check"></i><b>9.1.2</b> Problem: Crossing quantiles</a></li>
<li class="chapter" data-level="9.1.3" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-upper-quantiles"><i class="fa fa-check"></i><b>9.1.3</b> Problem: Upper quantiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html"><i class="fa fa-check"></i><b>10</b> The Model-Fitting Paradigm in R</a><ul>
<li class="chapter" data-level="10.1" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html#broom-package"><i class="fa fa-check"></i><b>10.1</b> <code>broom</code> package</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html"><i class="fa fa-check"></i><b>11</b> Estimating parametric model functions</a><ul>
<li class="chapter" data-level="11.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#writing-the-sample-mean-as-an-optimization-problem"><i class="fa fa-check"></i><b>11.1</b> Writing the sample mean as an optimization problem</a></li>
<li class="chapter" data-level="11.2" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#evaluating-model-goodness-quantiles"><i class="fa fa-check"></i><b>11.2</b> Evaluating Model Goodness: Quantiles</a></li>
<li class="chapter" data-level="11.3" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#simple-linear-regression"><i class="fa fa-check"></i><b>11.3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="11.3.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#model-specification"><i class="fa fa-check"></i><b>11.3.1</b> Model Specification</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#linear-models-in-general"><i class="fa fa-check"></i><b>11.4</b> Linear models in general</a></li>
<li class="chapter" data-level="11.5" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#reference-treatment-parameterization"><i class="fa fa-check"></i><b>11.5</b> reference-treatment parameterization</a><ul>
<li class="chapter" data-level="11.5.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#more-than-one-category-lab-2"><i class="fa fa-check"></i><b>11.5.1</b> More than one category (Lab 2)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><i class="fa fa-check"></i><b>12</b> Estimating assumption-free: the world of supervised learning techniques</a><ul>
<li class="chapter" data-level="12.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#what-machine-learning-is"><i class="fa fa-check"></i><b>12.1</b> What machine learning is</a></li>
<li class="chapter" data-level="12.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#types-of-supervised-learning"><i class="fa fa-check"></i><b>12.2</b> Types of Supervised Learning</a></li>
<li class="chapter" data-level="12.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#local-regression"><i class="fa fa-check"></i><b>12.3</b> Local Regression</a><ul>
<li class="chapter" data-level="12.3.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#knn"><i class="fa fa-check"></i><b>12.3.1</b> kNN</a></li>
<li class="chapter" data-level="12.3.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess"><i class="fa fa-check"></i><b>12.3.2</b> loess</a></li>
<li class="chapter" data-level="12.3.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#in-class-exercises"><i class="fa fa-check"></i><b>12.3.3</b> In-Class Exercises</a></li>
<li class="chapter" data-level="12.3.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#hyperparameters-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>12.3.4</b> Hyperparameters and the bias/variance tradeoff</a></li>
<li class="chapter" data-level="12.3.5" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#extensions-to-knn-and-loess"><i class="fa fa-check"></i><b>12.3.5</b> Extensions to kNN and loess</a></li>
<li class="chapter" data-level="12.3.6" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#model-assumptions-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>12.3.6</b> Model assumptions and the bias/variance tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#splines-and-loess-regression"><i class="fa fa-check"></i><b>12.4</b> Splines and Loess Regression</a><ul>
<li class="chapter" data-level="12.4.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess-1"><i class="fa fa-check"></i><b>12.4.1</b> Loess</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html"><i class="fa fa-check"></i><b>13</b> Overfitting: The problem with adding too many parameters</a><ul>
<li class="chapter" data-level="13.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#classification-exercise-do-together"><i class="fa fa-check"></i><b>13.1</b> Classification Exercise: Do Together</a></li>
<li class="chapter" data-level="13.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#training-error-vs.generalization-error"><i class="fa fa-check"></i><b>13.2</b> Training Error vs. Generalization Error</a></li>
<li class="chapter" data-level="13.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-complexity"><i class="fa fa-check"></i><b>13.3</b> Model complexity</a><ul>
<li class="chapter" data-level="13.3.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#activity"><i class="fa fa-check"></i><b>13.3.1</b> Activity</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducible-error"><i class="fa fa-check"></i><b>13.4</b> Reducible Error</a><ul>
<li class="chapter" data-level="13.4.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#what-is-it"><i class="fa fa-check"></i><b>13.4.1</b> What is it?</a></li>
<li class="chapter" data-level="13.4.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#example"><i class="fa fa-check"></i><b>13.4.2</b> Example</a></li>
<li class="chapter" data-level="13.4.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#bias-and-variance"><i class="fa fa-check"></i><b>13.4.3</b> Bias and Variance</a></li>
<li class="chapter" data-level="13.4.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducing-reducible-error"><i class="fa fa-check"></i><b>13.4.4</b> Reducing reducible error</a></li>
<li class="chapter" data-level="13.4.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#error-decomposition"><i class="fa fa-check"></i><b>13.4.5</b> Error decomposition</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection"><i class="fa fa-check"></i><b>13.5</b> Model Selection</a><ul>
<li class="chapter" data-level="13.5.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#exercise-cv"><i class="fa fa-check"></i><b>13.5.1</b> Exercise: CV</a></li>
<li class="chapter" data-level="13.5.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#out-of-sample-error"><i class="fa fa-check"></i><b>13.5.2</b> Out-of-sample Error</a></li>
<li class="chapter" data-level="13.5.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#alternative-measures-of-model-goodness"><i class="fa fa-check"></i><b>13.5.3</b> Alternative measures of model goodness</a></li>
<li class="chapter" data-level="13.5.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-and-model-selection-setup"><i class="fa fa-check"></i><b>13.5.4</b> Feature and model selection: setup</a></li>
<li class="chapter" data-level="13.5.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection-1"><i class="fa fa-check"></i><b>13.5.5</b> Model selection</a></li>
<li class="chapter" data-level="13.5.6" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-predictor-selection"><i class="fa fa-check"></i><b>13.5.6</b> Feature (predictor) selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="describing-relationships.html"><a href="describing-relationships.html"><i class="fa fa-check"></i>Describing Relationships</a></li>
<li class="chapter" data-level="14" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html"><i class="fa fa-check"></i><b>14</b> There’s meaning in parameters</a><ul>
<li class="chapter" data-level="14.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#the-types-of-parametric-assumptions"><i class="fa fa-check"></i><b>14.1</b> The types of parametric assumptions</a><ul>
<li class="chapter" data-level="14.1.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#when-defining-a-model-function."><i class="fa fa-check"></i><b>14.1.1</b> 1. When defining a <strong>model function</strong>.</a></li>
<li class="chapter" data-level="14.1.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#when-defining-probability-distributions."><i class="fa fa-check"></i><b>14.1.2</b> 2. When defining <strong>probability distributions</strong>.</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#the-value-of-making-parametric-assumptions"><i class="fa fa-check"></i><b>14.2</b> The value of making parametric assumptions</a><ul>
<li class="chapter" data-level="14.2.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#value-1-reduced-error"><i class="fa fa-check"></i><b>14.2.1</b> Value #1: Reduced Error</a></li>
<li class="chapter" data-level="14.2.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#value-2-interpretation"><i class="fa fa-check"></i><b>14.2.2</b> Value #2: Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#anova"><i class="fa fa-check"></i><b>14.3</b> ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="the-meaning-of-interaction.html"><a href="the-meaning-of-interaction.html"><i class="fa fa-check"></i><b>15</b> The meaning of interaction</a></li>
<li class="chapter" data-level="16" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html"><i class="fa fa-check"></i><b>16</b> Scales and the restricted range problem</a><ul>
<li class="chapter" data-level="16.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#problems"><i class="fa fa-check"></i><b>16.1</b> Problems</a></li>
<li class="chapter" data-level="16.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solutions"><i class="fa fa-check"></i><b>16.2</b> Solutions</a><ul>
<li class="chapter" data-level="16.2.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-1-transformations"><i class="fa fa-check"></i><b>16.2.1</b> Solution 1: Transformations</a></li>
<li class="chapter" data-level="16.2.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-2-link-functions"><i class="fa fa-check"></i><b>16.2.2</b> Solution 2: Link Functions</a></li>
<li class="chapter" data-level="16.2.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-3-scientifically-backed-functions"><i class="fa fa-check"></i><b>16.2.3</b> Solution 3: Scientifically-backed functions</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#glms-in-r"><i class="fa fa-check"></i><b>16.3</b> GLM’s in R</a><ul>
<li class="chapter" data-level="16.3.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#broomaugment"><i class="fa fa-check"></i><b>16.3.1</b> <code>broom::augment()</code></a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#options-for-logistic-regression"><i class="fa fa-check"></i><b>16.4</b> Options for Logistic Regression</a><ul>
<li class="chapter" data-level="16.4.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#models"><i class="fa fa-check"></i><b>16.4.1</b> Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="improving-estimation-through-distributional-assumptions.html"><a href="improving-estimation-through-distributional-assumptions.html"><i class="fa fa-check"></i><b>17</b> Improving estimation through distributional assumptions</a></li>
<li class="chapter" data-level="18" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html"><i class="fa fa-check"></i><b>18</b> When we only want interpretation on some predictors</a><ul>
<li class="chapter" data-level="18.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability-in-gams"><i class="fa fa-check"></i><b>18.1</b> Non-identifiability in GAMS</a><ul>
<li class="chapter" data-level="18.1.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability"><i class="fa fa-check"></i><b>18.1.1</b> Non-identifiability</a></li>
<li class="chapter" data-level="18.1.2" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#question-1b"><i class="fa fa-check"></i><b>18.1.2</b> Question 1b</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="special-cases.html"><a href="special-cases.html"><i class="fa fa-check"></i>Special cases</a></li>
<li class="chapter" data-level="19" data-path="regression-when-data-are-censored-survival-analysis.html"><a href="regression-when-data-are-censored-survival-analysis.html"><i class="fa fa-check"></i><b>19</b> Regression when data are censored: survival analysis</a></li>
<li class="chapter" data-level="20" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html"><i class="fa fa-check"></i><b>20</b> Regression in the presence of outliers: robust regression</a><ul>
<li class="chapter" data-level="20.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#robust-regression-in-r"><i class="fa fa-check"></i><b>20.1</b> Robust Regression in R</a><ul>
<li class="chapter" data-level="20.1.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#heavy-tailed-regression"><i class="fa fa-check"></i><b>20.1.1</b> Heavy Tailed Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="21" data-path="regression-in-the-presence-of-extremes-extreme-value-regression.html"><a href="regression-in-the-presence-of-extremes-extreme-value-regression.html"><i class="fa fa-check"></i><b>21</b> Regression in the presence of extremes: extreme value regression</a></li>
<li class="chapter" data-level="22" data-path="regression-when-data-are-ordinal.html"><a href="regression-when-data-are-ordinal.html"><i class="fa fa-check"></i><b>22</b> Regression when data are ordinal</a></li>
<li class="chapter" data-level="23" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html"><i class="fa fa-check"></i><b>23</b> Regression when data are missing: multiple imputation</a><ul>
<li class="chapter" data-level="23.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#mean-imputation"><i class="fa fa-check"></i><b>23.1</b> Mean Imputation</a></li>
<li class="chapter" data-level="23.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation"><i class="fa fa-check"></i><b>23.2</b> Multiple Imputation</a><ul>
<li class="chapter" data-level="23.2.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#patterns"><i class="fa fa-check"></i><b>23.2.1</b> Patterns</a></li>
<li class="chapter" data-level="23.2.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation-1"><i class="fa fa-check"></i><b>23.2.2</b> Multiple Imputation</a></li>
<li class="chapter" data-level="23.2.3" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#pooling"><i class="fa fa-check"></i><b>23.2.3</b> Pooling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="24" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html"><i class="fa fa-check"></i><b>24</b> Regression under many groups: mixed effects models</a><ul>
<li class="chapter" data-level="24.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#motivation-for-lme"><i class="fa fa-check"></i><b>24.1</b> Motivation for LME</a><ul>
<li class="chapter" data-level="24.1.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#definition"><i class="fa fa-check"></i><b>24.1.1</b> Definition</a></li>
<li class="chapter" data-level="24.1.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#r-tools-for-fitting"><i class="fa fa-check"></i><b>24.1.2</b> R Tools for Fitting</a></li>
</ul></li>
<li class="chapter" data-level="24.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#mixed-effects-models-in-r-tutorial"><i class="fa fa-check"></i><b>24.2</b> Mixed Effects Models in R: tutorial</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html"><i class="fa fa-check"></i><b>25</b> Regression on an entire distribution: Probabilistic Forecasting</a><ul>
<li class="chapter" data-level="25.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasting-what-it-is"><i class="fa fa-check"></i><b>25.1</b> Probabilistic Forecasting: What it is</a></li>
<li class="chapter" data-level="25.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#review-univariate-distribution-estimates"><i class="fa fa-check"></i><b>25.2</b> Review: Univariate distribution estimates</a><ul>
<li class="chapter" data-level="25.2.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#continuous-response"><i class="fa fa-check"></i><b>25.2.1</b> Continuous response</a></li>
<li class="chapter" data-level="25.2.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discrete-response"><i class="fa fa-check"></i><b>25.2.2</b> Discrete Response</a></li>
</ul></li>
<li class="chapter" data-level="25.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasts-subset-based-learning-methods"><i class="fa fa-check"></i><b>25.3</b> Probabilistic Forecasts: subset-based learning methods</a><ul>
<li class="chapter" data-level="25.3.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#the-techniques"><i class="fa fa-check"></i><b>25.3.1</b> The techniques</a></li>
<li class="chapter" data-level="25.3.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#exercise-1"><i class="fa fa-check"></i><b>25.3.2</b> Exercise</a></li>
<li class="chapter" data-level="25.3.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>25.3.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="25.3.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#evaluating-model-goodness"><i class="fa fa-check"></i><b>25.3.4</b> Evaluating Model Goodness</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discussion-points"><i class="fa fa-check"></i><b>25.4</b> Discussion Points</a></li>
<li class="chapter" data-level="25.5" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#when-are-they-not-useful"><i class="fa fa-check"></i><b>25.5</b> When are they not useful?</a></li>
</ul></li>
<li class="chapter" data-level="26" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html"><i class="fa fa-check"></i><b>26</b> Regression when order matters: time series and spatial analysis</a><ul>
<li class="chapter" data-level="26.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#timeseries-in-base-r"><i class="fa fa-check"></i><b>26.1</b> Timeseries in (base) R</a></li>
<li class="chapter" data-level="26.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#spatial-example"><i class="fa fa-check"></i><b>26.2</b> Spatial Example</a></li>
<li class="chapter" data-level="26.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#a-model-for-river-rock-size"><i class="fa fa-check"></i><b>26.3</b> A Model for River Rock Size</a><ul>
<li class="chapter" data-level="26.3.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#average-rock-size"><i class="fa fa-check"></i><b>26.3.1</b> 1. Average rock size:</a></li>
<li class="chapter" data-level="26.3.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-rock-size"><i class="fa fa-check"></i><b>26.3.2</b> 2. Mean rock size:</a></li>
<li class="chapter" data-level="26.3.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#downstream-fining-curve"><i class="fa fa-check"></i><b>26.3.3</b> 3. Downstream fining curve:</a></li>
</ul></li>
<li class="chapter" data-level="26.4" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objectives"><i class="fa fa-check"></i><b>26.4</b> Statistical Objectives</a><ul>
<li class="chapter" data-level="26.4.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#preliminaries-variance-and-correlation"><i class="fa fa-check"></i><b>26.4.1</b> Preliminaries: Variance and Correlation</a></li>
</ul></li>
<li class="chapter" data-level="26.5" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#three-concepts"><i class="fa fa-check"></i><b>26.5</b> Three Concepts</a><ul>
<li class="chapter" data-level="26.5.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#error-variance-sigma_e2leftxright"><i class="fa fa-check"></i><b>26.5.1</b> Error Variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span></a></li>
<li class="chapter" data-level="26.5.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-variance-sigma_m2"><i class="fa fa-check"></i><b>26.5.2</b> Mean Variance <span class="math inline">\(\sigma_{M}^{2}\)</span></a></li>
<li class="chapter" data-level="26.5.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-correlation-rholeftdright"><i class="fa fa-check"></i><b>26.5.3</b> Mean Correlation <span class="math inline">\(\rho\left(d\right)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="26.6" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#estimation"><i class="fa fa-check"></i><b>26.6</b> Estimation</a><ul>
<li class="chapter" data-level="26.6.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#constant-error-variance"><i class="fa fa-check"></i><b>26.6.1</b> Constant Error Variance</a></li>
<li class="chapter" data-level="26.6.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#non-constant-error-variance"><i class="fa fa-check"></i><b>26.6.2</b> Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="26.7" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-1-downstream-fining-curve"><i class="fa fa-check"></i><b>26.7</b> Statistical Objective 1: Downstream Fining Curve</a><ul>
<li class="chapter" data-level="26.7.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#regression-form"><i class="fa fa-check"></i><b>26.7.1</b> Regression Form</a></li>
</ul></li>
<li class="chapter" data-level="26.8" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-2-river-profile"><i class="fa fa-check"></i><b>26.8</b> Statistical Objective 2: River Profile</a><ul>
<li class="chapter" data-level="26.8.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#simple-kriging"><i class="fa fa-check"></i><b>26.8.1</b> Simple Kriging</a></li>
<li class="chapter" data-level="26.8.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#universal-kriging"><i class="fa fa-check"></i><b>26.8.2</b> Universal Kriging</a></li>
<li class="chapter" data-level="26.8.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#kriging-under-non-constant-error-variance"><i class="fa fa-check"></i><b>26.8.3</b> Kriging under Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="26.9" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#confidence-intervals-of-the-river-profile"><i class="fa fa-check"></i><b>26.9</b> Confidence Intervals of the River Profile</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/vincenzocoia/Interpreting-Regression" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpreting Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="scales-and-the-restricted-range-problem" class="section level1">
<h1><span class="header-section-number">Chapter 16</span> Scales and the restricted range problem</h1>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<p>link functions and alternative parameter interpretations (categorical data too)</p>
<p>In Regression I, the response was allowed to take on any real number. But what if the range is restricted?</p>
<div id="problems" class="section level2">
<h2><span class="header-section-number">16.1</span> Problems</h2>
<p>Here are some common examples.</p>
<ol style="list-style-type: decimal">
<li>Positive values: river flow.
<ul>
<li>Lower limit: 0</li>
</ul></li>
<li>Percent/proportion data: proportion of income spent on housing in Vancouver.
<ul>
<li>Lower limit: 0</li>
<li>Upper limit: 1.</li>
</ul></li>
<li>Binary data: success/failure data.
<ul>
<li>Only take values of 0 and 1.</li>
</ul></li>
<li>Count data: number of male crabs nearby a nesting female
<ul>
<li>Only take count values (0, 1, 2, …)</li>
</ul></li>
</ol>
<p>Here is an example of the fat content of a cow’s milk, which was recorded over time. Data are from the paper <a href="https://core.ac.uk/download/pdf/79036775.pdf">“Transform or Link?”</a>. Let’s consider data as of week 10:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(plot_cow &lt;-<span class="st"> </span>cow <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">filter</span>(week <span class="op">&gt;=</span><span class="st"> </span><span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(week, fat<span class="op">*</span><span class="dv">100</span>)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">y =</span> <span class="st">&quot;Fat Content (%)&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">ggtitle</span>(<span class="st">&quot;Fat content of cow milk&quot;</span>))</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>Let’s try fitting a linear regression model.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plot_cow <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_smooth</span>(<span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>, <span class="dt">se =</span> <span class="ot">FALSE</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<p>Notice the problem here – <strong>the regression lines extend beyond the possible range of the response</strong>. This is <em>mathematically incorrect</em>, since the expected value cannot extend outside of the range of Y. But what are the <em>practical</em> consequences of this?</p>
<p>In practice, when fitting a linear regression model when the range of the response is restricted, we lose hope for extrapolation, as we obtain logical fallacies if we do. In this example, a cow is expected to produce <em>negative</em> fat content after week 35!</p>
<p>Despite this, a linear regression model might still be useful in these settings. After all, the linear trend looks good for the range of the data.</p>
</div>
<div id="solutions" class="section level2">
<h2><span class="header-section-number">16.2</span> Solutions</h2>
<p>How can we fit a regression curve to stay within the bounds of the data, while still retaining the interpretability that we have with a linear model function? Remember, non-parametric methods like random forests or loess will not give us interpretation. Here are some options:</p>
<ol style="list-style-type: decimal">
<li>Transform the data.</li>
<li>Transform the linear model function: link functions</li>
<li>Use a scientifically-backed parametric function.</li>
</ol>
<div id="solution-1-transformations" class="section level3">
<h3><span class="header-section-number">16.2.1</span> Solution 1: Transformations</h3>
<p>One solution that <em>might</em> be possible is to transform the response so that its range is no longer restricted. The most typical example is for positive data, like river flow. If we log-transform the response, then the new response can be any real number. All we have to do is fit a linear regression model to this transformed data.</p>
<p>One downfall is that we lose interpretability, since we are estimating the mean of <span class="math inline">\(\log(Y)\)</span> (or some other transformation) given the predictors, not <span class="math inline">\(Y\)</span> itself! Transforming the model function by exponentiating will not fix this problem, either, since the exponential of an expectation is not the expectation of an exponential. Though, this is a mathematical technicality, and might still be a decent approximation in practice.</p>
<p>Also, transforming the response might not be fruitful. For example, consider a binary response. No transformation can spread the two values to be non-binary!</p>
</div>
<div id="solution-2-link-functions" class="section level3">
<h3><span class="header-section-number">16.2.2</span> Solution 2: Link Functions</h3>
<p>Instead of transforming the data, why not transform the model function? For example, instead of taking the logarithm of the response, perhaps fit the model <span class="math display">\[ E(Y|X=x) = \exp(\beta_0 + \beta x) = \alpha \exp(\beta x) \]</span>. Or, in general, <span class="math display">\[ g(E(Y|X=x)) = X^T \beta \]</span> for some increasing function <span class="math inline">\(g\)</span> called the <em>link function</em>.</p>
<p>This has the added advantage that we do not need to be able to transform the response.</p>
<p>Two common examples of link functions:</p>
<ul>
<li><span class="math inline">\(\log\)</span>, for positive response values.
<ul>
<li>Parameter interpretation: an increase of one unit in the predictor is associated with an <span class="math inline">\(\exp(\beta)\)</span> times increase in the mean response, where <span class="math inline">\(\beta\)</span> is the slope parameter.</li>
</ul></li>
<li><span class="math inline">\(\text{logit}(x)=\log(x/(1-x))\)</span>, for binary response values.
<ul>
<li>Parameter interpretation: an increase of one unit in the predictor is associated with an <span class="math inline">\(\exp(\beta)\)</span> times increase in the odds of “success”, where <span class="math inline">\(\beta\)</span> is the slope parameter, and odds is the ratio of success to failure probabilities.</li>
</ul></li>
</ul>
</div>
<div id="solution-3-scientifically-backed-functions" class="section level3">
<h3><span class="header-section-number">16.2.3</span> Solution 3: Scientifically-backed functions</h3>
<p>Sometimes there are theoretically derived formulas for the relationship between response and predictors, which have parameters that carry some meaning to them.</p>
</div>
</div>
<div id="glms-in-r" class="section level2">
<h2><span class="header-section-number">16.3</span> GLM’s in R</h2>
<p>This document introduces the <code>glm()</code> function in R for fitting a Generlized Linear Model (GLM). We’ll work with the <code>titanic_train</code> dataset in the <code>titanic</code> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(titanic)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    714 obs. of  12 variables:
##  $ PassengerId: int  1 2 3 4 5 7 8 9 10 11 ...
##  $ Survived   : int  0 1 1 1 0 0 0 1 1 1 ...
##  $ Pclass     : int  3 1 3 1 3 1 3 3 2 3 ...
##  $ Name       : chr  &quot;Braund, Mr. Owen Harris&quot; &quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot; &quot;Heikkinen, Miss. Laina&quot; &quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot; ...
##  $ Sex        : chr  &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ...
##  $ Age        : num  22 38 26 35 35 54 2 27 14 4 ...
##  $ SibSp      : int  1 1 0 1 0 0 3 0 1 1 ...
##  $ Parch      : int  0 0 0 0 0 0 1 2 0 1 ...
##  $ Ticket     : chr  &quot;A/5 21171&quot; &quot;PC 17599&quot; &quot;STON/O2. 3101282&quot; &quot;113803&quot; ...
##  $ Fare       : num  7.25 71.28 7.92 53.1 8.05 ...
##  $ Cabin      : chr  &quot;&quot; &quot;C85&quot; &quot;&quot; &quot;C123&quot; ...
##  $ Embarked   : chr  &quot;S&quot; &quot;C&quot; &quot;S&quot; &quot;S&quot; ...
##  - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int  6 18 20 27 29 30 32 33 37 43 ...
##   ..- attr(*, &quot;names&quot;)= chr  &quot;6&quot; &quot;18&quot; &quot;20&quot; &quot;27&quot; ...</code></pre>
<p>Consider the regression of <code>Survived</code> on <code>Age</code>. Let’s take a look at the data with jitter:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(titanic, <span class="kw">aes</span>(Age, Survived)) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_jitter</span>(<span class="dt">height=</span><span class="fl">0.1</span>, <span class="dt">alpha=</span><span class="fl">0.25</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">scale_y_continuous</span>(<span class="dt">breaks=</span><span class="dv">0</span><span class="op">:</span><span class="dv">1</span>, <span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&quot;Perished&quot;</span>, <span class="st">&quot;Survived&quot;</span>)) <span class="op">+</span>
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-49-1.png" width="672" /></p>
<p>Recall that the linear regression can be done with the <code>lm</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res_lm &lt;-<span class="st"> </span><span class="kw">lm</span>(Survived <span class="op">~</span><span class="st"> </span>Age, <span class="dt">data=</span>titanic)
<span class="kw">summary</span>(res_lm)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Survived ~ Age, data = titanic)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.4811 -0.4158 -0.3662  0.5789  0.7252 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.483753   0.041788  11.576   &lt;2e-16 ***
## Age         -0.002613   0.001264  -2.067   0.0391 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4903 on 712 degrees of freedom
## Multiple R-squared:  0.005963,   Adjusted R-squared:  0.004567 
## F-statistic: 4.271 on 1 and 712 DF,  p-value: 0.03912</code></pre>
<p>In this case, the regression line is <code>0.4837526</code> + <code>-0.0026125</code> <code>Age</code>.</p>
<p>A GLM can be fit in a similar way, using the <code>glm</code> function – we just need to indicate what type of regression we’re doing (binomial? poission?) and the link function. We are doing bernoulli (binomial) regression, since the response is binary (0 or 1); lets choose a <code>probit</code> link function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res_glm &lt;-<span class="st"> </span><span class="kw">glm</span>(<span class="kw">factor</span>(Survived) <span class="op">~</span><span class="st"> </span>Age, <span class="dt">data=</span>titanic, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;probit&quot;</span>))</code></pre></div>
<p>The <code>family</code> argument takes a <strong>function</strong>, indicating the type of regression. See <code>?family</code> for the various types of regression allowed by <code>glm()</code>.</p>
<p>Let’s see a summary of the GLM regression:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(res_glm)</code></pre></div>
<pre><code>## 
## Call:
## glm(formula = factor(Survived) ~ Age, family = binomial(link = &quot;probit&quot;), 
##     data = titanic)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1477  -1.0363  -0.9549   1.3158   1.5929  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -0.037333   0.107944  -0.346   0.7295  
## Age         -0.006773   0.003294  -2.056   0.0397 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 964.52  on 713  degrees of freedom
## Residual deviance: 960.25  on 712  degrees of freedom
## AIC: 964.25
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>We can make predictions too, but this is not as straight-forward as in <code>lm()</code> – here are the “predictions” using the <code>predict()</code> generic function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(res_glm)
<span class="kw">qplot</span>(titanic<span class="op">$</span>Age, pred) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&quot;Age&quot;</span>, <span class="dt">y=</span><span class="st">&quot;Default Predictions&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<p>Why the negative predictions? It turns out this is just the linear predictor, <code>-0.0373331</code> + <code>-0.0067733</code> <code>Age</code>.</p>
<p>The documentation for the <code>predict()</code> generic function on <code>glm</code> objects can be found by typing <code>?predict.glm</code>. Notice that the <code>predict()</code> generic function allows you to specify the <em>type</em> of predictions to be made. To make predictions on the mean (probability of <code>Survived=1</code>), indicate <code>type=&quot;response&quot;</code>, which is the equivalent of applying the inverse link function to the linear predictor.</p>
<p>Here are those predictions again, this time indicating <code>type=&quot;response&quot;</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred &lt;-<span class="st"> </span><span class="kw">predict</span>(res_glm, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)
<span class="kw">qplot</span>(titanic<span class="op">$</span>Age, pred) <span class="op">+</span><span class="st"> </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&quot;Age&quot;</span>, <span class="dt">y=</span><span class="st">&quot;Mean Estimates&quot;</span>)</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-54-1.png" width="672" /></p>
<p>Look closely – these predictions don’t actually fall on a straight line. They follow an inverse probit function (i.e., a Gaussian cdf):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mu &lt;-<span class="st"> </span><span class="cf">function</span>(x) <span class="kw">pnorm</span>(res_glm<span class="op">$</span>coefficients[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>res_glm<span class="op">$</span>coefficients[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x)
<span class="kw">qplot</span>(titanic<span class="op">$</span>Age, pred) <span class="op">+</span><span class="st"> </span>
<span class="st">    </span><span class="kw">labs</span>(<span class="dt">x=</span><span class="st">&quot;Age&quot;</span>, <span class="dt">y=</span><span class="st">&quot;Mean Estimates&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun=</span>mu, <span class="dt">colour=</span><span class="st">&quot;blue&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">limits=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">200</span>, <span class="dv">200</span>))</code></pre></div>
<p><img src="bookdown-demo_files/figure-html/unnamed-chunk-55-1.png" width="672" /></p>
<div id="broomaugment" class="section level3">
<h3><span class="header-section-number">16.3.1</span> <code>broom::augment()</code></h3>
<p>We can use the <code>broom</code> package on <code>glm</code> objects, too. But, just like we had to specify <code>type=&quot;response&quot;</code> when using the <code>predict()</code> function in order to evaluate the model function, so to do we have to specify something in the <code>broom::augment()</code> function. Here, the <code>type.predict</code> argument gets passed to the <code>predict()</code> generic function (actually, the <code>predict.glm()</code> method). This means that indicating <code>type.predict=&quot;response&quot;</code> will evaluate the model function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res_glm <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">augment</span>(<span class="dt">type.predict =</span> <span class="st">&quot;response&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">head</span>()</code></pre></div>
<pre><code>## # A tibble: 6 x 10
##   .rownames factor.Survived.   Age .fitted .se.fit .resid    .hat .sigma
##   &lt;chr&gt;     &lt;fct&gt;            &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1 1         0                   22   0.426  0.0209 -1.05  0.00179   1.16
## 2 2         1                   38   0.384  0.0211  1.38  0.00188   1.16
## 3 3         1                   26   0.415  0.0190  1.33  0.00149   1.16
## 4 4         1                   35   0.392  0.0196  1.37  0.00160   1.16
## 5 5         0                   35   0.392  0.0196 -0.997 0.00160   1.16
## 6 7         0                   54   0.343  0.0345 -0.917 0.00528   1.16
## # … with 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;</code></pre>
</div>
</div>
<div id="options-for-logistic-regression" class="section level2">
<h2><span class="header-section-number">16.4</span> Options for Logistic Regression</h2>
<p>Some popular interpretable quantities (IQ’s) which compare exposure risk <span class="math inline">\(\pi_{E}\)</span> to baseline (unexposed) risk <span class="math inline">\(\pi_{B}\)</span> are</p>
<ol style="list-style-type: decimal">
<li><p>the <em>risk difference</em>, <span class="math inline">\(\pi_{E}-\pi_{B}\)</span>,</p></li>
<li><p>the reciprocal risk difference, or <em>number needed to treat</em> (NNT) (or sometimes <em>number needed to harm</em>),</p></li>
<li><p>the <em>relative risk</em>, <span class="math inline">\(\pi_{E}/\pi_{B}\)</span>, and</p></li>
<li><p>the <em>odds ratio</em>,</p></li>
</ol>
<p>These IQ’s consider all other factors to be equal.</p>
<div id="models" class="section level3">
<h3><span class="header-section-number">16.4.1</span> Models</h3>
<p>A first inclination may be to model the mean as one would in the case of multiple linear regression that is, as a linear combination of the covariates. The link function <span class="math inline">\(g\)</span> is the identity, and the model becomes (EQUATION). Kovalchik and others (2013) refer to this as the “Binomial Linear Model”, or BLM, though it is more commonly known as the “Linear Probability Model”, or LPM (see, for example, Aldrich and Nelson, 1984; Amemiya, 1977; Horrace and Oaxaca, 2006). In this paper, the model is referred to as the LPM.</p>
<p>Before proceeding with any further discussion, the validity of this model must be enforced. The Bernoulli distribution requires <span class="math inline">\(0\leq\pi(X)\leq1\)</span> for all <span class="math inline">\(x\)</span> <span class="math inline">\(\boldsymbol{x}\in XX\)</span> to be a valid distribution. Validity can be ensured by restricting the parameter space of <span class="math inline">\(\left(\beta_{0},\boldsymbol{\beta}\right)\)</span> to . However, the parameter space can be severely restricted depending on the covariate space. For example, if predictor <span class="math inline">\(k\)</span> of is unbounded, then the only allowable value for <span class="math inline">\(\beta_{k}\)</span> is zero. In other words, any covariate in the LPM that has an unbounded range cannot technically be included in the LPM. Further, even if component <span class="math inline">\(k\)</span> is bounded, if it has a large range, then the slope is restricted to be small.</p>
<p>One reason why the LPM is used, despite the above restrictions, is for access to <em>constant interpretable quantities</em> that is, IQ’s discussed in section 1 which do not depend on other covariates. In an LPM, the risk difference by increasing <span class="math inline">\(X_{k}\)</span> by one unit is simply given by <span class="math inline">\(\beta_{k}\)</span>, and the NNT is <span class="math inline">\(1/\beta_{k}\)</span>. However, the relative risk and odds ratio are non-constant, as they are functions of the other covariates.</p>
<p>Since the LPM is just a multiple linear regression model, the regression parameters can be estimated without bias by ordinary least squares (OLS). However, we do not necessarily have homoskedastic errors, since <span class="math inline">\(Var(Y|X=x)\)</span> differs with the covariates. As such, the efficiency of the OLS estimator can be improved by the weighted least squares estimator with weights <span class="math inline">\(1/\sigma\left(\boldsymbol{x}\right)\)</span>. Since these weights are unknown, an iterative algorithm is used, which calculates weights using the fitted probabilities from parameter estimates of the previous step to compute a new “re-weighted” estimator. Iterating this beginning with the OLS estimator converges to the <em>iteratively re-weighted least squares</em> (IRLS) estimator. Amemiya (1977) shows that the IRLS is identical to the maximum likelihood estimator (MLE).</p>
<p>An alternative model which is sometimes confused for the LPM (for example, see Horrace and Oaxaca, 2006) is to allow for an arbitrary parameter space by taking <span class="math inline">\(\pi(x)\)</span> to be zero when <span class="math inline">\(\eta\)</span> is less than zero, and unity otherwise. This model, which I call the “truncated LPM” (TLPM), is (EQUATION) where the inverse-link function <span class="math inline">\(T\)</span> is the ramp function (actually, <span class="math inline">\(T\)</span> is not quite an inverse-link function because it is non-invertible, but this is unimportant). However, as one can see by the differing link function, this is not the LPM, although it is often mistaken for the LPM. Horrace and Oaxaca (2006) mistake the TLPM for the LPM, and in doing so, show that estimation of the model parameters through OLS or IRLS provide biased and inconsistent estimators. This is a good reason why the TLPM should not be used unless a different method of estimation is considered.</p>
<p>To rid the parameter space of restrictions, one may consider link functions similar to the ramp function (preferably smooth) to ensure <span class="math inline">\(0\leq\pi(x)\leq1\)</span>. Popular choices are logit, probit, the inverse Gumbel distribution function, or the angular function (Cox and Snell, 1989). Each of these link functions ensures a valid probability for an arbitrary parameter space. The logit link function is a popular choice because it has the best interpretability. It models the log-odds as a linear function of the covariates that is, (EQUATION). This model is known as the logistic regression model, and can be written equivalently as , where is the inverse logit function. The logistic model stands out over models with other link functions because a constant IQ can be obtained from it the odds ratio by increasing <span class="math inline">\(X_{k}\)</span> by one unit is simply <span class="math inline">\(\exp\left(\beta_{k}\right)\)</span>. However, of the interpretable quantities discussed in Section 1, the odds ratio is the most difficult to interpret. Though, if both risks are smallthe “rare disease assumption” with risks under <span class="math inline">\(0.1\)</span> then the odds ratio is a good approximation to the relative risk, which is easier to interpret. The lack of an easy interpretable constant IQ is why some researchers will opt for the LPM instead of the logistic model when the rare disease assumption is invalid. Indeed, this is one major reason behind the study done by Kovalchik and others (2013). One other method to decide is through Goodness of Fit criteria, which was the other deciding factor of Kovalchik and others.</p>
<p>Conveniently, the log odds appears in the likelihood of the logistic model, which simplifies some computations. This leads to the MLE which solves the equation . However, occasionally it is possible that no MLE exists when there is a <span class="math inline">\(\left(b_{0},\boldsymbol{b}\right)\in\mathcal{F}\)</span> such that <span class="math inline">\(b_{0}+\boldsymbol{x}_{i}^{T}\boldsymbol{b}&gt;0\)</span> has <span class="math inline">\(Y_{i}=1\)</span> and <span class="math inline">\(b_{0}+\boldsymbol{x}_{i}^{T}\boldsymbol{b}&lt;0\)</span> has <span class="math inline">\(Y_{i}=0\)</span> for each <span class="math inline">\(i=1,\ldots,n\)</span> (Albert and Anderson, 1984). This is called the case of “complete separation”, and the likelihood has no maximum, so a “perfect fit” is made by infinitely pushing the covariate data to the tails of the expit curve. This is not an issue with the LPM model.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="the-meaning-of-interaction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="improving-estimation-through-distributional-assumptions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/vincenzocoia/Interpreting-Regression/edit/master/130-Scales_and_the_restricted_range_problem.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["bookdown-demo.pdf", "bookdown-demo.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
