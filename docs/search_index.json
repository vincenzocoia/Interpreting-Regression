[
["index.html", "Interpreting Regression Chapter 1 Preamble 1.1 Caution 1.2 Purpose of the book 1.3 Tasks that motivate Regression 1.4 Examples", " Interpreting Regression Vincenzo Coia 2019-05-28 Chapter 1 Preamble 1.1 Caution This book is in its very preliminary stages. Content will be moving around and updated. Currently, much of the book is a stitching together of previous pieces of my writing that I think might be relevant to this book. These chapters will be updated to consider the different context, audience, and content organization that’s best for this book. 1.2 Purpose of the book There’s a vast and powerful statistical framework out there. This book takes a modularized approach to making this framework accessible, so that as a problem solver, the reader can make a sequences of decisions to build models that are best suited to address the problem. Regression analysis can help solve two main types of problems: Interpreting the relationship between variables. Predicting a new outcome. This book primarily focusses on models for interpretation, and often references these two competing needs. The prediction problem is still discussed to some extent: once a model suited for interpretation is developed, it is still important to be able to use it to make predictions. For those interested primarily in prediction, check out resources on supervised learning, which aims to optimize predictions. There is another layer to interpretation that this book adopts, in terms of describing and understanding the motivation and inner workings behind each method. This is in contrast to a purely mathematical presentation of statistical methods. This book presents both an interpretation of a the high-level idea behind amethod, as well as a mathematical presentation to make these concepts precise. For example, the Kaplan-Meier estimate of the survival function is explained both intuitively and mathematically. Most statistical methods are built on a foundation of assumptions imposed on the data. But an assumption is almost never exactly true; so we instead explore consequences based on “how close” an assumption is to being true. In some cases, we even find that an anticipated assumption is not required at all, depending on how we would like to interpret the model – an example being the “requirement” of linear data in linear regression. Consequenty, one aim of this book is to discourage the thinking that a method either “can” or “cannot” be applied, instead thinking about pros and cons of various methods. Methods are demonstrated using the R statistical software. This is because R has an extensive selection of packages available for statistical analysis, and the tidyverse and tidymodels meta-packages make data analysis readable and organized. Since this book does not focus on optimally flexing a model function to conform to data (non-parametric supervised learning), languages better suited for this task, such as python, are not considered. The audience of this book is quite wide, with the aspects of modularization, interpretation, and non-binary view of assumptions perhaps appealing to various readers: Practitioners may find the modularization useful when making decisions when fitting models, the non-binary view of assumptions useful when evaluating the trustworthiness of their models, and the interpretation of their model useful when communicating their results. Experts in the field of Statistics might benefit from the unique modularized framework of the field, as they find well-used notions such as the expected value being challenged and expanded upon. Learners may find the modularization useful for compartmentalizing concepts, and the interpretation of methods useful for understanding concepts. 1.3 Tasks that motivate Regression Real world problems for which regression is an appropriate tool generally fall into two categories: Prediction: Predicting the response of a new unit/individual, sometimes also describing uncertainty in the prediction. Interpretation: Interpreting how predictors influence the response. For example, consider a person undergoing artificial insemination. Prediction: Given the person’s age, what is the chance of pregnancy? Interpretation: How does age influence the chance of pregnancy? How does time of insemination after a spike in Luteinizing hormone affect the chance of pregnancy, and how is this different for people over 40? This book does not focus on optimizing predictions, but focusses on the other tasks. This means: describing the uncertainty in predictions, or estimates in general, and interpreting how predictors influence the response. Why not focus on optimizing predictions? This is the objective of supervised learning, an entire discipline in itself. The scope of this book would just be too big to include this, too. 1.4 Examples library(tidyverse) ## ── Attaching packages ──────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.1 ✔ dplyr 0.8.0.1 ## ✔ tidyr 0.8.3 ✔ stringr 1.4.0 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;tidyr&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 ## ── Conflicts ─────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() suppressPackageStartupMessages(library(Lahman)) baseball &lt;- Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. "],
["an-outcome-on-its-own.html", "An outcome on its own", " An outcome on its own How can we get a handle on an outcome that seems random? Although the score of a Canucks game, a stock price, or river flow is uncertain, this does not mean that these quantities are futile to predict or describe. This part of the book describes how to do just that, using only observations on a single outcome, by shedding light on concepts of probability and univariate analysis as they apply to data science. "],
["distributions-uncertainty-is-worth-explaining.html", "Chapter 2 Distributions: Uncertainty is worth explaining", " Chapter 2 Distributions: Uncertainty is worth explaining "],
["explaining-an-uncertain-outcome-interpretable-quantities.html", "Chapter 3 Explaining an uncertain outcome: interpretable quantities 3.1 Probabilistic Quantities 3.2 What is the mean, anyway? 3.3 Quantiles", " Chapter 3 Explaining an uncertain outcome: interpretable quantities Caution: in a highly developmental stage! See Section 1.1. 3.1 Probabilistic Quantities Sometimes confusingly called “parameters”. Explain the quantities by their interpretation/usefulness, using examples. Mean: what number would you want if you have 10 weeks worth of data, and you want to estimate the total after 52 weeks (one year)? Median: Mode: High Quantile: Low Quantile: Extreme quantile? When you want information about an unknown quantity, it’s up to you what you decide to use. The mean is the most commonly sought when the unknown is numeric. I suspect this is the case for two main reasons: It simplifies computations. It’s what’s taught in school. 3.2 What is the mean, anyway? Imagine trying to predict your total expenses for the next two years. You have monthly expenses listed for the past 12 months. What’s one simple way of making your prediction? Calculate the average expense from the past 12 months, and multiply that by 24. In general, a mean (or expected value) can be interpreted as the long-run average. However, the mean tends to be interpreted as a measure of central tendency, which has a more nebulous interpretation as a “typical” outcome, or an outcome for which most of the data will be “nearest”. 3.3 Quantiles It’s common to “default” to using the mean to make decisions. But, the mean is not always appropriate (I wrote a blog post about this): Sometimes it makes sense to relate the outcome to a coin toss. For example, find an amount for which next month’s expenditures will either exceed or be under with a 50% chance. Sometimes a conservative/liberal estimate is wanted. For example, a bus company wants conservative estimates so that most busses fall within the estimated travel time. In these cases, we care about quantiles, not the mean. Estimating them is called quantile regression (as opposed to mean regression). Recall what quantiles are: the \\(\\tau\\)-quantile (for \\(\\tau\\) between 0 and 1) is the number that will be exceeded by the outcome with a \\((1-\\tau)\\) chance. In other words, there is a probability of \\(\\tau\\) that the outcome will be below the \\(\\tau\\)-quantile. \\(\\tau\\) is referred to as the quantile level, or sometimes the quantile index. For example, a bus company might want to predict the 0.8-quantile of transit time – 80% of busses will get to their destination within that time. "],
["data-versions-of-interpretable-quantities.html", "Chapter 4 Data versions of interpretable quantities 4.1 Estimation of Probabilistic Quantities", " Chapter 4 Data versions of interpretable quantities Caution: in a highly developmental stage! See Section 1.1. 4.1 Estimation of Probabilistic Quantities What makes a formula an estimator? It should go to the actual value as the sample size increases. It’s important to note that there are many ways to estimate a probabilistic quantity. Some of these will be better than others, often depending on the situation. Typically, the bias-variance tradeoff is at play here, where some estimators sacrifice bias to reduce variance, or vice versa. "],
["sampling-distributions-another-layer-of-uncertainty-added-from-estimation.html", "Chapter 5 Sampling distributions: Another layer of uncertainty added from estimation", " Chapter 5 Sampling distributions: Another layer of uncertainty added from estimation "],
["improving-estimator-quality-by-parametric-distributional-assumptions-and-mle.html", "Chapter 6 Improving estimator quality by parametric distributional assumptions and MLE", " Chapter 6 Improving estimator quality by parametric distributional assumptions and MLE "],
["prediction-harnessing-the-signal.html", "Prediction: harnessing the signal", " Prediction: harnessing the signal "],
["reducing-uncertainty-of-the-outcome-including-predictors.html", "Chapter 7 Reducing uncertainty of the outcome: including predictors", " Chapter 7 Reducing uncertainty of the outcome: including predictors "],
["the-signal-model-functions.html", "Chapter 8 The signal: model functions 8.1 Linear Quantile Regression", " Chapter 8 The signal: model functions 8.1 Linear Quantile Regression suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(Lahman)) The idea here is to model \\[Q(\\tau)=\\beta_0(\\tau) + \\beta_1(\\tau) X_1 + \\cdots + \\beta_p(\\tau) X_p,\\] where \\(Q(\\tau)\\) is the \\(\\tau\\)-quantile. In other words, each quantile level gets its own line, and are each fit independently of each other. Here are the 0.25-, 0.5-, and 0.75-quantile regression lines for the baseball data: ggplot(baseball, aes(hits, runs)) + geom_point(alpha=0.1, colour=&quot;orange&quot;) + geom_quantile(colour=&quot;black&quot;) + theme_bw() + labs(x=&quot;Number of Hits (X)&quot;, y=&quot;Number of Runs (Y)&quot;) ## Loading required package: SparseM ## ## Attaching package: &#39;SparseM&#39; ## The following object is masked from &#39;package:base&#39;: ## ## backsolve ## Smoothing formula not specified. Using: y ~ x I did this easily with ggplot2, just by adding a layer geom_quantile to my scatterplot, specifying the quantile levels with the quantiles= argument. We could also use the function rq in the quantreg package in R: (fit_rq &lt;- rq(runs ~ hits, data=baseball, tau=c(0.25, 0.5, 0.75))) ## Call: ## rq(formula = runs ~ hits, tau = c(0.25, 0.5, 0.75), data = baseball) ## ## Coefficients: ## tau= 0.25 tau= 0.50 tau= 0.75 ## (Intercept) -118.8297872 8.2101818 64.0347349 ## hits 0.5531915 0.4923636 0.4908592 ## ## Degrees of freedom: 2835 total; 2833 residual If we were to again focus on the two teams (one with 1000 hits, and one with 1500 hits), we have (by evaluating the above three lines): predict(fit_rq, newdata=data.frame(hits=c(1000, 1500))) ## tau= 0.25 tau= 0.50 tau= 0.75 ## 1 434.3617 500.5738 554.8940 ## 2 710.9574 746.7556 800.3236 So, we could say that the team with 1000 hits: is estimated to have a 50% chance to have between 434 and 555 runs; has a 25% chance of achieving over 555 runs; has a 25% chance of getting less than 434 runs; would typically get 501 runs (median); amongst other things. 8.1.1 Exercise Get a 95% prediction interval using linear quantile regression, with Y=R (number of runs), X=H (number of hits), when X=1500. What about a 95% PI using kNN, going back to the earlier example we did? 8.1.2 Problem: Crossing quantiles Because each quantile is allowed to have its own line, some of these lines might cross, giving an invalid result. Here is an example with the iris data set, fitting the 0.2- and 0.3-quantiles: ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point(alpha=0.25, colour=&quot;orange&quot;) + geom_quantile(aes(colour=&quot;0.2&quot;), quantiles=0.2) + geom_quantile(aes(colour=&quot;0.3&quot;), quantiles=0.3) + scale_colour_discrete(&quot;Quantile\\nLevel&quot;) + theme_bw() + labs(x=&quot;Sepal Length&quot;, y=&quot;Sepal Width&quot;) ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x fit_iris &lt;- rq(Sepal.Width ~ Sepal.Length, data=iris, tau=2:3/10) b &lt;- coef(fit_iris) at8 &lt;- round(predict(fit_iris, newdata=data.frame(Sepal.Length=8)), 2) Quantile estimates of Sepal Width for plants with Sepal Length less than 7.3 are valid, but otherwise, are not. For example, for plants with a Sepal Length of 8, this model predicts 30% of such plants to have a Sepal Width of less than 2.75, but only 20% of such plants should have Sepal Width less than 2.82. This is an illogical statement. There have been several “adjustments” proposed to ensure that this doesn’t happen (see below), but ultimately, this suggests an inadequacy in the model assumptions. Luckily, this usually only happens at extreme values of the predictor space, and/or for large quantile levels, so is usually not a problem. Bondell HD, Reich BJ, Wang H. Noncrossing quantile regression curve estimation. Biometrika. 2010;97(4):825-838. Dette H, Volgushev S. Non-crossing non-parametric estimates of quantile curves. J R Stat Soc Ser B Stat Methodol. 2008;70(3):609-627. Tokdar ST, Kadane JB. Simultaneous linear quantile regression: a semiparametric Bayesian approach. Bayesian Anal. 2011;6(4):1-22. 8.1.3 Problem: Upper quantiles Estimates of higher quantiles usually become worse for large/small values of \\(\\tau\\). This is especially true when data are heavy-tailed. Check out the Chapter on Extreme Value Regression for more info. "],
["estimating-parametric-model-functions.html", "Chapter 9 Estimating parametric model functions 9.1 Writing the sample mean as an optimization problem 9.2 Evaluating Model Goodness: Quantiles", " Chapter 9 Estimating parametric model functions Caution: in a highly developmental stage! See Section 1.1. 9.1 Writing the sample mean as an optimization problem (DSCI 561 lab2, 2018-2019) It’s important to know that the sample mean can also be calculated by finding the value that minimizes the sum of squared errors with respect to that value. We’ll explore that here. Store some numbers in the vector y. Calculate the sample mean of the data, stored in mu_y. This is not worth any marks, but having it as its own question jibes better with the autograder. We’ve defined sse() below, a function that takes some number and returns the sum of squared “errors” of all values of y with respect to the inputted number. An “error” is defined as the difference between two values. We’ve also generated a quick plot of this function for you. sse &lt;- Vectorize(function(m) sum((y - m)^2)) curve(sse, mu_y - 2*sd(y), mu_y + 2*sd(y)) Your task: use the optimize() function to find the value that minimizes the sum of squared errors. Hint: for the interval argument, specify an interval that contains the sample mean. Important points: You should recognize that the sample mean minimizes this function! You’ll be seeing the sum of squared errors a lot through the program (mean squared error and R^2 are based on it). This is because the mean is a very popular quantity to model and use as a prediction. If you’re not convinced, play with different numbers to see for yourself. 9.2 Evaluating Model Goodness: Quantiles The question here is: if we have two or more models that predicts the \\(\\tau\\)-quantile, which model is best? We’ll need some way to score different models to do things such as: Choose which predictors to include in a model; Choose optimal hyperparameters; Estimate parameters in a quantile regression model. **NOTE**: Mean Squared Error is not appropriate here!! This is very important to remember. The reason is technical – the MSE is not a proper scoring rule for quantiles. In other words, the MSE does not elicit an honest prediction. If we’re predicting the median, then the mean absolute error works. This is like the MSE, but instead of squaring the errors, we take the absolute value. In general, a “correct” scoring rule for the \\(\\tau\\)-quantile is as follows: \\[ S = \\sum_{i=1}^{n} \\rho_{\\tau}(Y_i - \\hat{Q}_i(\\tau)), \\] where \\(Y_i\\) for \\(i=1,\\ldots,n\\) is the response data, \\(\\hat{Q}_i(\\tau)\\) are the \\(\\tau\\)-quantile estimates, and \\(\\rho_{\\tau}\\) is the check function (also known as the absolute asymmetric deviation function or tick function), given by \\[ \\rho_{\\tau}(s) = (\\tau - I(s&lt;0))s \\] for real \\(s\\). This scoring rule is negatively oriented, meaning the lower the score, the better. It cannot be below 0. Here is a plot of various check functions. Notice that, when \\(\\tau=0.5\\) (corresponding to the median), this is proportional to the absolute value: base &lt;- ggplot(data.frame(x=c(-2,2)), aes(x)) + theme_bw() + labs(y=expression(rho)) + theme(axis.title.y=element_text(angle=0, vjust=0.5)) + ylim(c(0, 1.5)) rho &lt;- function(tau) function(x) (tau - (x&lt;0))*x cowplot::plot_grid( base + stat_function(fun=rho(0.2)) + ggtitle(expression(paste(tau, &quot;=0.2&quot;))), base + stat_function(fun=rho(0.5)) + ggtitle(expression(paste(tau, &quot;=0.5&quot;))), base + stat_function(fun=rho(0.8)) + ggtitle(expression(paste(tau, &quot;=0.8&quot;))), ncol=3 ) ## Warning: Removed 4 rows containing missing values (geom_path). ## Warning: Removed 4 rows containing missing values (geom_path). For quantile regression estimation, we minimize the sum of scores instead of the sum of squared residuals, as in the usual (mean) linear regression. "],
["estimating-assumption-free-the-world-of-supervised-learning-techniques.html", "Chapter 10 Estimating assumption-free: the world of supervised learning techniques", " Chapter 10 Estimating assumption-free: the world of supervised learning techniques "],
["overfitting-the-problem-with-adding-too-many-parameters.html", "Chapter 11 Overfitting: The problem with adding too many parameters", " Chapter 11 Overfitting: The problem with adding too many parameters "],
["describing-relationships.html", "Describing Relationships", " Describing Relationships "],
["theres-meaning-in-parameters.html", "Chapter 12 There’s meaning in parameters", " Chapter 12 There’s meaning in parameters "],
["the-meaning-of-interaction.html", "Chapter 13 The meaning of interaction", " Chapter 13 The meaning of interaction Interaction terms, and when relationships change given a variable. "],
["scales-and-the-restricted-range-problem.html", "Chapter 14 Scales and the restricted range problem", " Chapter 14 Scales and the restricted range problem link functions and alternative parameter interpretations (categorical data too) "],
["improving-estimation-through-distributional-assumptions.html", "Chapter 15 Improving estimation through distributional assumptions", " Chapter 15 Improving estimation through distributional assumptions "],
["when-we-only-want-interpretation-on-some-predictors.html", "Chapter 16 When we only want interpretation on some predictors", " Chapter 16 When we only want interpretation on some predictors "],
["special-cases.html", "Special cases", " Special cases "],
["regression-when-data-are-censored-survival-analysis.html", "Chapter 17 Regression when data are censored: survival analysis", " Chapter 17 Regression when data are censored: survival analysis "],
["regression-in-the-presence-of-outliers-robust-regression.html", "Chapter 18 Regression in the presence of outliers: robust regression", " Chapter 18 Regression in the presence of outliers: robust regression "],
["regression-in-the-presence-of-extremes-extreme-value-regression.html", "Chapter 19 Regression in the presence of extremes: extreme value regression", " Chapter 19 Regression in the presence of extremes: extreme value regression Caution: in a highly developmental stage! See Section 1.1. The problem with estimating extreme quantiles in the “usual” sense: Here is a histogram of 100 observations generated from a Student’s t(1) distribution (it’s heavy-tailed): set.seed(4) y &lt;- rt(100, df=1) qplot(y) + theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Here are estimates of high and low quantiles, compared to the actual. You can see the discrepency grows quickly. Extreme-low quantiles are too high, whereas extreme-high quantiles are too low. As a rule of thumb, it’s best to stay below \\(\\tau=0.95\\) or above \\(\\tau=0.05\\). If you really want estimates of these extreme quantiles, you’ll need to turn to Extreme Value Theory to make an assumption on the tail of the distribution of the data. One common approach is to fit a generalized Pareto distribution to the upper portion of the data, after which you can extract high quantiles. "],
["regression-when-data-are-ordinal.html", "Chapter 20 Regression when data are ordinal", " Chapter 20 Regression when data are ordinal "],
["regression-when-data-are-missing-multiple-imputation.html", "Chapter 21 Regression when data are missing: multiple imputation", " Chapter 21 Regression when data are missing: multiple imputation "],
["regression-under-many-groups-mixed-effects-models.html", "Chapter 22 Regression under many groups: mixed effects models", " Chapter 22 Regression under many groups: mixed effects models "],
["regression-on-an-entire-distribution-probabilistic-forecasting.html", "Chapter 23 Regression on an entire distribution: Probabilistic Forecasting 23.1 Probabilistic Forecasting: What it is 23.2 Review: Univariate distribution estimates 23.3 Probabilistic Forecasts: subset-based learning methods 23.4 Discussion Points 23.5 When are they not useful?", " Chapter 23 Regression on an entire distribution: Probabilistic Forecasting Caution: in a highly developmental stage! See Section 1.1. suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(Lahman)) my_accent &lt;- &quot;#d95f02&quot; Up until now, we’ve only seen different ways of using a predictor to give us more information the mean and mode of the response. The world holds a huge emphasis on the mean and mode, but these are not always what’s important. Two alternatives are: Probabilistic forecasting Quantile Regression (numeric response only) 23.1 Probabilistic Forecasting: What it is The idea here is to put forth an entire probability distribution as a prediction. Let’s look at an example. Suppose there are two baseball teams, one that gets 1000 total hits in a year, and another that gets 1500. Using “total hits in a year” as a predictor, we set out to predict the total number of runs of both teams. Here’s the top snippet of the data: dat &lt;- Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) Let’s not concern ourselves with the methods yet. Using a standard regression technique, here are our predictions: r &lt;- 20 datsub &lt;- filter(dat, (hits&gt;=1000-r &amp; hits&lt;=1000+r) | (hits&gt;=1500-r &amp; hits&lt;=1500+r)) %&gt;% mutate(approx_hits = if_else(hits&gt;=1000-r &amp; hits&lt;=1000+r, 1000, 1500)) datsub %&gt;% group_by(approx_hits) %&gt;% summarize(expected_runs=round(mean(runs))) %&gt;% select(hits=approx_hits, expected_runs) ## # A tibble: 2 x 2 ## hits expected_runs ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1000 558 ## 2 1500 768 Using a probabilistic forecast, here are our predictions: Don’t you think this is far more informative than the mean estimates in the above table? The probabilistic forecast/prediction contains the most amount of information about the response as possible (based on a set of predictors), because it communicates the entire belief of what \\(Y\\) values are most plausible, given values of the predictor. Predictions/forecasts here are called predictive distributions. From @gneiting_raftery: Indeed, over the past two decades, probabilistic forecasting has become routine in such applications as weather and climate prediction (Palmer 2002; Gneiting and Raftery 2005), computational finance (Duffle and Pan 1997), and macroeconomic forecasting (Garratt, Lee, Pesaran, and Shin 2003; Granger 2006). 23.2 Review: Univariate distribution estimates Let’s review how to estimate a univariate probability density function or probability mass function. 23.2.1 Continuous response Here’s a random sample of 10 continuous variables, ordered from smallest to largest, stored in the variable x: Recall that we can use histograms to estimate the density of the data. The idea is: Cut the range of the data into “bins” of a certain width. For these data, the range is 40. Let’s set up four bins of width 10: -19.8 to -9.8, -9.8 to 0.2, etc. Count the number of observations that fall into each bin. For our setup, the number of observations falling into the four bins, in order, are: 3,2,2,3. Make a bar plot (with no space between the bars), where the bar width corresponds to the bins, and the bar height corresponds to the number of observations in that bin. For our setup, we have: ggplot(data.frame(x=x), aes(x)) + geom_histogram(binwidth=10, center=min(x)+5, fill=my_accent, colour=&quot;black&quot;) + theme_bw() (Note: this is not a true density, since the area under the curve is not 1, but the shape is what matters) You’d have to play with the binwidth to get a histogram that looks about right (not too jagged, not too coarse). For the above example, there are too few data to make a good estimate. Let’s now generate 1000 observations, and make a histogram using qplot from R’s ggplot2 package, with a variety of binwidths – too small, too large, and just right. x &lt;- rnorm(1000, sd=10) qplot(x, binwidth=1) # too small qplot(x, binwidth=10) # too big qplot(x, binwidth=3.5) # just right Advanced method: There’s a technique called the kernel density estimate that works as an alernative to the histogram. The idea is to put a “little mound” (a kernel) on top of each observation, and add them up. Instead of playing with the binwidth, you can play with the “bandwidth” of the kernels. Use geom=&quot;density&quot; in qplot, and use bw to play with the bandwidth: qplot(x, geom=&quot;density&quot;, bw=2.5) 23.2.2 Discrete Response When the response is discrete (this includes categorical), the approach is simpler: Calculate the proportion of observations that fall into each category. Make a bar chart, placing a bar over each category, and using the proportions as the bar heights. Here are ten observations, stored in x: x ## [1] 1 0 0 0 2 0 1 2 3 0 The proportions are as follows: props &lt;- tibble(Value=x) %&gt;% group_by(Value) %&gt;% summarize(Proportion=length(Value)/length(x)) You can plot these proportions with qplot, specifying geom=&quot;col&quot;: qplot(x=Value, y=Proportion, data=props, geom=&quot;col&quot;) You can use ggplot2 to calculate the proportions, but it’s more complex. It’s easier to plot the raw counts, instead of proportions – and that’s fine, you’ll still get the same shape. Using qplot again, let’s make a plot for 1000 observations (note that I indicate that my data are discrete by using the factor function): set.seed(2) x &lt;- rpois(1000, lambda=1) qplot(factor(x)) Here’s the code to get proportions instead of counts: qplot(factor(x), mapping=aes(y=..prop..), group=1) 23.3 Probabilistic Forecasts: subset-based learning methods 23.3.1 The techniques The local methods and classification/regression trees that we’ve seen so far can be used to produce probabilistic forecasts. For local methods, let’s ignore the complications of kernel weighting and local polynomials. These methods result in a subset of the data, for which we’re used to taking the mean or mode. Instead, use the subsetted data to plot a distribution. For kNN, form a histogram/density plot/bar plot using the \\(k\\) nearest neighbours. For the moving window (loess), form a histogram/density plot/bar plot using the observations that fall in the window. For tree-based methods, use the observations within a leaf to form a histogram/density plot/bar plot for that leaf. The above baseball example used a moving window with a radius of 20 hits. Visually, you can see the data that I subsetted within these two narrow windows, for hits of 1000 and 1500: ggplot(dat, aes(hits, runs)) + geom_point(colour=my_accent, alpha=0.1) + geom_vline(xintercept=c(1000+c(-r,r), 1500+c(-r,r)), linetype=&quot;dashed&quot;) + theme_bw() + labs(x=&quot;Number of Hits (X)&quot;, y=&quot;Number of Runs (Y)&quot;) 23.3.2 Exercise Install the Lahman package, which contains the Teams dataset. Build a null model probabilistic forecast of “number of runs” (R column). Build a probabilistic forecast, using kNN, of “number of runs” for a team that has 1500 hits (H column) and 70 wins (W column). Don’t forget to scale the predictors! Do the same thing, but using linear regression. What additional assumption(s) is/are needed here? 23.3.3 Bias-variance tradeoff Let’s examine the bias-variance / overfitting-underfitting tradeoff with kNN-based probabilistic forecasts. I’ll run a simulation like so: Generate data from a bivariate Normal distribution, so that \\(X \\sim N(0, 100)\\), and \\(Y = X + N(0, 100)\\). Training data will contain 500 observations, for which a kNN probabilistic forecast will be built when \\(X=25\\). Try both a small (k=15) and large (k=100) value of \\(k\\). For each value of \\(k\\), we’ll generate 20 training data sets. Here are the 20 estimates for the values of \\(k\\). The overall mean of the distributions are indicated by a vertical dashed line. Notice that: When \\(k\\) is large, our estimates are biased, because the distributions are not centered correctly. But, the estimates are more consistent. When \\(k\\) is small, our estimates are less biased, because the distributions overall have a mean that is close to the true mean. But the variance is high – we get all sorts of distribution shapes here. A similar thing happens with a moving window, with the window width parameter. For tree-based methods, the amount that you partition the predictor space controls the bias-variance tradeoff. 23.3.4 Evaluating Model Goodness To choose a balance between bias and variance, we need a measure of prediction goodness. When predicting the mean, the MSE works. When predicting the mode, the classification error works. But what works for probabilistic forecasts? This is an active area of research. The idea is to use a proper scoring rule – a way of assigning a score based on the forecast distribution and the outcome only, that also encourages honesty. We won’t go into details – see [@gneiting_raftery] for details. At the very least, one should check that the forecast distributions are “calibrated” – that is, the actual outcomes are spread evenly amongst the forecasts. You can check this by applying the forecast cdf to the corresponding outcome – the resulting sample should be Uniform(0,1). Note that this is built-in to at least some proper scoring rules. For this course, we won’t be picky about how you choose your tuning parameters. Just look for a subset that you think has “enough” observations in it so that the distribution starts to take some shape, but not so much that it starts to shift. 23.4 Discussion Points For (1) and (2) below, you’re choosing between two candidates to hire. Discuss the pros and cons of choosing one candidate over the other in the following situations. Both are predicted to have the same productivity score of 75, but have the following probabilistic forecasts. It’s hard to make a decision here. On the one hand, we can be fairly certain that the actual productivity of candidate A will be about 75, but there’s more of a gamble with candidate B. There’s a very real chance that B’s productivity is actually quite a bit higher than A – for example, a productivity of 80 is plausible for B, but not for A. On the other hand, there’s also a very real chance that B’s productivity is actually quite a bit lower than A, for the same reason. Your decision would depend on whether you would want to take a risk or not. On the other hand, in reality, this is only one tool out of many other aspects of the candidate that you would consider. It might be a good idea to chat with B to get a better sense of what their productivity might actually be. Two “non-overlapping” forecasts: In this case, B is very very likely to have higher productivity than A, because all “plausible” productivity values for B are higher than all “plausible” productivity values of A. Again, this is just one tool you might use to make a decision. You’ve formed a probabilistic forecast for a particular value of the predictors, displayed below as a density. You then collect test data for that same value of the predictor, indicated as the points below the density. What is the problem with the probabilistic forecast? The forecast is biased, because the actual values are occuring near the upper tail of the distribution – they should be scattered about the middle, with a higher density of points occuring near 0. If using local methods, we’d have to reduce \\(k\\) or the window width to decrease bias (to remove “further” data that are less relevant); if using a tree-based method, you could grow the tree deeper to lower the bias. 23.5 When are they not useful? Probabilistic forecasts are useful if you’re making a small amount of decisions at a time. For example: Predicting which hockey team will win the Stanley Cup Looking at the 2-day-ahead prediction of river flow every day to decide whether to take flood mitigation measures. But they are not appropriate when making decisions en-masse. For example: A bus company wants to know how long it takes a bus to travel between stops, for all stops and all busses. You want to predict future behaviour of customers. "]
]
