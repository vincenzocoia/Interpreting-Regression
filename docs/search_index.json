[
["index.html", "Interpreting Regression Chapter 1 Preamble 1.1 Caution 1.2 Purpose of the book 1.3 Tasks that motivate Regression", " Interpreting Regression Vincenzo Coia 2019-05-28 Chapter 1 Preamble 1.1 Caution This book is in its very preliminary stages. Content will be moving around and updated. Currently, much of the book is a stitching together of previous pieces of my writing that I think might be relevant to this book. These chapters will be updated to consider the different context, audience, and content organization that’s best for this book. 1.2 Purpose of the book There’s a vast and powerful statistical framework out there. This book takes a modularized approach to making this framework accessible, so that as a problem solver, the reader can make a sequences of decisions to build models that are best suited to address the problem. Regression analysis can help solve two main types of problems: Interpreting the relationship between variables. Predicting a new outcome. This book primarily focusses on models for interpretation, and often references these two competing needs. The prediction problem is still discussed to some extent: once a model suited for interpretation is developed, it is still important to be able to use it to make predictions. For those interested primarily in prediction, check out resources on supervised learning, which aims to optimize predictions. There is another layer to interpretation that this book adopts, in terms of describing and understanding the motivation and inner workings behind each method. This is in contrast to a purely mathematical presentation of statistical methods. This book presents both an interpretation of a the high-level idea behind amethod, as well as a mathematical presentation to make these concepts precise. For example, the Kaplan-Meier estimate of the survival function is explained both intuitively and mathematically. Most statistical methods are built on a foundation of assumptions imposed on the data. But an assumption is almost never exactly true; so we instead explore consequences based on “how close” an assumption is to being true. In some cases, we even find that an anticipated assumption is not required at all, depending on how we would like to interpret the model – an example being the “requirement” of linear data in linear regression. Consequenty, one aim of this book is to discourage the thinking that a method either “can” or “cannot” be applied, instead thinking about pros and cons of various methods. Methods are demonstrated using the R statistical software. This is because R has an extensive selection of packages available for statistical analysis, and the tidyverse and tidymodels meta-packages make data analysis readable and organized. Since this book does not focus on optimally flexing a model function to conform to data (non-parametric supervised learning), languages better suited for this task, such as python, are not considered. The audience of this book is quite wide, with the aspects of modularization, interpretation, and non-binary view of assumptions perhaps appealing to various readers: Practitioners may find the modularization useful when making decisions when fitting models, the non-binary view of assumptions useful when evaluating the trustworthiness of their models, and the interpretation of their model useful when communicating their results. Experts in the field of Statistics might benefit from the unique modularized framework of the field, as they find well-used notions such as the expected value being challenged and expanded upon. Learners may find the modularization useful for compartmentalizing concepts, and the interpretation of methods useful for understanding concepts. 1.3 Tasks that motivate Regression Real world problems for which regression is an appropriate tool generally fall into two categories: Prediction: Predicting the response of a new unit/individual, sometimes also describing uncertainty in the prediction. Interpretation: Interpreting how predictors influence the response. For example, consider a person undergoing artificial insemination. Prediction: Given the person’s age, what is the chance of pregnancy? Interpretation: How does age influence the chance of pregnancy? How does time of insemination after a spike in Luteinizing hormone affect the chance of pregnancy, and how is this different for people over 40? This book does not focus on optimizing predictions, but focusses on the other tasks. This means: describing the uncertainty in predictions, or estimates in general, and interpreting how predictors influence the response. Why not focus on optimizing predictions? This is the objective of supervised learning, an entire discipline in itself. The scope of this book would just be too big to include this, too. This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. "],
["distributions.html", "Chapter 2 Distributions 2.1 Probabilistic Quantities", " Chapter 2 Distributions Caution: in a highly developmental stage! See Section 1.1. 2.1 Probabilistic Quantities Sometimes confusingly called “parameters”. Explain the quantities by their interpretation/usefulness, using examples. Mean: what number would you want if you have 10 weeks worth of data, and you want to estimate the total after 52 weeks (one year)? Median: Mode: High Quantile: Low Quantile: Extreme quantile? When you want information about an unknown quantity, it’s up to you what you decide to use. The mean is the most commonly sought when the unknown is numeric. I suspect this is the case for two main reasons: It simplifies computations. It’s what’s taught in school. "],
["estimation.html", "Chapter 3 Estimation 3.1 Estimation of Probabilistic Quantities", " Chapter 3 Estimation Caution: in a highly developmental stage! See Section 1.1. 3.1 Estimation of Probabilistic Quantities What makes a formula an estimator? It should go to the actual value as the sample size increases. It’s important to note that there are many ways to estimate a probabilistic quantity. Some of these will be better than others, often depending on the situation. Typically, the bias-variance tradeoff is at play here, where some estimators sacrifice bias to reduce variance, or vice versa. "],
["part-title.html", "Chapter 4 Part title", " Chapter 4 Part title "],
["my-part.html", "My Part", " My Part Text for my part. "],
["writing-the-sample-mean-as-an-optimization-problem.html", "Chapter 5 Writing the sample mean as an optimization problem", " Chapter 5 Writing the sample mean as an optimization problem Caution: in a highly developmental stage! See Section 1.1. (DSCI 561 lab2, 2018-2019) It’s important to know that the sample mean can also be calculated by finding the value that minimizes the sum of squared errors with respect to that value. We’ll explore that here. Store some numbers in the vector y. Calculate the sample mean of the data, stored in mu_y. This is not worth any marks, but having it as its own question jibes better with the autograder. We’ve defined sse() below, a function that takes some number and returns the sum of squared “errors” of all values of y with respect to the inputted number. An “error” is defined as the difference between two values. We’ve also generated a quick plot of this function for you. sse &lt;- Vectorize(function(m) sum((y - m)^2)) curve(sse, mu_y - 2*sd(y), mu_y + 2*sd(y)) Your task: use the optimize() function to find the value that minimizes the sum of squared errors. Hint: for the interval argument, specify an interval that contains the sample mean. Important points: You should recognize that the sample mean minimizes this function! You’ll be seeing the sum of squared errors a lot through the program (mean squared error and R^2 are based on it). This is because the mean is a very popular quantity to model and use as a prediction. If you’re not convinced, play with different numbers to see for yourself. "]
]
