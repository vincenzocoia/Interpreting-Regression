[
["index.html", "Interpreting Regression Chapter 1 Preamble 1.1 Caution 1.2 Purpose of the book 1.3 Tasks that motivate Regression 1.4 Examples", " Interpreting Regression Vincenzo Coia 2019-08-09 Chapter 1 Preamble 1.1 Caution This book is in its very preliminary stages. Content will be moving around and updated. Currently, much of the book is a stitching together of previous pieces of my writing that I think might be relevant to this book. These chapters will be updated to consider the different context, audience, and content organization that’s best for this book. Stage of development: Gather existing and potentially relevant pieces of writing Create a new chapter structure Parse existing writing into new chapters Writing: Write concepts/objectives of each chapter Fill in content Write preambles for each chapter and part 1.2 Purpose of the book There’s a vast and powerful statistical framework out there. This book takes a modularized approach to making this framework accessible, so that as a problem solver, the reader can make a sequences of decisions to build models that are best suited to address the problem. Regression analysis can help solve two main types of problems: Interpreting the relationship between variables. Predicting a new outcome. This book primarily focusses on models for interpretation, and often references these two competing needs. The prediction problem is still discussed to some extent: once a model suited for interpretation is developed, it is still important to be able to use it to make predictions. For those interested primarily in prediction, check out resources on supervised learning, which aims to optimize predictions. There is another layer to interpretation that this book adopts, in terms of describing and understanding the motivation and inner workings behind each method. This is in contrast to a purely mathematical presentation of statistical methods. This book presents both an interpretation of a the high-level idea behind amethod, as well as a mathematical presentation to make these concepts precise. For example, the Kaplan-Meier estimate of the survival function is explained both intuitively and mathematically. Most statistical methods are built on a foundation of assumptions imposed on the data. But an assumption is almost never exactly true; so we instead explore consequences based on “how close” an assumption is to being true. In some cases, we even find that an anticipated assumption is not required at all, depending on how we would like to interpret the model – an example being the “requirement” of linear data in linear regression. Consequenty, one aim of this book is to discourage the thinking that a method either “can” or “cannot” be applied, instead thinking about pros and cons of various methods. Methods are demonstrated using the R statistical software. This is because R has an extensive selection of packages available for statistical analysis, and the tidyverse and tidymodels meta-packages make data analysis readable and organized. Since this book does not focus on optimally flexing a model function to conform to data (non-parametric supervised learning), languages better suited for this task, such as python, are not considered. The audience of this book is quite wide, with the aspects of modularization, interpretation, and non-binary view of assumptions perhaps appealing to various readers: Practitioners may find the modularization useful when making decisions when fitting models, the non-binary view of assumptions useful when evaluating the trustworthiness of their models, and the interpretation of their model useful when communicating their results. Experts in the field of Statistics might benefit from the unique modularized framework of the field, as they find well-used notions such as the expected value being challenged and expanded upon. Learners may find the modularization useful for compartmentalizing concepts, and the interpretation of methods useful for understanding concepts. 1.3 Tasks that motivate Regression Real world problems for which regression is an appropriate tool generally fall into two categories: Prediction: Predicting the response of a new unit/individual, sometimes also describing uncertainty in the prediction. Interpretation: Interpreting how predictors influence the response. For example, consider a person undergoing artificial insemination. Prediction: Given the person’s age, what is the chance of pregnancy? Interpretation: How does age influence the chance of pregnancy? How does time of insemination after a spike in Luteinizing hormone affect the chance of pregnancy, and how is this different for people over 40? This book does not focus on optimizing predictions, but focusses on the other tasks. This means: describing the uncertainty in predictions, or estimates in general, and interpreting how predictors influence the response. Why not focus on optimizing predictions? This is the objective of supervised learning, an entire discipline in itself. The scope of this book would just be too big to include this, too. 1.4 Examples library(mice) ## Loading required package: lattice ## ## Attaching package: &#39;mice&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## cbind, rbind library(broom) library(tidyverse) ## ── Attaching packages ────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 3.1.1 ✔ purrr 0.3.2 ## ✔ tibble 2.1.3 ✔ dplyr 0.8.3 ## ✔ tidyr 0.8.3.9000 ✔ stringr 1.4.0 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 ## ── Conflicts ───────────────────────────────────── tidyverse_conflicts() ── ## ✖ tidyr::complete() masks mice::complete() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() Wage &lt;- ISLR::Wage NCI60 &lt;- ISLR::NCI60 baseball &lt;- Lahman::Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) cow &lt;- suppressMessages(read_csv(&quot;data/milk_fat.csv&quot;)) esoph &lt;- as_tibble(esoph) %&gt;% mutate(agegp = as.character(agegp)) titanic &lt;- na.omit(titanic::titanic_train) This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. "],
["regression-in-the-context-of-problem-solving.html", "Chapter 2 Regression in the context of problem solving 2.1 Communicating (Distillation 3-4) 2.2 Modelling (Distillation 2-3) 2.3 Asking useful statistical questions (Distillation 1-2) 2.4 Prerequisites to an analysis", " Chapter 2 Regression in the context of problem solving Caution: in a highly developmental stage! See Section 1.1. (BAIT 509 Class Meeting 06) Today, we’re going to take a break from supervised learning methods, and look at the process involved to use machine learning to address a question/problem faced by an organization. Generally, there are four parts of a machine learning analysis. In order from high to low level: The Business Question/Objective The Statistical Question(s)/Objective(s) The data and model The data product Doing an analysis is about distilling from the highest level to the lowest level. As such, there are three distillations to keep in mind: 1-2, 2-3, and 3-4: 1-2 is about asking the right questions. Our focus today. 2-3 is about building a useful model. Our focus for much of the course. 3-4 is about communicating the results. An analysis isn’t (and shouldn’t be) a linear progression through these as if they’re “steps”; rather, the process is iterative, revisiting any of the distillations at any point after embarking on the project. This is because none of the components are independent. Making progress on any of the three distillations gives you more information as to what the problem is. In all of this, it’s most beneficial to consider developing your analysis through small, simple steps. Always start with a basic approach, and gradually tweak the various components. Right off the bat, strive to get some sort of crude end product that resembles what your final product might look like. Not only is it more useful to have a “working version” of your product right away, but it gives you more information about the project, and will inform other distillations. We’ll look at these in turn, in reverse order. 2.1 Communicating (Distillation 3-4) Once you have a model, it needs to be delivered and used by others. (And if you are truly the only one who will use the model, then you still need to package it up as if it will be used by others, because it will – the “other” person here is future-you!) This is typically called the “data product”, because it can consist of a variety of things: a report a presentation an app a software package/pipeline …and any combination of these things. These almost always (or at least, almost always should) contain data visualizations. Your client might request this outright, but their suggestion might not be the best option. Perhaps they request a report and presentation communicating your findings, when a more appropriate product also includes an interactive app that allows them to explore your findings for themselves. Either way, the key here is communication. The challenges here include: Communication with language: understanding your audience, and tailoring your language for them. This usually means no jargon allowed. The key is to talk more about the output and the general idea of your model(s), but not machine learning or statistical jargon. Communication with visual design: this is about choosing what visuals are the most effective for communicating (called design choice). It’s key when you’re developing a first draft to not fret about details. For example, there’s no use interpreting the results of an analysis if you know the analysis will change. The idea is to set up a framework. For a report, this means outlining what you intend to write about, and where. Having this, and showing it to your client is useful as a sanity check to see that you’re about to produce something that the client currently sees as being potentially useful. This course is not about developing a data product. This is a topic that can be a discipline on its own, so we will not dwell any more on this. 2.2 Modelling (Distillation 2-3) This involves things like kNN, loess, classification/regression trees, etc. – things that the bulk of this course is focussed on. As such, I won’t dwell here. One thing I will say, though, is that there is temptation here to jump immediately into a complex analysis. Resist the temptation. Always start with a very basic approach – perhaps just linear regression, for example. The amount of complex approaches far exceeds the amount of simple ones, and starting with a simple one will help you end up in an appropriate complex analysis, if one is required. 2.3 Asking useful statistical questions (Distillation 1-2) Usually, a company is not served up a machine learning problem, complete with data and a description of the response and predictors. Instead, they’re faced with some high-level objective/question that we’ll call the business question/objective, which needs refining to a statistical question/objective – one that is directly addressable by machine learning. 2.3.1 Business objectives: examples Examples of business objectives (for which machine learning is a relevant approach) (more examples at this altexsoft blog post): Reduce the amount of spam email received Early prediction of product failure Find undervalued mines Make a transit system more efficient Hire efficient staff 2.3.2 Statistical objectives: refining Statistical objectives should be specific. For some business objectives, the distillation is not dramatic; for others, it is. Because supervised learning is about predicting a response \\(Y\\) from predictors \\(X_1, \\ldots, X_p\\), identifying a statistical objective involves: Most importantly, identifying what response is the most valuable/aligned with the business objective. Identifying the cases that will be used for learning, and what cases you’ll be predicting on. What predictors you plan on using, or at least where we’ll be looking for these. Note: half of this is the task of feature selection – a topic we may cover last in the course – but, largely, this is a human decision based on what we think is more informative, so is important to include in your supervised learning question/objective. 2.3.3 Statistical objectives: examples Statistical objectives corresponding to the above business objective examples might be: Predict monthly sales (response) from a personality test (predictors), issued to our current employees (learning cases) and prospective employees (prediction cases). Obtain individual-specific classifications of email spam (response) based on features present in the name and body of an email (predictors). Cases of spam will be gathered over time, as the employee manually identifies mistakes in classification (prediction cases become learning cases after emails are viewed). Classify a device as either faulty or not (response) based on predictors chosen by expert advice (predictors). Of those predicted faulty, check their performance for one hour (predictors, round 2) to predict lifetime of the device (response, round 2). Sample cases from our test facility will be used (learning cases) to make predictions on items sent to the shelf. Predict total volume of gold and silver at a site (response) based on concentrations of various minerals found in drill samples (predictors). Use cases where total volume is known (learning cases) to make predictions on mines where total volume is not known or uncertain (prediction cases). Predict the time it takes a bus to travel between set stops (responses), based on time factors such as time of the day, time of the week, and time of the year (predictors). Use data from the server, available for the past 8 years (learning cases) to make predictions for 2019 (prediction cases). 2.3.4 Statistical questions are not the full picture Almost (?) always, the business objective is more complex than the statistical objective. By refining a business objective to a statistical one, we lose part of the essence of the business objective. It’s important to have a sense of the ways in which your statistical objective falls short, and the ways in which it’s on the mark, so that you keep a sense of the big picture. For instance, you’ll better understand how your machine learning model(s) fit into the big picture of the business question, and how you might ask a different statistical question to gain different insight. 2.3.5 Statistical objectives unrelated to supervised learning Naturally, for this course, we’ll focus on statistical questions that require supervised learning. But in reality, you should consider other cases too. Let’s look at other branches of data analysis that you might wish to consider (though, not for this course), through an example. Business objective: To gain insight into the productivity of two branches of a company. Examples of statistical questions: Hypothesis testing. Is the mean number of sick days per employee different between two branches of a company? Supervised learning doesn’t cover testing for differences. Unsupervised learning. What is the mean sentiment of the internal discussion channels from both branches? There are no records of the response here, as required by supervised learning (by definition). Statistical inference. Estimate the mean difference in monthly revenue generated by both branches, along with how certain you are with that estimate. Supervised learning typically isn’t concerned about communicating how certain your estimate is. 2.4 Prerequisites to an analysis Before going into any analysis, you need to know what data are available, and what state they’re in. Why: The statistical questions you form will depend on the data that are available. For example, you can’t form a statistical question about the performance of a division of your business if there are no data available on that division, or if the data you collected aren’t indicators of performance. How: Always make exploratory plots to get a feel for the data – and keep doing so to explore other aspects of the data, as you learn more about the data set by doing your analysis. This is often called Exploratory Data Analysis (or EDA). "],
["an-outcome-on-its-own.html", "An outcome on its own", " An outcome on its own How can we get a handle on an outcome that seems random? Although the score of a Canucks game, a stock price, or river flow is uncertain, this does not mean that these quantities are futile to predict or describe. This part of the book describes how to do just that, using only observations on a single outcome, by shedding light on concepts of probability and univariate analysis as they apply to data science. "],
["distributions-uncertainty-is-worth-explaining.html", "Chapter 3 Distributions: Uncertainty is worth explaining", " Chapter 3 Distributions: Uncertainty is worth explaining Concepts: Variable types. Numeric variables vs. categorical. Ordinal as being “in between” numeric and categorical. Discrete as a special case of numeric, sometimes worth distinguishing. Distributions as the limiting collection of iid data. Ways to depict a distribution, and the interpretation of each. The concept that one tells you everything about the distribution, so you can in theory derive one form from another. One does not give you more information than another. These distributions might not seem practical, but they certainly are. Even in cases where we are considering many other pieces of information aside from the response, we are still dealing with univariate distributions – we’ll see later that the only difference is that they are no longer marginal distributions (they are conditional). "],
["explaining-an-uncertain-outcome-interpretable-quantities.html", "Chapter 4 Explaining an uncertain outcome: interpretable quantities 4.1 Probabilistic Quantities 4.2 What is the mean, anyway? 4.3 Quantiles", " Chapter 4 Explaining an uncertain outcome: interpretable quantities Caution: in a highly developmental stage! See Section 1.1. Concepts: Probabilistic quantities and their interpretation Prediction as choosing a probabilistic quantity to put forth. Irreducible error 4.1 Probabilistic Quantities Sometimes confusingly called “parameters”. Explain the quantities by their interpretation/usefulness, using examples. Mean: what number would you want if you have 10 weeks worth of data, and you want to estimate the total after 52 weeks (one year)? Mode Quantiles: Measures of discrepency/“distance” (for prediction): difference ratio Measures of spread: Variance IQR Coefficient of Variance (point to its usefulness on a positive ratio scale) Information measures When you want information about an unknown quantity, it’s up to you what you decide to use. The mean is the most commonly sought when the unknown is numeric. I suspect this is the case for two main reasons: It simplifies computations. It’s what’s taught in school. 4.2 What is the mean, anyway? Imagine trying to predict your total expenses for the next two years. You have monthly expenses listed for the past 12 months. What’s one simple way of making your prediction? Calculate the average expense from the past 12 months, and multiply that by 24. In general, a mean (or expected value) can be interpreted as the long-run average. However, the mean tends to be interpreted as a measure of central tendency, which has a more nebulous interpretation as a “typical” outcome, or an outcome for which most of the data will be “nearest”. 4.3 Quantiles It’s common to “default” to using the mean to make decisions. But, the mean is not always appropriate (I wrote a blog post about this): Sometimes it makes sense to relate the outcome to a coin toss. For example, find an amount for which next month’s expenditures will either exceed or be under with a 50% chance. Sometimes a conservative/liberal estimate is wanted. For example, a bus company wants conservative estimates so that most busses fall within the estimated travel time. In these cases, we care about quantiles, not the mean. Estimating them is called quantile regression (as opposed to mean regression). Recall what quantiles are: the \\(\\tau\\)-quantile (for \\(\\tau\\) between 0 and 1) is the number that will be exceeded by the outcome with a \\((1-\\tau)\\) chance. In other words, there is a probability of \\(\\tau\\) that the outcome will be below the \\(\\tau\\)-quantile. \\(\\tau\\) is referred to as the quantile level, or sometimes the quantile index. For example, a bus company might want to predict the 0.8-quantile of transit time – 80% of busses will get to their destination within that time. "],
["estimation.html", "Chapter 5 Estimation 5.1 Estimation of Probabilistic Quantities", " Chapter 5 Estimation Caution: in a highly developmental stage! See Section 1.1. Concepts: What is an estimator? Sampling distributions Reducible error 5.1 Estimation of Probabilistic Quantities What makes a formula an estimator? It should go to the actual value as the sample size increases. It’s important to note that there are many ways to estimate a probabilistic quantity. Some of these will be better than others, often depending on the situation. Typically, the bias-variance tradeoff is at play here, where some estimators sacrifice bias to reduce variance, or vice versa. "],
["parametric-families-of-distributions.html", "Chapter 6 Parametric Families of Distributions 6.1 Parametric Families of Distributions 6.2 Analyses under a Distributional Assumption", " Chapter 6 Parametric Families of Distributions Caution: in a highly developmental stage! See Section 1.1. Concepts: Common scales: Positive ratio scale, binary, (0,1) Different data generating processes give rise to various parametric families of distributions. We’ll explain a good chunk of them. These are useful in data analysis because they narrow down the things that need to be estimated. Improving estimator quality by parametric distributional assumptions and MLE suppressPackageStartupMessages(library(tidyverse)) ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 6.1 Parametric Families of Distributions What is meant by “family”? This means that there are more than one of them, and that a specific distribution from the family can be characterized the family’s parameters. This is what is meant by “parametric” – the distribution can be distilled down to a set of parameters. For example, to identify a Gaussian distribution, we need to know the mean and variance. Note that some families are characterized by parameters that do not necessarily have an interpretation (or at least an easy one) – for example, to identify a Beta distribution, we need to know the two shape parameters \\(\\alpha\\) and \\(\\beta\\). Technicality that you can safely skip: It’s not that there are “set” parameters that identify a distribution. For example, although a Beta distribution is identified by the two shape parameters \\(\\alpha\\) and \\(\\beta\\), the distribution can also be uniquely identified by its mean and variance. Or, by two quantiles. In general, we need as many pieces of information as there are parameters. This needs to be done in an identifiable way, however. For example, it’s not enough to identify an Exponential distibution by its skewness, because all Exponential distributions have a skewness of 2! The way in which we identify a distribution from parameters is called the parameterization of the distribution. 6.2 Analyses under a Distributional Assumption Remember that the purpose of a univariate analysis on its own is to estimate parameters, communicate uncertainty about the estimates, and specify the distribution that “generated” the data. So far, we’ve seen how to do this using empirical methods, which don’t make any assumptions about how the data were obtained. But, if we suspect we know what distribution family generated the data, then this is potentially valuable information that can benefit the analysis. This chapter explains how to proceed with your analysis by making a distributional assumption, and when this might be a good idea. In the previous chapter, we estimated parameters like means and probabilities using empirical (or “sample versions”) of these parameters. Usually, these estimators perform well in the univariate setting, but there are some circumstances where they do not. There is a technique called Maximum Likelihood Estimation (MLE) that approximates the data distribution using a parametric family, and if done carefully, allows for significant improvements to estimation. This chapter first explains what MLE is and how to implement the technique, as well as why and when it would be of use. 6.2.1 Maximum Likelihood Estimation Maximum likelihood estimation is a way of estimating parameters by first estimating the data distribution from a specified parametric family. The steps are as follows. Make a distributional assumption: Choose a parametric family of distributions that you think is a decent approximation to the data distribution. Estimate: From that family, choose the distribution that fits the data “best”. Extract: Using the fitted distribution, extract the parameter(s) of interest (such as the mean and/or quantiles). Check the assumption: Check that the fitted distribution is a reasonable approximation to the data distribution (not required for estimation, but is good practice). Let’s look at these steps in turn, together with the following examples. Consider the following data set of damages caused by hurricanes, in billions of USD. The distribution of the 144 observations is depicted in the following histogram: suppressPackageStartupMessages(library(tidyverse)) data(damage, package = &quot;extRemes&quot;) house &lt;- suppressMessages(read_csv(&quot;data/house.csv&quot;)) (hurricane_hist &lt;- ggplot(damage, aes(Dam)) + geom_histogram(aes(y = ..density..), bins = 50, alpha = 0.75) + theme_bw() + scale_x_continuous(&quot;Damage (billions of USD)&quot;, labels = scales::dollar_format()) + ylab(&quot;Density&quot;)) Data such as this can help an insurance company with their financial planning. Knowing an upper quantile, such as the 0.8-quantile, would give a sense of the damage caused by the “worst” hurricanes. Consider the following sale prices of houses: house &lt;- read_csv(&quot;data/house.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_character(), ## Id = col_integer(), ## MSSubClass = col_integer(), ## LotFrontage = col_integer(), ## LotArea = col_integer(), ## OverallQual = col_integer(), ## OverallCond = col_integer(), ## YearBuilt = col_integer(), ## YearRemodAdd = col_integer(), ## MasVnrArea = col_integer(), ## BsmtFinSF1 = col_integer(), ## BsmtFinSF2 = col_integer(), ## BsmtUnfSF = col_integer(), ## TotalBsmtSF = col_integer(), ## `1stFlrSF` = col_integer(), ## `2ndFlrSF` = col_integer(), ## LowQualFinSF = col_integer(), ## GrLivArea = col_integer(), ## BsmtFullBath = col_integer(), ## BsmtHalfBath = col_integer(), ## FullBath = col_integer() ## # ... with 18 more columns ## ) ## See spec(...) for full column specifications. house_hist &lt;- ggplot(house, aes(SalePrice)) + geom_histogram(aes(y = ..density..), alpha = 0.75) + theme_bw() + labs(x = &quot;Price&quot;, y = &quot;Density&quot;) + scale_x_continuous(labels = scales::dollar_format()) cowplot::plot_grid( house_hist, house_hist + scale_x_log10(labels = scales::dollar_format()) ) ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which ## will replace the existing scale. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.2.1.1 Step 1: Make a distributional assumption This step requires choosing a family to approximate the data distribution (you can find several families defined in the previous chapter). Here are two general guidelines that may help you choose a distribution family. Visually match the variable characteristics and shape of the data distribution to a distribution family. If possible, think about the process that “generated” the data, and match that process to the data-generating process defining a family. Always keep in mind that there is almost never a “correct” choice! Remember, we are making an approximation here, not seeking a “true” distribution family. Example 1. In the hurricane damages example, the data variable is a positive continuous variable, and has a histogram that appears to decay starting from a damage of zero. The selected distribution should accomodate this – Weibull, Gamma, or GPD can all accomodate this. But the family should allow for heavy-tailed distributions to accomodate the two large data values that we see in the histogram, leaving a GPD as a good candidate. As for the way the data are “generated”, the data are inherently recorded because they are extreme, and this matches the way that a GPD is derived. Example 2. In the house prices example, the family should also accomodate a positive continuous random variable, should be unimodal with two tails, and be skewed to the right. A Weibull or lognormal distribution so far seem like good candidates. Since the log of house prices looks like a Gaussian distribution, we choose a lognormal distribution, since this is how the lognormal family is defined. 6.2.1.2 Step 2: Estimate So far, we’ve selected a family of distributions. Now, in this step, we need to select the distribution from this family that best matches the data. This step is the namesake of MLE. The key is to select the distribution for which the observed data are most likely to have been drawn from. We can do this through a quantity called the likelihood, which can be calculated for any distribution that has a density/pmf, then finding the distribution that has the largest likelihood. Let’s break these two concepts down. Likelihood. The likelihood is a useful way to measure how well a distribution fits the data. To calculate the likelihood, denoted \\(\\mathcal{L}\\), from data \\(y_1, \\ldots, y_n\\) and a distribution with density/pmf \\(f\\), calculate the product of the densities/pmf’s evaluated at the data: \\[\\mathcal{L} = \\prod_{i=1}^n f(y_i).\\] When \\(f\\) is a pmf, you can interpret the likelihood as the probability of observing the data under the distribution \\(f\\). When \\(f\\) is a density, the interpretation of likelihood is less tangible, and is the probability density of observing the data under the distribution \\(f\\). These interpretations are exactly true if the data are independent, but are still approximately true if data are “almost” independent. Even with non-independent data, the likelihood is still a useful measurement. A similar quantity to the likelihood is the negative log likelihood (nllh), defined as \\[\\ell = -\\log\\mathcal{L} = -\\sum_{i=1}^n \\log f(y_i).\\] The nllh is numerically more convenient than the likelihood. For example, the likelihood (\\(\\mathcal{L}\\)) tends to be an extremely small number, whereas the nllh typically is not. For instance, 100 draws from a N(0,1) distribution results in a likelihood that’s typically around \\(3 \\times 10^{-62},\\) whereas the nllh is typically around 141.5. Note that minimizing the nllh is the same as maximizing the likelihood. Finding the distribution that has the largest likelihood Remember that each distribution in the distribution family that we selected can be represented by its parameters – for example, the Normal distribution by its mean and variance, or the Beta distribution by \\(\\alpha\\) and \\(\\beta\\). This means that we can view the likelihood as a function of the family’s parameters, and optimize this function! This can sometimes be done using calculus, but is most often done numerically. We end up with estimates of the distribution’s parameters, which is the same thing as having an estimate of the data’s distribution. Don’t stop here if you are looking to estimate something other than the distibution’s parameters – move on to Step 3. Example 1. For the hurricane example, first find the GPD shape and scale parameters that maximize the likelihood (or, minimize the nllh). gpd_nllh &lt;- function(parameters) { scale &lt;- parameters[1] shape &lt;- parameters[2] if (scale &lt;= 0) return(Inf) -sum(evd::dgpd(damage$Dam, scale = scale, shape = shape, log = TRUE)) } gpd_optim &lt;- optim(c(1, 1), gpd_nllh) gpd_scale &lt;- gpd_optim$par[1] gpd_shape &lt;- gpd_optim$par[2] Take a look at the GPD density corresponding to these parameters, with the data histogram in the background: gpd_density &lt;- function(x) evd::dgpd(x, scale = gpd_scale, shape = gpd_shape) hurricane_hist + stat_function(fun = gpd_density, colour = &quot;blue&quot;) + ylim(c(0, 0.13)) ## Warning: Removed 1 rows containing missing values (geom_bar). ## Warning: Removed 2 rows containing missing values (geom_path). Example 2. ln_nllh &lt;- function(parameters) { loc &lt;- parameters[1] scale &lt;- parameters[2] if (scale &lt;= 0) return(Inf) -sum(dlnorm(house$SalePrice, meanlog = loc, sdlog = scale, log = TRUE)) } ln_optim &lt;- optim(c(0, 1), ln_nllh) ln_loc &lt;- ln_optim$par[1] ln_scale &lt;- ln_optim$par[2] ln_density &lt;- function(x) dlnorm(x, meanlog = ln_loc, sdlog = ln_scale) house_hist + stat_function(fun = ln_density, colour = &quot;blue&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.2.1.3 Step 3: Extract Now that we have the data distribution estimated, we can extract any parameter we’d like, such as the mean or quantiles. This might involve looking up formulas based on the distribution’s parameters – for example, the mean of a Beta distribution with parameters \\(\\alpha\\) and \\(\\beta\\) is \\(\\alpha/(\\alpha + \\beta)\\). Example 1. Here is the MLE of the median hurricane damage: gpd_qf &lt;- function(p) evd::qgpd(p, scale = gpd_scale, shape = gpd_shape) gpd_qf(0.5) ## [1] 0.1884567 Here is the MLE of the 0.9-quantile: gpd_qf(0.9) ## [1] 6.659546 Here is the MLE of the IQR: gpd_qf(0.75) - gpd_qf(0.25) ## [1] 0.9199308 Example 2. Mean of the Lognormal distribution can be computed as \\(\\exp(\\mu + \\sigma^2/2)\\); so the MLE for the mean is: exp(ln_loc + ln_scale^2/2) ## [1] 180579.9 Variance of the Lognormal distribution can be computed as \\((\\exp(\\sigma^2)-1)(\\exp(2\\mu + \\sigma^2)\\); so the MLE for the variance is: (exp(ln_scale^2) - 1) * (exp(2*ln_loc + ln_scale^2)) ## [1] 5642912264 The MLE for the 0.9-quantile is: qlnorm(0.9, meanlog = ln_loc, sdlog = ln_scale) ## [1] 278205 6.2.1.4 Step 4: Check the Assumption In order to end up with “good” estimates from MLE, the fitted distribution from Step 2 should be a decent approximation to the data distribution. To see why, consider a poor distributional assumption – such as approximating the distribution of house prices as Uniform. We end up with the following density fitted by MLE: ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## [1] 214925 ## [1] 574975 The fitted density is flat, and does not match the data (histogram) at all. This means we can anticipate parameter estimates to be way off. For example, according to the model, the mean is somewhere around $400,000, and a half of the house prices roughly lie somewhere between $200,000 and $600,000. The above method of comparing the modelled density to the histogram is one effective way of checking the distributional assumption. Another way is to use a QQ-plot. These visual methods might seem informal, but they are very powerful, and should always be investigated if possible. After visualizing the fit, if you want to add more rigor to your assumption checking, you can consider a hypothesis test such as the Kolmogorov-Smirnov test, the Anderson-Darling test, or the Cram'{e}r-von Mises test for equality of distributions. Just remember that there is no such thing as a “correct” distributional assumption in practice, but rather just approximations that have different degrees of plausibility. The key is in avoiding bad assumptions, as opposed to finding the “right” assumption. 6.2.2 Usefulness in Practice The MLE is an overall superior way to estimate parameters, and has some theoretically desirable properties – as long as the distributional assumption from Step 1 is not a bad one. Overall, in the univariate setting, the improvement brought about by MLE is in general underwhelming, except in some situations. Where MLE really shines is in the regression setting, as is shown in a later chapter, but it’s important to understand the fundamentals in the univariate case before extending concepts to the regression setting. This chapter explores the following: Under what situations in the univariate setting does the MLE really shine? Just how “bad” does an assumption have to be in order for the MLE to be worse than the empirical estimate? Ultimately, the question is not whether the MLE is better than empirical estimates, but rather whether making a distributional assumption is better than not. 6.2.2.1 Where MLE really shines 6.2.2.2 Effect of Assumption Badness Take-aways: sampling distributions are narrower for MLE, but more and more biased as the distributional assumption gets worse and worse. Most of the time in the univariate setting, the MLE is often not much better than the empirical estimate, and is sometimes even identical. Aside from the cases described below, where MLE really shines is in the regression setting, Take a look at the sampling distributions of three estimators of the 0.8-quantile: the sample version, the MLE under a Gaussian assumption, and the MLE under a generalized Pareto distribution (GPD) assumption. The sampling distributions are obtained using bootstrapping. The Gaussian-based MLE is an example of a bad MLE that uses a poorly chosen assumption – its sampling distribution is much wider than that of the sample version. On the other hand, the GPD-based MLE is based on a much more realistic assumption, and shows a significant improvement to the sample version – its sampling distribution is much narrower. # dgpd &lt;- function(x, sig, xi) 1/sig*(1 + xi*(x/sig))^(-1/(xi+1)) # gpd_quantile &lt;- function(x, p = 0.5) { # fit &lt;- ismev::gpd.fit(x, 0, show = FALSE) # sigma &lt;- fit$mle[1] # xi &lt;- fit$mle[2] # sigma * (p^(-xi) - 1) / xi # } # wei_quantile &lt;- function(x, p = 0.5) { # nllh &lt;- function(par) -sum(dweibull(x, par[1], par[2], log = TRUE)) # par_hat &lt;- optim(c(1,1), nllh)$par # qweibull(p, par_hat[1], par_hat[2]) # } # sampling_dist &lt;- damage %&gt;% # bootstraps(times = 1000) %&gt;% # pull(splits) %&gt;% # map(as_tibble) %&gt;% # map_df(~ summarise( # .x, # bar = quantile(Dam, probs = 0.8), # mle_gpd = gpd_quantile(Dam, 0.8), # mle_gau = qnorm(0.8, mean = mean(Dam), sd = sd(Dam))) # ) # sampling_dist %&gt;% # gather(key = &quot;method&quot;, value = &quot;estimate&quot;) %&gt;% # ggplot(aes(estimate)) + # facet_wrap(~method, nrow = 1, scales = &quot;free_x&quot;) + # geom_histogram(bins = 30) + # theme_bw() Caution: A common misconception is that the two large observations in the dataset are outliers, and therefore should be removed. However, doing so would bias our understanding of hurricance damages, since these “outliers” are real occurences. In other cases, an MLE does not result in much of an improvement at all. Although there are far fewer cases in the univariate case compared to the regression setting where MLE gives a dramatic improvement to estimation, it’s still worth discussing when it’s most useful in the univariate setting and to ground concepts. High quantile example for a PI: # N &lt;- 10000 # rate &lt;- 1 # ordered &lt;- numeric(0) # mle &lt;- numeric(0) # for (i in 1:N) { # x &lt;- rexp(10, rate = rate) # ordered[i] &lt;- quantile(x, probs = 0.975, type = 1) # mle[i] &lt;- qexp(0.975, rate = 1/mean(x)) # } # tibble(ordered, mle) %&gt;% # gather(value = &quot;estimate&quot;) %&gt;% # ggplot(aes(estimate)) + # geom_density(aes(group = key, fill = key), alpha = 0.5) + # geom_vline(xintercept = qexp(0.975, rate = rate), # linetype = &quot;dashed&quot;) + # theme_bw() # sd(ordered) # sd(mle) In both cases, the sampling distribution of the MLE is better than that of the sample version – that is, more narrow and (sometimes) centered closer to the true value. Is there a better estimator than the MLE? It turns out that the MLE is realistically the best that we can do – as long as the distributional assumption is not too bad of an approximation. If you’re curious to learn more, the end of this chapter fleshes this out using precise terminology. If the improvement by using MLE does not seem very impressive to you, you’d be right – at least in the univariate world. To see much difference between the MLE and sample version estimators, you’d need to be estimating low-probability events with a small amount of data. In fact, estimating the mean using MLE most often results in the same estimator as the sample mean! Don’t write off the MLE just yet – it really shines in the regression setting, where it has even more benefits than just improved estimation. Tune in to Part II to learn more. As an example of (1), suppose you are measuring the ratio of torso height to body height. Since your sample falls between 0 and 1, and probably does not have a weirdly shaped density, a Beta distribution would be a good assumption, since the Beta family spans non-weird densities over (0, 1). However, not knowing the data-generating process, you would not be able to justify the distribution completely (and that’s OK). As an example of (2), perhaps you are operating the port of Vancouver, BC, and based on your experience, know that vessels arrive more-or-less independently at some average rate. This is how a Poisson distribution is defined. Not only that, but the data appear to be shaped like a Poisson distribution. Then it would be justifiable to assume the data follow a Poisson distribution. # n &lt;- 50 # N &lt;- 1000 # fit_mle &lt;- numeric(0) # fit_ls &lt;- numeric(0) # for (i in 1:N) { # x &lt;- rnorm(n) # mu &lt;- 1/(1+exp(-x)) # y &lt;- rbinom(n, size = 1, prob = mu) # fit_mle[i] &lt;- glm(y ~ x, family = &quot;binomial&quot;)$coefficients[2] # ls &lt;- function(par) sum((y - 1/(1+exp(-par[1]-par[2]*x)))^2) # fit_ls[i] &lt;- optim(c(0,1), ls)$par[2] # } # tibble(fit_mle, fit_ls) %&gt;% # gather(value = &quot;beta&quot;) %&gt;% # ggplot(aes(beta)) + # # scale_x_log10() + # geom_density(aes(group = key, fill = key), alpha = 0.5) # sd(fit_mle) # sd(fit_ls) # IQR(fit_mle) # IQR(fit_ls) # ## More extremes show up with LS (at least with n=50): # sort(fit_ls) %&gt;% tail(10) # sort(fit_mle) %&gt;% tail(10) # ## Gaussian assumption # ## - LS not even that good at n=100 -- bowed down. MLE is good. # ## - MLE qqplot with n=50 looks about the same as LS with n=100 # ## - LS at n=50 is heavy tailed (seemingly). # qqnorm(fit_mle) # qqnorm(fit_ls) For n=50, check out an example that results in an extreme beta: # n &lt;- 50 # beta &lt;- 0 # while (beta &lt; 300) { # x &lt;- rnorm(n) # mu &lt;- 1/(1+exp(-x)) # y &lt;- rbinom(n, size = 1, prob = mu) # ls &lt;- function(par) sum((y - 1/(1+exp(-par[1]-par[2]*x)))^2) # .optim &lt;- optim(c(0,1), ls) # beta &lt;- .optim$par[2] # alpha &lt;- .optim$par[1] # } # if (.optim$convergence == 0) stop(&quot;optim didn&#39;t successfully converge.&quot;) # mle &lt;- glm(y ~ x, family = &quot;binomial&quot;)$coefficients # qplot(x, y) + # stat_function(fun = function(x) 1/(1+exp(-alpha-beta*x)), mapping = aes(colour = &quot;LS&quot;)) + # stat_function(fun = function(x) 1/(1+exp(-mle[1]-mle[2]*x)), mapping = aes(colour = &quot;MLE&quot;)) # # MLE is still slightly narrower, even for a Beta(2,2) distribution (which is # # symmetric and bell-like) -- for n=5 and n=50. Both close to Gaussian, even at # # n=5 (as expected). # shape1 &lt;- 2 # shape2 &lt;- 2 # foo &lt;- function(x) dbeta(x, shape1, shape2) # curve(foo, 0, 1) # n &lt;- 5 # N &lt;- 1000 # xbar &lt;- numeric(0) # mle &lt;- numeric(0) # for (i in 1:N) { # x &lt;- rbeta(n, shape1, shape2) # xbar[i] &lt;- mean(x) # nllh &lt;- function(par) { # if (min(par) &lt;= 0) return(Inf) # -sum(dbeta(x, par[1], par[2], log = TRUE)) # } # .optim &lt;- optim(c(shape1, shape2), nllh) # par_hat &lt;- .optim$par # mle[i] &lt;- par_hat[1] / sum(par_hat) # } # plot(mle - xbar) # The estimates aren&#39;t the same. # tibble(mle, xbar) %&gt;% # gather() %&gt;% # ggplot(aes(value)) + # geom_density(aes(group = key, fill = key), alpha = 0.5) # sd(mle) # sd(xbar) # qqnorm(mle) # qqnorm(xbar) # # Univariate MLE *especially* important for heavy tailed distributions! # nu &lt;- 1.5 # n &lt;- 5 # N &lt;- 1000 # xbar &lt;- numeric(0) # mle &lt;- numeric(0) # for (i in 1:N) { # x &lt;- rt(n, df = nu) # xbar[i] &lt;- mean(x) # nllh &lt;- function(par) -sum(dt(x, df = par[1], ncp = par[2], log = TRUE)) # .optim &lt;- optim(c(nu, 0), nllh) # mle[i] &lt;- .optim$par[2] # } # plot(mle - xbar) # The estimates aren&#39;t the same. # tibble(mle, xbar) %&gt;% # gather() %&gt;% # ggplot(aes(value)) + # geom_density(aes(group = key, fill = key), alpha = 0.5) # sd(mle) # sd(xbar) # qqnorm(mle) # qqnorm(xbar) "],
["prediction-harnessing-the-signal.html", "Prediction: harnessing the signal", " Prediction: harnessing the signal suppressPackageStartupMessages(library(tidyverse)) ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 "],
["reducing-uncertainty-of-the-outcome-including-predictors.html", "Chapter 7 Reducing uncertainty of the outcome: including predictors 7.1 Variable terminology 7.2 Irreducible Error 7.3 In-class Exercises: Irreducible Error", " Chapter 7 Reducing uncertainty of the outcome: including predictors Caution: in a highly developmental stage! See Section 1.1. 7.1 Variable terminology In supervised learning: The output is a random variable, typically denoted \\(Y\\). The input(s) variables (which may or may not be random), if there are \\(p\\) of them, are typically denoted \\(X_1\\), …, \\(X_p\\) – or just \\(X\\) if there’s one. There are many names for the input and output variables. Here are some (there are more, undoubtedly): Output: response, dependent variable. Input: predictors, covariates, features, independent variables, explanatory variables, regressors. In BAIT 509, we will use the terminology predictors and response. 7.1.1 Variable types Terminology surrounding variable types can be confusing, so it’s worth going over it. Here are some non-technical definitions. A numeric variable is one that has a quantity associated with it, such as age or height. Of these, a numeric variable can be one of two things: A categorical variable, as the name suggests, is a variable that can be one of many categories. For example, type of fruit; success or failure. 7.2 Irreducible Error The concept of irreducible error is paramount to supervised learning. Next time, we’ll look at the concept of reducible error. When building a supervised learning model (like linear regression), we can never build a perfect forecaster – even if we have infinite data! Let’s explore this notion. When we hypothetically have an infinite amount of data to train a model with, what we actually have is the probability distribution of \\(Y\\) given any value of the predictors. The uncertainty in this probability distribution is the irreducible error. Example: Let’s say \\((X,Y)\\) follows a (known) bivariate Normal distribution. Then, for any input of \\(X\\), \\(Y\\) has a distribution. Here are some examples of this distribution for a few values of the predictor variable (these are called conditional distributions, because they’re conditional on observing particular values of the predictors). This means we cannot know what \\(Y\\) will be, no matter what! What’s one to do? In regression (i.e., when \\(Y\\) is numeric, as above), the go-to standard is to predict the mean as our best guess. We typically measure error with the mean squared error = average of (observed-predicted)^2. In classification, the conditional distributions are categorical variables, so the go-to standard is to predict the mode as our best guess (i.e., the category having the highest probability). A typical measurement of error is the error rate = proportion of incorrect predictions. A more “complete” picture of error is the entropy, or equivalently, the information measure. In Class Meeting 07, we’ll look at different options besides the mean and the mode. An important concept is that predictors give us more information about the response, leading to a more certain distribution. In the above example, let’s try to make a prediction when we don’t have knowledge of predictors. Here’s what the distribution of the response looks like: This is much more uncertain than in the case where we have predictors! 7.3 In-class Exercises: Irreducible Error NOT REQUIRED FOR PARTICIPATION 7.3.1 Oracle regression Suppose you have two independent predictors, \\(X_1, X_2 \\sim N(0,1)\\), and the conditional distribution of \\(Y\\) is \\[ Y \\mid (X_1=x_1, X_2=x_2) \\sim N(5-x_1+2x_2, 1). \\] From this, it follows that: The conditional distribution of \\(Y\\) given only \\(X_1\\) is \\[ Y \\mid X_1=x_1 \\sim N(5-x_1, 5). \\] The conditional distribution of \\(Y\\) given only \\(X_2\\) is \\[ Y \\mid X_2=x_2 \\sim N(5+2x_2, 2). \\] The (marginal) distribution of \\(Y\\) (not given any of the predictors) is \\[ Y \\sim N(5, 6). \\] The following R function generates data from the joint distribution of \\((X_1, X_2, Y)\\). It takes a single positive integer as an input, representing the sample size, and returns a tibble (a fancy version of a data frame) with columns named x1, x2, and y, corresponding to the random vector \\((X_1, X_2, Y)\\), with realizations given in the rows. genreg &lt;- function(n){ x1 &lt;- rnorm(n) x2 &lt;- rnorm(n) eps &lt;- rnorm(n) y &lt;- 5-x1+2*x2+eps tibble(x1=x1, x2=x2, y=y) } Generate data – as much as you’d like. dat &lt;- genreg(1000) For now, ignore the \\(Y\\) values. Use the means from the distributions listed above to predict \\(Y\\) under four circumstances: Using both the values of \\(X_1\\) and \\(X_2\\). Using only the values of \\(X_1\\). Using only the values of \\(X_2\\). Using neither the values of \\(X_1\\) nor \\(X_2\\). (Your predictions in this case will be the same every time – what is that number?) dat &lt;- mutate(dat, yhat = FILL_THIS_IN, yhat1 = FILL_THIS_IN, yhat2 = FILL_THIS_IN, yhat12 = FILL_THIS_IN) Now use the actual outcomes of \\(Y\\) to calculate the mean squared error (MSE) for each of the four situations. Try re-running the simulation with a new batch of data. Do your MSE’s change much? If so, choose a larger sample so that these numbers are more stable. (mse &lt;- mean((dat$FILL_THIS_IN - dat$y)^2)) (mse1 &lt;- mean((dat$FILL_THIS_IN - dat$y)^2)) (mse2 &lt;- mean((dat$FILL_THIS_IN - dat$y)^2)) (mse12 &lt;- mean((dat$FILL_THIS_IN - dat$y)^2)) knitr::kable(tribble( ~ Case, ~ MSE, &quot;No predictors&quot;, mse, &quot;Only X1&quot;, mse1, &quot;Only X2&quot;, mse2, &quot;Both X1 and X2&quot;, mse12 )) Order the situations from “best forecaster” to “worst forecaster”. Why do we see this order? 7.3.2 Oracle classification Consider a categorical response that can take on one of three categories: A, B, or C. The conditional probabilities are: \\[ P(Y=A \\mid X=x) = 0.2, \\] \\[ P(Y=B \\mid X=x) = 0.8/(1+e^{-x}), \\] To help you visualize this, here is a plot of \\(P(Y=B \\mid X=x)\\) vs \\(x\\) (notice that it is bounded above by 0.8, and below by 0). ggplot(tibble(x=c(-7, 7)), aes(x)) + stat_function(fun=function(x) 0.8/(1+exp(-x))) + ylim(c(0,1)) + geom_hline(yintercept=c(0,0.8), linetype=&quot;dashed&quot;, alpha=0.5) + theme_bw() + labs(y=&quot;P(Y=B|X=x)&quot;) Here’s an R function to generate data for you, where \\(X\\sim N(0,1)\\). As before, it accepts a positive integer as its input, representing the sample size, and returns a tibble with column names x and y corresponding to the predictor and response. gencla &lt;- function(n) { x &lt;- rnorm(n) pB &lt;- 0.8/(1+exp(-x)) y &lt;- map_chr(pB, function(t) sample(LETTERS[1:3], size=1, replace=TRUE, prob=c(0.2, t, 1-t-0.2))) tibble(x=x, y=y) } Calculate the probabilities of each category when \\(X=1\\). What about when \\(X=-2\\)? With this information, what would you classify \\(Y\\) as in both cases? BONUS: Plot these two conditional distributions. ## X=1: (pB &lt;- FILL_THIS_IN) (pA &lt;- FILL_THIS_IN) (pC &lt;- FILL_THIS_IN) ggplot(tibble(p=c(pA,pB,pC), y=LETTERS[1:3]), aes(y, p)) + geom_col() + theme_bw() + labs(y=&quot;Probabilities&quot;, title=&quot;X=1&quot;) ## X=-2 (pB &lt;- FILL_THIS_IN) (pA &lt;- FILL_THIS_IN) (pC &lt;- FILL_THIS_IN) ggplot(tibble(p=c(pA,pB,pC), y=LETTERS[1:3]), aes(y, p)) + geom_col() + theme_bw() + labs(&quot;Probabilities&quot;, title=&quot;X=-2&quot;) In general, when would you classify \\(Y\\) as A? B? C? 7.3.3 (BONUS) Random prediction You might think that, if we know the conditional distribution of \\(Y\\) given some predictors, why not take a random draw from that distribution as our prediction? After all, this would be simulating nature. The problem is, this prediction doesn’t do well. Re-do the regression exercise above (feel free to only do Case 1 to prove the point), but this time, instead of using the mean as a prediction, use a random draw from the conditional distributions. Calculate the MSE. How much worse is it? How does this error compare to the original Case 1-4 errors? 7.3.4 (BONUS) A more non-standard regression The regression example given above is your perfect, everything-is-linear-and-Normal world. Let’s see an example of a joint distribution of \\((X,Y)\\) that’s not Normal. The joint distribution in question can be respresented as follows: \\[ Y|X=x \\sim \\text{Beta}(e^{-x}, 1/x), \\] \\[ X \\sim \\text{Exp}(1). \\] Write a formula that gives a prediction of \\(Y\\) from \\(X\\) (you might have to look up the formula for the mean of a Beta random variable). Generate data, and evaluate the MSE. Plot the data, and the conditional mean as a function of \\(x\\) overtop. 7.3.5 (BONUS) Oracle MSE What statistical quantity does the mean squared error (MSE) reduce to when we know the true distribution of the data? Hint: if each conditional distribution has a certain variance, what then is the MSE? What is the error rate in the classification setting? "],
["the-signal-model-functions.html", "Chapter 8 The signal: model functions 8.1 Linear Quantile Regression", " Chapter 8 The signal: model functions suppressPackageStartupMessages(library(tidyverse)) ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 Wage &lt;- ISLR::Wage NCI60 &lt;- ISLR::NCI60 baseball &lt;- Lahman::Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) cow &lt;- suppressMessages(read_csv(&quot;data/milk_fat.csv&quot;)) esoph &lt;- as_tibble(esoph) %&gt;% mutate(agegp = as.character(agegp)) titanic &lt;- na.omit(titanic::titanic_train) 8.1 Linear Quantile Regression The idea here is to model \\[Q(\\tau)=\\beta_0(\\tau) + \\beta_1(\\tau) X_1 + \\cdots + \\beta_p(\\tau) X_p,\\] where \\(Q(\\tau)\\) is the \\(\\tau\\)-quantile. In other words, each quantile level gets its own line, and are each fit independently of each other. Here are the 0.25-, 0.5-, and 0.75-quantile regression lines for the baseball data: ggplot(baseball, aes(hits, runs)) + geom_point(alpha=0.1, colour=&quot;orange&quot;) + geom_quantile(colour=&quot;black&quot;) + theme_bw() + labs(x=&quot;Number of Hits (X)&quot;, y=&quot;Number of Runs (Y)&quot;) ## Loading required package: SparseM ## ## Attaching package: &#39;SparseM&#39; ## The following object is masked from &#39;package:base&#39;: ## ## backsolve ## Smoothing formula not specified. Using: y ~ x I did this easily with ggplot2, just by adding a layer geom_quantile to my scatterplot, specifying the quantile levels with the quantiles= argument. We could also use the function rq in the quantreg package in R: (fit_rq &lt;- quantreg::rq(runs ~ hits, data=baseball, tau=c(0.25, 0.5, 0.75))) ## Call: ## quantreg::rq(formula = runs ~ hits, tau = c(0.25, 0.5, 0.75), ## data = baseball) ## ## Coefficients: ## tau= 0.25 tau= 0.50 tau= 0.75 ## (Intercept) -118.8297872 8.2101818 64.0347349 ## hits 0.5531915 0.4923636 0.4908592 ## ## Degrees of freedom: 2835 total; 2833 residual If we were to again focus on the two teams (one with 1000 hits, and one with 1500 hits), we have (by evaluating the above three lines): predict(fit_rq, newdata=data.frame(hits=c(1000, 1500))) ## tau= 0.25 tau= 0.50 tau= 0.75 ## 1 434.3617 500.5738 554.8940 ## 2 710.9574 746.7556 800.3236 So, we could say that the team with 1000 hits: is estimated to have a 50% chance to have between 434 and 555 runs; has a 25% chance of achieving over 555 runs; has a 25% chance of getting less than 434 runs; would typically get 501 runs (median); amongst other things. 8.1.1 Exercise Get a 95% prediction interval using linear quantile regression, with Y=R (number of runs), X=H (number of hits), when X=1500. What about a 95% PI using kNN, going back to the earlier example we did? 8.1.2 Problem: Crossing quantiles Because each quantile is allowed to have its own line, some of these lines might cross, giving an invalid result. Here is an example with the iris data set, fitting the 0.2- and 0.3-quantiles: ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point(alpha=0.25, colour=&quot;orange&quot;) + geom_quantile(aes(colour=&quot;0.2&quot;), quantiles=0.2) + geom_quantile(aes(colour=&quot;0.3&quot;), quantiles=0.3) + scale_colour_discrete(&quot;Quantile\\nLevel&quot;) + theme_bw() + labs(x=&quot;Sepal Length&quot;, y=&quot;Sepal Width&quot;) ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x fit_iris &lt;- quantreg::rq(Sepal.Width ~ Sepal.Length, data=iris, tau=2:3/10) b &lt;- coef(fit_iris) at8 &lt;- round(predict(fit_iris, newdata=data.frame(Sepal.Length=8)), 2) Quantile estimates of Sepal Width for plants with Sepal Length less than 7.3 are valid, but otherwise, are not. For example, for plants with a Sepal Length of 8, this model predicts 30% of such plants to have a Sepal Width of less than 2.75, but only 20% of such plants should have Sepal Width less than 2.82. This is an illogical statement. There have been several “adjustments” proposed to ensure that this doesn’t happen (see below), but ultimately, this suggests an inadequacy in the model assumptions. Luckily, this usually only happens at extreme values of the predictor space, and/or for large quantile levels, so is usually not a problem. Bondell HD, Reich BJ, Wang H. Noncrossing quantile regression curve estimation. Biometrika. 2010;97(4):825-838. Dette H, Volgushev S. Non-crossing non-parametric estimates of quantile curves. J R Stat Soc Ser B Stat Methodol. 2008;70(3):609-627. Tokdar ST, Kadane JB. Simultaneous linear quantile regression: a semiparametric Bayesian approach. Bayesian Anal. 2011;6(4):1-22. 8.1.3 Problem: Upper quantiles Estimates of higher quantiles usually become worse for large/small values of \\(\\tau\\). This is especially true when data are heavy-tailed. Check out the Chapter on Extreme Value Regression for more info. "],
["the-model-fitting-paradigm-in-r.html", "Chapter 9 The Model-Fitting Paradigm in R 9.1 broom package", " Chapter 9 The Model-Fitting Paradigm in R Caution: in a highly developmental stage! See Section 1.1. suppressPackageStartupMessages(library(tidyverse)) ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 Wage &lt;- ISLR::Wage NCI60 &lt;- ISLR::NCI60 baseball &lt;- Lahman::Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) cow &lt;- suppressMessages(read_csv(&quot;data/milk_fat.csv&quot;)) esoph &lt;- as_tibble(esoph) %&gt;% mutate(agegp = as.character(agegp)) titanic &lt;- na.omit(titanic::titanic_train) Scratch notes from our in-class activities in Lecture 3. This covers the model-fitting paradigm in R. Use the iris data to demonstrate. Split into training and test data: head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa set.seed(100) iris &lt;- mutate( iris, training = caTools::sample.split(Sepal.Width, SplitRatio=0.8)) # training = sample(1:2, replace=TRUE, prob=c(0.8, 0.2), size=nrow(iris))) iris_train &lt;- filter(iris, training) iris_test &lt;- filter(iris, !training) #caret::createDataPartition() Fit a LOESS model: fit &lt;- loess(Sepal.Width ~ Petal.Width, data=iris_train) Make predictions: yhat &lt;- predict(fit, newdata=iris_test) Calculate error: mean((yhat - iris_test$Sepal.Width)^2) ## [1] 0.08020987 9.1 broom package #tidy(fit) #glance(fit) broom::augment(fit, newdata = iris_test) ## # A tibble: 30 x 8 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species training ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;lgl&gt; ## 1 4.9 3 1.4 0.2 setosa FALSE ## 2 4.6 3.4 1.4 0.3 setosa FALSE ## 3 4.9 3.1 1.5 0.1 setosa FALSE ## 4 5.1 3.8 1.5 0.3 setosa FALSE ## 5 5.2 3.5 1.5 0.2 setosa FALSE ## 6 4.7 3.2 1.6 0.2 setosa FALSE ## 7 4.8 3.1 1.6 0.2 setosa FALSE ## 8 4.9 3.6 1.4 0.1 setosa FALSE ## 9 4.4 3.2 1.3 0.2 setosa FALSE ## 10 4.8 3 1.4 0.3 setosa FALSE ## # … with 20 more rows, and 2 more variables: .fitted &lt;dbl&gt;, .se.fit &lt;dbl&gt; With linear regression: fit2 &lt;- lm(Sepal.Width ~ Petal.Width, data=iris_train) #unclass(fit2) #unclass(summary(fit2)) broom::tidy(fit2) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.34 0.0717 46.6 7.64e-78 ## 2 Petal.Width -0.218 0.0500 -4.36 2.78e- 5 broom::augment(fit2) ## # A tibble: 120 x 9 ## Sepal.Width Petal.Width .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.5 0.2 3.30 0.0634 0.203 0.0236 0.414 2.98e-3 ## 2 3.2 0.2 3.30 0.0634 -0.0973 0.0236 0.415 6.86e-4 ## 3 3.1 0.2 3.30 0.0634 -0.197 0.0236 0.414 2.82e-3 ## 4 3.6 0.2 3.30 0.0634 0.303 0.0236 0.414 6.64e-3 ## 5 3.9 0.4 3.25 0.0557 0.646 0.0182 0.410 2.31e-2 ## 6 3.4 0.2 3.30 0.0634 0.103 0.0236 0.415 7.64e-4 ## 7 2.9 0.2 3.30 0.0634 -0.397 0.0236 0.413 1.14e-2 ## 8 3.7 0.2 3.30 0.0634 0.403 0.0236 0.413 1.18e-2 ## 9 3.4 0.2 3.30 0.0634 0.103 0.0236 0.415 7.64e-4 ## 10 3 0.1 3.32 0.0675 -0.319 0.0267 0.414 8.42e-3 ## # … with 110 more rows, and 1 more variable: .std.resid &lt;dbl&gt; broom::glance(fit2) ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.139 0.132 0.413 19.0 2.78e-5 2 -63.1 132. 141. ## # … with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; "],
["estimating-parametric-model-functions.html", "Chapter 10 Estimating parametric model functions 10.1 Writing the sample mean as an optimization problem 10.2 Evaluating Model Goodness: Quantiles 10.3 Simple Linear Regression 10.4 Linear models in general 10.5 reference-treatment parameterization", " Chapter 10 Estimating parametric model functions Caution: in a highly developmental stage! See Section 1.1. suppressPackageStartupMessages(library(tidyverse)) ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 Wage &lt;- ISLR::Wage NCI60 &lt;- ISLR::NCI60 baseball &lt;- Lahman::Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) cow &lt;- suppressMessages(read_csv(&quot;data/milk_fat.csv&quot;)) esoph &lt;- as_tibble(esoph) %&gt;% mutate(agegp = as.character(agegp)) titanic &lt;- na.omit(titanic::titanic_train) 10.1 Writing the sample mean as an optimization problem (DSCI 561 lab2, 2018-2019) It’s important to know that the sample mean can also be calculated by finding the value that minimizes the sum of squared errors with respect to that value. We’ll explore that here. Store some numbers in the vector y. Calculate the sample mean of the data, stored in mu_y. This is not worth any marks, but having it as its own question jibes better with the autograder. We’ve defined sse() below, a function that takes some number and returns the sum of squared “errors” of all values of y with respect to the inputted number. An “error” is defined as the difference between two values. We’ve also generated a quick plot of this function for you. sse &lt;- Vectorize(function(m) sum((y - m)^2)) curve(sse, mu_y - 2*sd(y), mu_y + 2*sd(y)) Your task: use the optimize() function to find the value that minimizes the sum of squared errors. Hint: for the interval argument, specify an interval that contains the sample mean. Important points: You should recognize that the sample mean minimizes this function! You’ll be seeing the sum of squared errors a lot through the program (mean squared error and R^2 are based on it). This is because the mean is a very popular quantity to model and use as a prediction. If you’re not convinced, play with different numbers to see for yourself. 10.2 Evaluating Model Goodness: Quantiles The question here is: if we have two or more models that predicts the \\(\\tau\\)-quantile, which model is best? We’ll need some way to score different models to do things such as: Choose which predictors to include in a model; Choose optimal hyperparameters; Estimate parameters in a quantile regression model. **NOTE**: Mean Squared Error is not appropriate here!! This is very important to remember. The reason is technical – the MSE is not a proper scoring rule for quantiles. In other words, the MSE does not elicit an honest prediction. If we’re predicting the median, then the mean absolute error works. This is like the MSE, but instead of squaring the errors, we take the absolute value. In general, a “correct” scoring rule for the \\(\\tau\\)-quantile is as follows: \\[ S = \\sum_{i=1}^{n} \\rho_{\\tau}(Y_i - \\hat{Q}_i(\\tau)), \\] where \\(Y_i\\) for \\(i=1,\\ldots,n\\) is the response data, \\(\\hat{Q}_i(\\tau)\\) are the \\(\\tau\\)-quantile estimates, and \\(\\rho_{\\tau}\\) is the check function (also known as the absolute asymmetric deviation function or tick function), given by \\[ \\rho_{\\tau}(s) = (\\tau - I(s&lt;0))s \\] for real \\(s\\). This scoring rule is negatively oriented, meaning the lower the score, the better. It cannot be below 0. Here is a plot of various check functions. Notice that, when \\(\\tau=0.5\\) (corresponding to the median), this is proportional to the absolute value: base &lt;- ggplot(data.frame(x=c(-2,2)), aes(x)) + theme_bw() + labs(y=expression(rho)) + theme(axis.title.y=element_text(angle=0, vjust=0.5)) + ylim(c(0, 1.5)) rho &lt;- function(tau) function(x) (tau - (x&lt;0))*x cowplot::plot_grid( base + stat_function(fun=rho(0.2)) + ggtitle(expression(paste(tau, &quot;=0.2&quot;))), base + stat_function(fun=rho(0.5)) + ggtitle(expression(paste(tau, &quot;=0.5&quot;))), base + stat_function(fun=rho(0.8)) + ggtitle(expression(paste(tau, &quot;=0.8&quot;))), ncol=3 ) ## Warning: Removed 4 rows containing missing values (geom_path). ## Warning: Removed 4 rows containing missing values (geom_path). For quantile regression estimation, we minimize the sum of scores instead of the sum of squared residuals, as in the usual (mean) linear regression. 10.3 Simple Linear Regression (From lab2, DSCI 561, 2018-2019) When a predictor is categorical, it’s easy to estimate the mean given a certain predictor value (i.e., given the category): just take the sample average of the data in that group. Now let’s consider a numeric predictor. Using the iris dataset again with sepal width as a response, use sepal length as the predictor. Here is a scatterplot of the data: (p_numeric_x &lt;- ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point(alpha=0.25) + theme_bw() + labs(x = &quot;Sepal Length&quot;, y = &quot;Sepal Width&quot;)) How can we estimate the mean sepal width (\\(Y\\)) for any given sepal length (\\(X\\))? Say we want the mean of \\(Y\\) at \\(X=x\\) (for some pre-decided \\(x\\)). Last week in DSCI 571 Lab 2 Exercise 5, you saw one way of estimating this: calculate the mean sepal width (\\(Y\\)) using only the \\(k\\) plants having sepal lengths (\\(X\\) values) closest to \\(x\\) (the sepal length you’re interested in). Methods like this are very powerful estimation methods, but there’s merit in assuming the mean is linear in \\(x\\): \\[E(Y \\mid X=x) = \\beta_0 + \\beta_1 x,\\] for some numbers \\(\\beta_0\\) and \\(\\beta_1\\) (to be estimated). How do we estimate \\(\\beta_0\\) and \\(\\beta_1\\)? In other words, how do we pick an acceptable line? Since we want the line to represent the mean, choose the line that minimizes the sum of squared errors – remember, this is another way of writing the sample average in the univariate case, and now we can generalize the univariate mean to the regression setting in this way. Is it possible to find a line that has a smaller sum of squared errors than what you found in Exercise 3.3? Why or why not? Is it possible to find a line that has a smaller sum of absolute errors (i.e., the absolute value of the errors)? Elaborate. 10.3.1 Model Specification You might see linear regression models specified in different ways. In this exercise, we’re still working with sepal length as the only predictor of sepal width. Denote \\(\\beta_0\\) as the true intercept of the regression line, and \\(\\beta_1\\) as the true slope. As we’ve said, we’re assuming that the mean of \\(Y\\) is linear in the predictor: \\[E(Y \\mid X=x) = \\beta_0 + \\beta_1 x.\\] There are other ways to write this model; i.e., different ways of saying the same thing (not to be confused with different parameterizations). We’ll explore this here. 4.1 rubric={reasoning:3} One way to write this model is to emphasize that this model holds for every single observation, instead of for a generic \\(Y\\). Denote \\(Y_i\\) as the random variable corresponding to the \\(i\\)’th observation of the response, and \\(x_i\\) the corresponding observed value of the predictor. Let \\(n\\) be the sample size. Your task: specify what goes in the \\(?\\) in the following equation: \\[E(Y_i | X_i = x_i) = \\text{ ?}, \\text{ for each } i=1,\\ldots,n.\\] YOUR ANSWER HERE 4.2 rubric={reasoning:3} We could also specify how \\(Y_i\\) itself was supposedly calculated. Your task: specify what goes in the \\(?\\) in the following equation. \\[Y_i = \\text{ ?}, \\text{ for each } i=1,\\ldots,n.\\] Hint: you’ll have to introduce a variable. Be sure to specify any assumptions about this variable so that your equation is equivalent to the one in Exercise 4.1 – this means not putting more assumptions than are necessary, too! YOUR ANSWER HERE 4.3 rubric={reasoning:3} Instead of having to say “for each \\(i=1,\\ldots,n\\)”, we could just write out each of the \\(n\\) equations. It’s actually convenient to do so, but expressed as one equation by using matrix algebra. We’ll use bold-face to denote vectors. Denote \\(\\boldsymbol{Y}\\) as the vector containing \\(Y_1,\\ldots,Y_n\\) (in that order), and similarly for \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{x}\\). Denote \\(\\boldsymbol{\\beta}\\) as the vector containing \\(\\beta_0\\) and \\(\\beta_1\\) (in that order). Then, the same equation becomes: \\[E(\\boldsymbol{Y} \\mid \\boldsymbol{X} = \\boldsymbol{x}) = \\text{? }\\boldsymbol{\\beta}, \\] where “?” is an \\(n \\times 2\\) matrix. Your task: specify the matrix indicated by “?” in the above equation. It’s probably most convenient to describe what each column contains. Each column is worth approx. 50% of your grade for this question. YOUR ANSWER HERE 10.4 Linear models in general Caution: in a highly developmental stage! See Section 1.1. (DSCI 561 lab 2, 2018-2019) In general, linear models estimate the mean using \\(p\\) predictors \\(X_1, \\ldots, X_p\\) (this time, the subscripts denote “predictor number” instead of “observation number”, and the vectors \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{x}\\) contain the predictors, not the observations), according to the following generic equation: \\[E(Y \\mid \\boldsymbol{X}=\\boldsymbol{x}) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p.\\] We saw that: a \\(K\\)-level categorical predictor enters the equation through \\(K-1\\) binary predictors (relative to a “baseline” category), and a numeric predictor enters the equation as itself. We will now consider using both sepal length (numeric) and species (categorical) as predictors of sepal width. 6.1 Fit a linear regression line to sepal length (\\(X\\)) vs. sepal width (\\(Y\\)) for each species independently. Plot the results by facetting by species. Note that all we’re looking for here is the plot. You can bypass the lm() calls by adding the layer geom_smooth(method=“lm”, se=FALSE), which runs the linear regression separately in each panel. Although these look like three separate models, it’s still just one model: one specification as to how to estimate the mean. We can write a single equation that describes this specification, using the following variables: \\[X_1 = \\text{sepal length}\\]\\[X_2 = 1 \\text{ if versicolor, } 0, \\text{ otherwise}\\]\\[X_3 = 1 \\text{ if virginica, } 0, \\text{ otherwise}\\]\\[X_4 = X_2 X_1\\]\\[X_5 = X_3 X_1\\] The model becomes: \\[E(Y \\mid \\boldsymbol{X} = \\boldsymbol{x}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + \\beta_5 X_5 \\] Your task: specify the slope and intercept of the regression lines for each species, in terms of the \\(X\\)’s and \\(\\beta\\)’s above. We’ve given you the answer for Setosa already. Answer by copying and pasting the below table into the answer cell, and filling in the missing table cells. Hint: evaluate whatever \\(X\\)’s you can. Species Intercept Slope Setosa \\(\\beta_0\\) \\(\\beta_1\\) Versicolor Virginica YOUR ANSWER HERE 6.3 \\(X_4\\) and \\(X_5\\) are called interaction terms, and are not present by default in the lm() function. In their absence, what are the slopes and intercepts of the regression line for each species? Answer like you did above, in terms of the \\(X\\)’s and \\(\\beta\\)’s, by filling in the below table. We’ve given you the answer for Setosa already. Species Intercept Slope Setosa \\(\\beta_0\\) \\(\\beta_1\\) Versicolor Virginica YOUR ANSWER HERE 6.4 Make a similar plot as in Exercise 6.1, but for the model without interaction. 10.5 reference-treatment parameterization Caution: in a highly developmental stage! See Section 1.1. (From DSCI 561 lab1, 2018-2019) When data fall into groups, you already know how to estimate the population mean for each group: calculate the sample mean of the data in each group. For example, the mean Sepal.Width of the three species in the iris dataset are: iris %&gt;% group_by(Species) %&gt;% summarize(mean_sepal_width = mean(Sepal.Width)) In this exercise, you’ll start exploring a “trick” that linear regression models use so that we can obtain the same mean estimates by evaluating a line. Although it might seem silly to do, especially when we can just do a group_by() and summarize(), using this trick will be very important when we start incorporating more variables in our model, as we’ll soon see in this course. Next, using the “trick” that linear regression models use, you’ll write these estimates as a line, so that we can obtain the mean estimates by evaluating the line. Here’s the trick: convert the categories to a numeric variable, where one category takes the value 0, and the other takes the value 1. Let’s convert “first” to 0, and “other” to 1: x_map &lt;- c(first=0, other=1) preg &lt;- mutate(preg, num_x = x_map[birth_order]) head(preg) What’s the equation of the line that goes through the mean estimates of both groups? Specify this by storing the slope and (y-) intercept of the line in the variables preg_slope and preg_int, respectively. This line is called the regression line. In Inf-1 lab3, you made a plot of the data, including the two means with a confidence interval. Here’s the code (using asymptotics to form the CI), zooming in on the “center” of the data (uncomment preg_plot to view the plot): preg_plot &lt;- preg %&gt;% group_by(birth_order, num_x) %&gt;% summarize(mean = mean(prglngth), n = length(prglngth), se = sd(prglngth) / sqrt(n)) %&gt;% ggplot(aes(x = num_x)) + geom_violin(data = preg, mapping = aes(y = prglngth, group = birth_order), adjust = 5) + geom_point(aes(y = mean), colour = &quot;red&quot;) + geom_errorbar(aes(ymin = mean + qnorm(alpha/2)*se, ymax = mean - qnorm(alpha/2)*se), colour = &quot;red&quot;, width = 0.2) + theme_bw() + labs(x = &quot;Birth Order&quot;, y = &quot;Pregnancy Length (weeks)&quot;) + ylim(c(30, 50)) + scale_x_continuous(breaks = enframe(x_map)$value, labels = enframe(x_map)$name) Add the line to this plot. Can we always draw a straight line through the means of two groups? Why or why not? In this lab, we won’t be exploring the trick that linear regression models use when we have multiple groups. But, you’ll explore what we can’t do. For each species in the iris (three-group) data set, the code below: calculates the mean sepal width in the column mean_sepwid, along with the standard error of the mean in the se column, placed in the data frame iris_est. plots the raw data with a violin+jitter plot, stored in the variable iris_plot (uncomment it to view it). (iris_est &lt;- iris %&gt;% group_by(Species) %&gt;% summarize( mean_sepwid = mean(Sepal.Width), se = sd(Sepal.Width)/sqrt(length(Sepal.Width)) )) iris_plot &lt;- iris %&gt;% mutate(Species = fct_reorder(Species, Sepal.Width)) %&gt;% ggplot(aes(Species)) + geom_violin(aes(y = Sepal.Width)) + geom_jitter(aes(y = Sepal.Width), alpha=0.2, width=0.1, size=0.5) + theme_bw() + labs(x = &quot;Species&quot;, y = &quot;Sepal Width&quot;) iris_plot Your task is to add the group means and confidence intervals to the plot. You can do this by adding layers to iris_plot. You can use asymptotic theory to calculate the confidence intervals, calculated by: \\[\\bar{x} \\pm z_{\\alpha/2} \\text{SE}.\\] Can we fit a single straight line through the mean sepal widths across the three species groups? Why or why not? 10.5.1 More than one category (Lab 2) In class, we saw two “ways to store information” about groups means – in technical terms, two parameterizations. The first and most “direct” parameterization is cell-wise parameterization, a fancy way of saying that we’re just going to consider the raw means themselves: one mean for each group. For the three species in the iris dataset, here are estimates of these parameters: iris %&gt;% group_by(Species) %&gt;% summarize(mean_sepal_width = mean(Sepal.Width)) The mean of the response (conditional on species) can be written as a linear model, if we call the above means \\(\\mu_0,\\mu_1,\\mu_2\\) (respectively), and define the following three predictors: \\[X_0 = 1 \\text{ if setosa, } 0 \\text{ otherwise},\\] \\[X_1 = 1 \\text{ if versicolor, } 0 \\text{ otherwise},\\] \\[X_2 = 1 \\text{ if virginica, } 0 \\text{ otherwise}.\\] Then, the linear model is \\[E(\\text{Sepal Width} \\mid \\text{species}) = \\mu_0 X_0 + \\mu_1 X_1 + \\mu_2 X_2.\\] In this exercise, you’ll be exploring another parameterization that’s useful in linear regression: the reference-treatment parameterization. 1.1 To keep things different from lm(), let’s consider the virginica species as our “reference”. The reference-treatment parameterization is then: \\[\\theta=\\mu_{\\text{virginica}},\\] \\[\\tau_1=\\mu_{\\text{versicolor}}-\\theta,\\] \\[\\tau_2=\\mu_{\\text{setosa}}-\\theta,\\] where \\(\\mu\\) denotes the mean of that species’ sepal width. Your task: Calculate estimates of these parameters, and store the estimates in the (respective) variables theta, tau1, and tau2. Provide an interpretation for \\(\\theta\\), \\(\\tau_1\\), and \\(\\tau_2\\). One brief sentence for each is enough. Let’s now write this information as a single (linear) equation containing: two predictors \\(X_1\\) and \\(X_2\\), and the parameters \\(\\theta\\), \\(\\tau_1\\), and \\(\\tau_2\\). Let’s focus on the predictors first. Define: \\[X_1 = 1 \\text{ if versicolor, } 0 \\text{ otherwise},\\] \\[X_2 = 1 \\text{ if setosa, } 0 \\text{ otherwise}.\\] We’ve deliberately not defined a predictor for virginica, the reference group. Your task: What are the values of \\(X_1\\) and \\(X_2\\) for each species? Store the values in three length-2 vectors called x_setosa, x_versicolor, and x_virginica. Use the predictors \\(X_1\\) and \\(X_2\\), along with the parameters \\(\\theta\\), \\(\\tau_1\\), and \\(\\tau_2\\), to write a linear equation that returns the species mean \\(\\mu\\). The equation should look like: \\[E(Y \\mid \\text{species}) = (1) + (2)\\times(3) + (4)\\times(5)\\] To use the autograder for this question, specify the order that \\(X_1\\), \\(X_2\\), \\(\\theta\\), \\(\\tau_1\\), and \\(\\tau_2\\) are to appear in the equation, respectively, in a vector containing the numbers 1 through 5, named eq_order. For example, specifying eq_order &lt;- c(1,2,3,4,5) corresponds to the equation \\(E(Y \\mid \\text{species}) = X_1 + X_2 \\theta + \\tau_1 \\tau_2\\) (which is not the correct equation) (Based on your answers to 1.4 and 1.5, can you see why the parameter that goes in place of (1) is also called the “intercept”?) Now try using lm(): use the iris data with the same predictor and response (don’t include -1 in the formula, so that you end up with a reference-treatment parameterization). Your task: What’s the reference species? Put the name of the species as a character in the variable iris_lm_ref_species. "],
["estimating-assumption-free-the-world-of-supervised-learning-techniques.html", "Chapter 11 Estimating assumption-free: the world of supervised learning techniques 11.1 What machine learning is 11.2 Types of Supervised Learning 11.3 Local Regression 11.4 Splines and Loess Regression", " Chapter 11 Estimating assumption-free: the world of supervised learning techniques Caution: in a highly developmental stage! See Section 1.1. suppressPackageStartupMessages(library(tidyverse)) ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 Wage &lt;- ISLR::Wage NCI60 &lt;- ISLR::NCI60 baseball &lt;- Lahman::Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) cow &lt;- suppressMessages(read_csv(&quot;data/milk_fat.csv&quot;)) esoph &lt;- as_tibble(esoph) %&gt;% mutate(agegp = as.character(agegp)) titanic &lt;- na.omit(titanic::titanic_train) 11.1 What machine learning is What is Machine Learning (ML) (or Statistical Learning)? As the ISLR book puts it, it’s a “vast set of tools for understanding data”. Before we explain more, we need to consider the two main types of ML: Supervised learning. (This is the focus of BAIT 509). Consider a “black box” that accepts some input(s), and returns some type of output. Feed it a variety of input, and write down the output each time (to obtain a data set). Supervised learning attempts to learn from these data to re-construct this black box. That is, it’s a way of building a forecaster/prediction tool. You’ve already seen examples throughout MBAN. For example, consider trying to predict someone’s wage (output) based on their age (input). Using the Wage data set from the ISLR R package, here are examples of inputs and outputs: ## age wage ## 1 18 75.04315 ## 2 24 70.47602 ## 3 45 130.98218 ## 4 43 154.68529 ## 5 50 75.04315 ## 6 54 127.11574 We try to model the relationship between age and wage so that we can predict the salary of a new individual, given their age. An example supervised learning technique is linear regression, which you’ve seen before in BABS 507/508. For an age x, let’s use linear regression to make a prediction that’s quadratic in x. Here’s the fit: The blue curve represents our attempt to “re-construct” the black box by learning from the existing data. So, for a new individual aged 70, we would predict a salary of about $100,000. A 50-year-old, about $125,000. Unsupervised learning. (BAIT 509 will not focus on this). Sometimes we can’t see the output of the black box. Unsupervised learning attempts to find structure in the data without any output. For example, consider the following two gene expression measurements (actually two principal components). Are there groups that we can identify here? You’ve seen methods for doing this in BABS 507/508, such as k-means. 11.2 Types of Supervised Learning There are two main types of supervised learning methods – determined entirely by the type of response variable. Regression is supervised learning when the response is numeric. Classification is supervised learning when the response is categorical. We’ll examine both equally in this course. Note: Don’t confuse classification with clustering! The latter is an unsupervised learning method. 11.3 Local Regression Caution: in a highly developmental stage! See Section 1.1. (BAIT 509 Class Meeting 03) Let’s turn our attention to the first “new” machine learning methods of the course: \\(k\\) Nearest Neighbours (aka kNN or \\(k\\)-NN) and loess (aka “LOcal regrESSion”). The fundamental idea behind these methods is to base your prediction on what happened in similar cases in the past. 11.3.1 kNN Pick a positive integer \\(k\\). To make a prediction of the response at a particular observation of the predictors (I’ll call this the query point) – that is, when \\(X_1=x_1\\), …, \\(X_p=x_p\\): Subset your data to \\(k\\) observations (rows) whose values of the predictors \\((X_1, \\ldots, X_p)\\) are closest to \\((x_1,\\ldots,x_p)\\). For kNN classificiation, use the “most popular vote” (i.e., the modal category) of the subsetted observations. For kNN regression, use the average \\(Y\\) of the remaining subsetted observations. Recall how to calculate distance between two vectors \\((a_1, \\ldots, a_p)\\) and \\((b_1, \\ldots, b_p)\\): \\[ \\text{distance} = \\sqrt{(a_1-b_1)^2 + \\cdots + (a_p-b_p)^2}. \\] It’s even easier when there’s one predictor: it’s just the absolute value of the difference. 11.3.2 loess (This is actually the simplest version of loess, sometimes called a moving window approach. We’ll get to the “full” loess). Pick a positive number \\(r\\) (not necessarily integer). To make a prediction of the response at a query point (that is, a particular observation of the predictors, \\(X_1=x_1\\), …, \\(X_p=x_p\\)): Subset your data to those observations (rows) having values of the predictors \\((X_1,\\ldots,X_p)\\) within \\(r\\) units of \\((x_1,\\ldots,x_p)\\). For kNN classificiation, use the “most popular vote” (i.e., the modal category) of the subsetted observations. For kNN regression, use the average \\(Y\\) of the remaining subsetted observations. Notice that Step 2 is the same as in kNN. \\(k\\) and \\(r\\) are called hyperparameters, because we don’t estimate them – we choose them outright. 11.3.3 In-Class Exercises Consider the following data set, given by dat. Here’s the top six rows of data: set.seed(87) dat &lt;- tibble(x = c(rnorm(100), rnorm(100)+5)-3, y = sin(x^2/5)/x + rnorm(200)/10 + exp(1)) Here’s a scatterplot of the data: ggplot(dat, aes(x,y)) + geom_point(colour=&quot;orange&quot;) + theme_bw() + rotate_y 11.3.3.1 Exercise 1: Mean at \\(X=0\\) Let’s check your understanding of loess and kNN. Consider estimating the mean of \\(Y\\) when \\(X=0\\) by using data whose \\(X\\) values are near 0. Eyeball the above scatterplot of the data. What would you say is a reasonable estimate of the mean of \\(Y\\) at \\(X=0\\)? Why? Estimate using loess and kNN (you choose the hyperparameters). Hints for kNN: First, add a new column in the data that stores the distance between \\(X=0\\) and each observation. If that column is named d, you can do this with the following partial code: dat$d &lt;- YOUR_CALCULATION_HERE. Recall that dat$x is a vector of the x column. Then, arrange the data from smallest distance to largest with arrange(dat) (you’ll need to load the tidyverse package first), and subset that to the first \\(k\\) rows. Hints for loess: Subset the data using the filter function. The condition to filter on: you want to keep rows whose distances (d) are … k &lt;- 10 r &lt;- 0.5 x0 &lt;- 0 dat$dist &lt;- FILL_THIS_IN dat &lt;- arrange(dat, dist) # sort by distance kNN_prediction &lt;- FILL_THIS_IN loess_prediction &lt;- FILL_THIS_IN What happens when you try to pick an \\(r\\) that is way too small? Say, \\(r=0.01\\)? Why? There’s a tradeoff between choosing large and small values of either hyperparameter. What’s good and what’s bad about choosing a large value? What about small values? 11.3.3.2 Exercise 2: Regression Curve Form the regression curve / model function by doing the estimation over a grid of x values, and connecting the dots: xgrid &lt;- seq(-5, 4, length.out=1000) k &lt;- 10 r &lt;- 0.5 kNN_estimates &lt;- map_dbl(xgrid, function(x_){ dat %&gt;% mutate(d = abs(x-x_)) %&gt;% arrange(d) %&gt;% summarize(yhat=mean(y[1:k])) %&gt;% `[[`(&quot;yhat&quot;) }) loess_estimates &lt;- map_dbl(xgrid, function(x_){ dat %&gt;% mutate(d = abs(x-x_)) %&gt;% filter(d&lt;r) %&gt;% summarize(yhat=mean(y)) %&gt;% `[[`(&quot;yhat&quot;) }) est &lt;- tibble(x=xgrid, kNN=kNN_estimates, loess=loess_estimates) %&gt;% gather(key=&quot;method&quot;, value=&quot;estimate&quot;, kNN, loess) ggplot() + geom_point(data=dat, mapping=aes(x,y)) + geom_line(data=est, mapping=aes(x,estimate, group=method, colour=method), size=1) + theme_bw() Exercises: Play with different values of \\(k\\) and \\(r\\), and regenerate the plot each time. What effect does increasing these values have on the regression curve? What about decreasing? What would you say is a “good” choice of \\(k\\) and \\(r\\), and why? What happens when you choose \\(k=n=200\\)? What happens if you choose \\(r=10\\) or bigger? The phenomenon you see when \\(k\\) and \\(r\\) are very small is called overfitting. This means that your model displays patterns that are not actually present. Underfitting, on the other hand, is when your model misses patterns in the data that are actually present. 11.3.4 Hyperparameters and the bias/variance tradeoff Let’s look at the bias and variance for different values of the hyperparameter in loess. set.seed(400) N &lt;- 100 xgrid &lt;- seq(-7, 6.5, length.out=300) true_mean &lt;- Vectorize(function(x){ if (x==0) return(exp(1)) else return(sin(x^2/5)/x + exp(1)) }) ## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop expand.grid(iter=1:N, r=c(0.5, 1, 2, 4)) %&gt;% group_by(iter, r) %&gt;% do({ this_r &lt;- unique(.$r) dat &lt;- tibble(x = c(rnorm(100), rnorm(100)+5)-3, y = sin(x^2/5)/x + rnorm(200)/10 + exp(1)) data.frame( ., x = xgrid, yhat = ksmooth(dat$x, dat$y, kernel=&quot;box&quot;, bandwidth=this_r, x.points=xgrid)$y ) }) %&gt;% ungroup() %&gt;% mutate(r=paste0(&quot;bandwidth=&quot;, r)) %&gt;% ggplot(aes(x=x, y=yhat)) + facet_wrap(~ r, ncol=1) + geom_line(aes(group=iter, colour=&quot;Estimates&quot;), alpha=0.1) + stat_function(fun=true_mean, mapping=aes(colour=&quot;True mean&quot;)) + theme_bw() + scale_colour_brewer(&quot;&quot;, palette=&quot;Dark2&quot;) + ylab(&quot;y&quot;) + rotate_y ## Warning: Removed 3240 rows containing missing values (geom_path). You can see the bias/variance tradeoff here: Notice that the estimates get narrower as the bandwidth increases – this means the variance reduces as the bandwidth increases. Notice that the estimates become biased as the bandwidth increases. A similar phenomenon exists with kNN regression. Notice some other things about these plots: There’s more variance whenever there’s less data – that’s at the tails, and (by design) at around \\(X=0\\). Estimates don’t exist sometimes, if no data fall in the “window”. You can see that the tails are cut short when the bandwidth is small. 11.3.5 Extensions to kNN and loess 11.3.5.1 Kernel weighting kNN and loess can be generalized by downweighing points that are further from the query point. In particular, we take a weighted average. Suppose \\(y_1, \\ldots, y_n\\) are \\(n\\) realizations of the response \\(Y\\). If we assign (respective) weights \\(w_1, \\ldots, w_n\\) to these realizations, then the weighted average is \\[ \\frac{\\sum_{i=1}^n w_i y_i}{\\sum_{i=1}^n w_i}. \\] If the response is categorical, then each subsetted observation gives a “weighted vote”. Sum up the weights corresponding to each category to obtain the total “number” of votes. We obtain these weights using a kernel function. A kernel function is any non-increasing function over the positive real numbers. Plug in the distance between the observed predictor(s) and the query point to obtain the weight. Some examples of kernel functions are plotted below. 11.3.5.2 Local polynomials Another extension of loess is to consider local polynomials. The idea here is, after subsetting the data lying within \\(r\\) units of the query point, add the following two steps: Fit a linear (or quadratic) regression model to the subsetted data only. This is the “local polynomial”. You can think of this as like a “mini linear regression”. Obtain your prediction by evaluating the regression curve at the query point. Throw away your fitted local polynomial. OK, the 3rd step isn’t really a true step, but I like to include it to emphasize that we only evaluate the local polynomial at the query point. Note: We could fit higher order polynomials, but that tends to overfit the data. We could fit any other curve locally besides a polynomial, but polynomials are justified by the Taylor approximation. Local polynomials with degree=0 is the same as “not doing” local polynomials. 11.3.5.3 Combination You can combine kernel weighting with local polynomials. When you fit the local polynomial to the subsetted data, you can run a weighted regression. Instead of minimizing the sum of squared errors, we minimize the weighted sum of squared errors. 11.3.5.4 Other distances We don’t have to use the “usual” notion of distance. The formula I gave you earlier (above) is called the Euclidean distance, or L2 norms. There’s also the L1 norm (also called the manhattan distance, which is distance by moving along the axes/rectangularly) and L0 norm (number of predictors having non-zero univariate distance). 11.3.5.5 Scaling When you’re using two or more predictors, your predictors might be on different scales. This means distances aren’t weighed equally, depending on the direction. Instead of measuring distance on the original scale of the predictors, consider re-scaling the predictors by subtracting the mean and dividing by the standard deviation for each predictor. 11.3.5.6 Demonstration Let’s look at the same example, but with kernel downweighting and local polynomials. Warning! The “bandwidth” hyperparameter in this plot is parameterized differently than in the previous plot, but carries the same interpretation. set.seed(400) N &lt;- 100 xgrid &lt;- seq(-7, 6.5, length.out=300) true_mean &lt;- Vectorize(function(x){ if (x==0) return(exp(1)) else return(sin(x^2/5)/x + exp(1)) }) ## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop expand.grid(iter=1:N, r=c(0.1, 0.5, 0.7), d=0:2) %&gt;% group_by(iter, r, d) %&gt;% do({ this_r &lt;- unique(.$r) this_d &lt;- unique(.$d) dat &lt;- tibble(x = c(rnorm(100), rnorm(100)+5)-3, y = sin(x^2/5)/x + rnorm(200)/10 + exp(1)) data.frame( ., x = xgrid, yhat = predict(loess(y~x, data=dat, span=this_r, degree=this_d), newdata=data.frame(x=xgrid)) ) }) %&gt;% ungroup() %&gt;% mutate(r=paste0(&quot;bandwidth=&quot;, r), d=paste0(&quot;degree=&quot;, d)) %&gt;% ggplot(aes(x=x, y=yhat)) + facet_grid(r ~ d) + geom_line(aes(group=iter, colour=&quot;Estimates&quot;), alpha=0.1) + stat_function(fun=true_mean, mapping=aes(colour=&quot;True mean&quot;)) + theme_bw() + scale_colour_brewer(&quot;&quot;, palette=&quot;Dark2&quot;) + ylab(&quot;y&quot;) + rotate_y ## Warning: Removed 8034 rows containing missing values (geom_path). Notice: For small bandwidth, increasing the degree of the poynomial just results in more variance – degree=0 looks best for this bandwidth. But by increasing the degree (inc. variance, dec. bias) and increasing the bandwidth (dec. variance, inc. bias), we end up getting an overall better fit: low bias, low variance. Bandwidth=0.5 and Degree=2 here seem to work best. 11.3.6 Model assumptions and the bias/variance tradeoff Recall that we saw an incorrect model assumption leads to bias, such as fitting linear regression when the true mean is non-linear. When you make model assumptions that are close to the truth, then this has the effect of decreasing variance. Adding good model assumptions is like adding more data – after all data is information, and a model assumption is also information. Here’s a demonstration: Consider \\[ Y = X + \\varepsilon, \\] where \\(X\\) (predictor) is N(0,1), and \\(\\varepsilon\\) (error term) is also N(0,1) (both are independent). I’ll generate a sample of size 100, 100 times. For each sample, I’ll fit a linear regression model and a loess model. Here are the resulting 100 regression curves for each (the dashed line is the true mean): set.seed(474) n &lt;- 100 N &lt;- 100 xgrid &lt;- data.frame(x=seq(-4,4, length.out=100)) ## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop tibble(iter=1:N) %&gt;% group_by(iter) %&gt;% do({ dat &lt;- tibble(x=rnorm(n), y=x+rnorm(n)) data.frame( ., xgrid, Local = predict(loess(y~x, data=dat), newdata=xgrid), Linear = predict(lm(y~x, data=dat), newdata=xgrid) ) }) %&gt;% gather(key=&quot;method&quot;, value=&quot;Prediction&quot;, Local, Linear) %&gt;% ggplot(aes(x=x, y=Prediction)) + facet_wrap(~ method) + geom_line(aes(group=iter), colour=&quot;orange&quot;, alpha=0.1) + geom_abline(intercept=0, slope=1, linetype=&quot;dashed&quot;) + theme_bw() ## Warning: Removed 1869 rows containing missing values (geom_path). Notice that the local method has higher variance than the linear regression method. 11.4 Splines and Loess Regression Caution: in a highly developmental stage! See Section 1.1. This tutorial describes spline and loess regression in R. You can think of splines as regression between knots. We’ll use the splines package to do this. Here’s some generated data: x &lt;- rnorm(1000) y &lt;- x^2 + rnorm(1000) qplot(x, y, alpha=I(0.5)) + theme_bw() First, we need to “set up” the regression by placing knots. x2 &lt;- splines::ns(x, knots=c(-2, 0, 2)) Now we can do regression between these knots, with the natural spline shape (as opposed to linear): fit &lt;- lm(y ~ x2) qplot(x, y, alpha=I(0.5)) + geom_line(data=data.frame(x=x, y=predict(fit)), mapping=aes(x, y), colour=&quot;blue&quot;) + theme_bw() 11.4.1 Loess Loess is “local regression”, and is based on the idea of estimating the mean response based on similar observed data in the predictor space. Kernel smoothing is a method that estimates the mean using a weighted sample average, where observations that are further away in the predictor space get downweighted more, according to some kernel function. Local polynomials is a method that takes kernel smoothing one step further: instead of a weighted sample average, we use nearby data (in the predictor space) to fit a polynomial regression, and then fit a polynomial regression model. There is a more basic version, too: a “moving window”, described next, before seeing how loess is done in R. 11.4.1.1 The “Moving Window” The “moving window” approach is a special type of kernel smoother. For a given value of the predictor \\(X=x\\), the mean response is estimated as the sample average of all response values whose predictor values are “near” \\(x\\) – within some distance \\(h\\). For example, if you want to estimate the mean number of “runs” of a baseball team when walks=100 and hits=1000, only look at cases where “walks” is approximately 100, “hits” is approximately 1000, and then average the response. It’s a special case of kernel regression, with a kernel function of \\[ k\\left(t\\right) = I\\left(|t-x| &lt; h \\right), \\] sometimes called the “boxcar” function. Note the similarity to kNN! kNN regression uses the nearest \\(k\\) points, resulting in a variable distance \\(h\\), whereas the moving window regression uses a fixed distance \\(h\\), resulting in a variable number of points \\(k\\) used. 11.4.1.2 ggplot2 ggplot2 comes with a fairly powerful tool for plotting a smoother, with geom_smooth. qplot(x, y) + geom_smooth(method=&quot;loess&quot;) + theme_bw() You can choose the bandwidth through the span argument: qplot(x, y) + geom_smooth(method=&quot;loess&quot;, span=0.1) + theme_bw() Too wiggly. The default looks fine. Note that geom_smooth can fit lm and glm fits too: qplot(x, y) + geom_smooth(method=&quot;lm&quot;, formula=y~x+I(x^2)) + theme_bw() 11.4.1.3 Manual method You can use the loess or ksmooth function in R to do kernel smoothing yourself. You get more flexibility here, since you can get things such as predictions. It works just like the other regression functions: (fit &lt;- loess(y ~ x)) ## Call: ## loess(formula = y ~ x) ## ## Number of Observations: 1000 ## Equivalent Number of Parameters: 5.18 ## Residual Standard Error: 1.037 We can make predictions as usual: yhat &lt;- predict(fit) qplot(x, y) + geom_line(data=data.frame(x=x, y=yhat), mapping=aes(x, y), colour=&quot;blue&quot;) + theme_bw() If you want the standard errors of the predictions, you can indicate this with se=TRUE in the predict function. Some key things that you might want to change in your kernel smoothing regression, through arguments in the loess function: Bandwidth/smoothing parameter. Indicate through the span argument. Degree of the local polynomial fitted. Indicate through the degree argument. Kernel function. The kernel function is not readily specified in loess. But you can use the ksmooth, where you’re allowed to specify a “box” or “normal” kernel. "],
["overfitting-the-problem-with-adding-too-many-parameters.html", "Chapter 12 Overfitting: The problem with adding too many parameters 12.1 Classification Exercise: Do Together 12.2 Training Error vs. Generalization Error 12.3 Model complexity 12.4 Reducible Error 12.5 Model Selection", " Chapter 12 Overfitting: The problem with adding too many parameters Caution: in a highly developmental stage! See Section 1.1. (Reducible Error) (BAIT 509 Class Meeting 02) suppressPackageStartupMessages(library(tidyverse)) ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 Wage &lt;- ISLR::Wage NCI60 &lt;- ISLR::NCI60 baseball &lt;- Lahman::Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) cow &lt;- suppressMessages(read_csv(&quot;data/milk_fat.csv&quot;)) esoph &lt;- as_tibble(esoph) %&gt;% mutate(agegp = as.character(agegp)) titanic &lt;- na.omit(titanic::titanic_train) 12.1 Classification Exercise: Do Together Let’s use the Default data in the ISLR R package for classification: predict whether or not a new individual will default on their credit card debt. Our task: make the best prediction model we can. Code the null model. What would you predict for a new person? What’s the error using the original data (aka training data)? Plot all the data, and come up with a classifier by eye. What would you predict for a non-student with income $40,000 and balance $1,011? Classify the original (training) data. What’s the overall error? How might we optimize the classifier? Let’s code it. Discussion: does it make sense to classify a new iris plant species based on the iris dataset? 12.2 Training Error vs. Generalization Error Discussion: What’s wrong with calculating error on the training data? What’s an alternative way to compute error? Activity: Get the generalization error for the above classifier. 12.3 Model complexity A big part of machine learning is choosing how complex to make a model. Examples: More partitions in the above credit card default classifier Adding higher order polynomials in linear regression And lots more that we’ll see when we explore more machine learning models. The difficulty: adding more complexity will decrease training error. What’s a way of dealing with this? 12.3.1 Activity Let’s return to the iris example from lecture 1, predicting Sepal Width, this time using Petal Width only, using polynomial linear regression. Task: Choose an optimal polynomial order. Here’s code for a graph to visualize the model fit: p &lt;- 4 ggplot(iris, aes(Petal.Width, Sepal.Width)) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, p)) + theme_bw() 12.4 Reducible Error 12.4.1 What is it? Last time, we saw what irreducible error is, and how to “beat” it. The other type of error is reducible error, which arises from not knowing the true distribution of the data (or some aspect of it, such as the mean or mode). We therefore have to estimate this. Error in this estimation is known as the reducible error. 12.4.2 Example one numeric predictor one numeric response true (unknown) distribution of the data is \\(Y|X=x \\sim N(5/x, 1)\\) (and take \\(X \\sim 1+Exp(1)\\)). you only see the following 100 observations stored in dat, plotted below, and choose to use linear regression as a model: set.seed(400) n &lt;- 100 dat &lt;- tibble( x = rexp(n) + 1, y = rnorm(n, mean = 5/x) ) ggplot(dat, aes(x, y)) + stat_smooth(method=&quot;lm&quot;, se=FALSE, size=0.5, mapping=aes(colour=&quot;Estimate&quot;)) + stat_function(fun=function(x) 5/x, mapping=aes(colour=&quot;True mean&quot;)) + scale_colour_brewer(&quot;&quot;, palette=&quot;Dark2&quot;) + theme_bw() + rotate_y The difference between the true curve and the estimated curve is due to reducible error. In the classification setting, a misidentification of the mode is due to reducible error. (Why the toy data set instead of real ones? Because I can embed characteristics into the data for pedagogical reasons. You’ll see real data at least in the assignments and final project.) 12.4.3 Bias and Variance There are two key aspects to reducible error: bias and variance. They only make sense in light of the hypothetical situation of building a model/forecaster over and over again as we generate a new data set over and over again. Bias occurs when your estimates are systematically different from the truth. For regression, this means that the estimated mean is either usually bigger or usually smaller than the true mean. For a classifier, it’s the systematic tendency to choosing an incorrect mode. Variance refers to the variability of your estimates. There is usually (always?) a tradeoff between bias and variance. It’s referred to as the bias/variance tradeoff, and we’ll see examples of this later. Let’s look at the above linear regression example again. I’ll generate 100 data sets, and fit a linear regression for each: set.seed(400) n &lt;- 100 N &lt;- 100 xgrid &lt;- data.frame(x=seq(0,6, length.out=100)) + 1 ## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop bias_plot &lt;- tibble(iter=1:N) %&gt;% group_by(iter) %&gt;% do({ dat &lt;- tibble(x=rexp(n)+1, y=5/x+rnorm(n)) data.frame( ., xgrid, Linear = predict(lm(y~x, data=dat), newdata=xgrid) ) }) %&gt;% ggplot(aes(x=x, y=Linear)) + geom_line(aes(group=iter, colour=&quot;Estimates&quot;), alpha=0.1) + stat_function(fun=function(x) 5/x, mapping=aes(colour=&quot;True mean&quot;)) + theme_bw() + scale_colour_brewer(&quot;&quot;, palette=&quot;Dark2&quot;) + ylab(&quot;y&quot;) + rotate_y bias_plot The spread of the linear regression estimates is the variance; the difference between the center of the regression lines and the true mean curve is the bias. 12.4.4 Reducing reducible error As the name suggests, we can reduce reducible error. Exactly how depends on the machine learning method, but in general: We can reduce variance by increasing the sample size, and adding more model assumptions. We can reduce bias by being less strict with model assumptions, OR by specifying them to be closer to the truth (which we never know). Consider the above regression example again. Notice how my estimates tighten up when they’re based on a larger sample size (1000 here, instead of 100): set.seed(400) n &lt;- 1000 N &lt;- 100 xgrid &lt;- data.frame(x=seq(0,6, length.out=100)) + 1 ## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop tibble(iter=1:N) %&gt;% group_by(iter) %&gt;% do({ dat &lt;- tibble(x=rexp(n)+1, y=5/x+rnorm(n)) data.frame( ., xgrid, Linear = predict(lm(y~x, data=dat), newdata=xgrid) ) }) %&gt;% ggplot(aes(x=x, y=Linear)) + geom_line(aes(group=iter, colour=&quot;Estimates&quot;), alpha=0.1) + stat_function(fun=function(x) 5/x, mapping=aes(colour=&quot;True mean&quot;)) + theme_bw() + scale_colour_brewer(&quot;&quot;, palette=&quot;Dark2&quot;) + ylab(&quot;y&quot;) + rotate_y Notice how, after fitting the linear regression \\(E(Y|X=x)=\\beta_0 + \\beta_1 (1/x)\\) (which is a correct model assumption), the regression estimates are centered around the truth – that is, they are unbiased: set.seed(400) n &lt;- 100 N &lt;- 100 xgrid &lt;- data.frame(xinv=(seq(0,6, length.out=100))) + 1 ## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop tibble(iter=1:N) %&gt;% group_by(iter) %&gt;% do({ dat &lt;- tibble(x=rexp(n)+1, xinv=1/x, y=5/x+rnorm(n)) data.frame( ., xgrid, Linear = predict(lm(y~xinv, data=dat), newdata=xgrid) ) }) %&gt;% ggplot(aes(x=1/xinv, y=Linear)) + geom_line(aes(group=iter, colour=&quot;Estimates&quot;), alpha=0.1) + stat_function(fun=function(x) 5/x, mapping=aes(colour=&quot;True mean&quot;)) + theme_bw() + scale_colour_brewer(&quot;&quot;, palette=&quot;Dark2&quot;) + ylab(&quot;y&quot;) + rotate_y + xlab(&quot;x&quot;) 12.4.5 Error decomposition We saw that we measure error using mean squared error (MSE) in the case of regression, and the error rate in the case of a classifier. These both contain all errors: irreducible error, bias, and variance: MSE = bias^2 + variance + irreducible variance A similar decomposition for error rate exists. Note: If you look online, the MSE is often defined as the expected squared difference between a parameter and its estimate, in which case the “irreducible error” is not present. We’re taking MSE to be the expected squared distance between a true “new” observation and our prediction (mean estimate). 12.5 Model Selection Caution: in a highly developmental stage! See Section 1.1. (BAIT 509 Class Meeting 04) &quot;orange&quot; &lt;- &quot;#d95f02&quot; rotate_y &lt;- theme(axis.title.y=element_text(angle=0, vjust=0.5)) 12.5.1 Exercise: CV k-fold cross validation with caret::train() in R. See this resource In python, can use sklearn.model_selection.cross_val_score. 12.5.2 Out-of-sample Error 12.5.2.1 The fundamental problem First, some terminology: The data that we use to fit a model is called training data, and the fitting procedure is called training. New data (or at least, data not used in the training process) is called test data. The goal of supervised learning is to build a model that has low error on new (test) data. *** A fundamental fact of supervised learning is that the error on the training data will (on average) be better (lower) than the error on new data! More terminology: training error and test error are errors computed on the respective data sets. Often, the test error is called generalization error. Let’s check using loess on an artificial data set (from last time). Here’s the training error (MSE): set.seed(87) n &lt;- 200 dat &lt;- tibble(x = c(rnorm(n/2), rnorm(n/2)+5)-3, y = sin(x^2/5)/x + rnorm(n)/10 + exp(1)) fit &lt;- loess(y ~ x, data=dat, span=0.3) yhat &lt;- predict(fit) mean((yhat - dat$y)^2) ## [1] 0.009599779 Here’s the test error: n &lt;- 1000 newdat &lt;- tibble(x = c(rnorm(n/2), rnorm(n/2)+5)-3, y = sin(x^2/5)/x + rnorm(n)/10 + exp(1)) yhat &lt;- predict(fit, newdata = newdat) mean((yhat - newdat$y)^2, na.rm = TRUE) ## [1] 0.0112968 If you think this was due to luck, go ahead and try changing the seed – more often than not, you’ll see the test error &gt; training error. This fundamental problem exists because, by definition, we build the model to be optimal based on the training data! For example, kNN and loess make a prediction that’s as close as possible to the training data. The more we try to make the model fit the training data – i.e., the more we overfit the data – the worse the problem gets. Let’s reduce the loess bandwidth to emulate this effect. Here’s the training error: set.seed(87) n &lt;- 200 dat &lt;- tibble(x = c(rnorm(n/2), rnorm(n/2)+5)-3, y = sin(x^2/5)/x + rnorm(n)/10 + exp(1)) fit &lt;- loess(y ~ x, data=dat, span=0.1) yhat &lt;- predict(fit) mean((yhat - dat$y)^2) ## [1] 0.008518578 Test error: n &lt;- 1000 newdat &lt;- tibble(x = c(rnorm(n/2), rnorm(n/2)+5)-3, y = sin(x^2/5)/x + rnorm(n)/10 + exp(1)) yhat &lt;- predict(fit, newdata = newdat) mean((yhat - newdat$y)^2, na.rm = TRUE) ## [1] 0.01233726 The effect gets even worse if we have less training data. For kNN and loess, we can play with the hyperparameter, weight function, and degree of local polynomial (in the case of regression) to try and avoid overfitting. Playing with these things is often called tuning. 12.5.2.2 Solution 1: Use a hold-out set. One solution is to split the data into two parts: training and validation data. The validation set is called a hold-out set, because we’re holding it out in the model training. Then, we can tune the model (such as choosing the \\(k\\) in kNN or \\(r\\) in loess) to minimize error on the validation set. set.seed(87) n &lt;- 200 dat &lt;- tibble(x = c(rnorm(n/2), rnorm(n/2)+5)-3, y = sin(x^2/5)/x + rnorm(n)/10 + exp(1)) n &lt;- 1000 newdat &lt;- tibble(x = c(rnorm(n/2), rnorm(n/2)+5)-3, y = sin(x^2/5)/x + rnorm(n)/10 + exp(1)) tibble(r = seq(0.05, 0.7, length.out=100)) %&gt;% group_by(r) %&gt;% do({ this_r &lt;- .$r fit &lt;- loess(y ~ x, data=dat, span=this_r) yhat_tr &lt;- predict(fit) yhat_val &lt;- predict(fit, newdata = newdat) data.frame( r = this_r, training = mean((yhat_tr - dat$y)^2), validation = mean((yhat_val - newdat$y)^2, na.rm = TRUE) ) }) %&gt;% gather(key=&quot;set&quot;, value=&quot;mse&quot;, training, validation) %&gt;% ggplot(aes(r, mse)) + geom_line(aes(group=set, colour=set)) + theme_bw() ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : k-d tree limited by memory. ncmax= 200 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : k-d tree limited by memory. ncmax= 200 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : k-d tree limited by memory. ncmax= 200 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = ## parametric, : k-d tree limited by memory. ncmax= 200 We would choose a bandwidth (\\(r\\)) of approximately 0.35, because the error on the validation set is smallest. Notice from this plot: The training error is lower than the out-of-sample error. We can make the training error arbitrarily small by decreasing \\(r\\). The out-of-sample error decreases, and then starts to increase again. NOTE: This doesn’t always happen, as you’ll see in Assignment 1. But it usually does. After choosing the model that gives the smallest error on the validation set, then the validation error is also going to be on average lower than in a test set – that is, if we get even more data! The more tuning parameters we optimize using a validation set, the more pronounced this effect will be. Two things to note from this: This is not as bad as the original problem (where the training error is less than the test error), because the tuning parameters are still chosen on an out-of-sample set. If we want to use the validation error as an estimate of the out-of-sample error, we just have to be mindful of the fact that this is an optimistic estimate of the generalization error. If you wanted an unbiased estimate of generalization error, you can start your procedure by splitting your data into three sets: training and validation as before, but also a test set that is never touched until you’ve claimed a final model! You only use the test set to get an unbiased estimate of generalization error. There’s not really a standard choice for deciding how much data to put aside for each set, but something like 60% training, 20% validation, and 20% test is generally acceptable. 12.5.2.3 Solution 2: Cross-validation The problem with the training-validation-test set approach is that you’re wasting a lot of data – lots of data are not being used in training! Another problem is that it’s not easy to choose how much data to put aside for each set. A solution is to use (\\(c\\)-fold) cross validation (CV), which can be used to estimate out-of-sample error, and to choose tuning parameters. (Note that usually people refer to this as \\(k\\)-fold cross validation, but I don’t want to overload \\(k\\) from kNN!) \\(c=10\\) is generally accepted as the defacto standard. Taking \\(c\\) equal to the sample size is a special case called leave-one-out cross validation. The general procedure is as follows: Partition the data into \\(c\\) (approximately equal) chunks. Hold out chunk 1; train the model on the other \\(c-1\\) chunks; calculate error on the held-out chunk. Hold out chunk 2; train the model on the other \\(c-1\\) chunks; calculate error on the held-out chunk. Hold out chunk 3; train the model on the other \\(c-1\\) chunks; calculate error on the held-out chunk. etc., until you’ve held out each chunk exactly once. Average the \\(c\\) errors to get an estimate of the generalization error. You can then repeat this procedure for different values of the tuning parameters, choosing values that give the lowest error. Once you choose this tuning parameter, go ahead and use all the data as training data, with the selected tuning parameters. CV is generally preferred to the hold-out set method, because we can fit a model that has overall lower error, but it’s computationally expensive. 12.5.3 Alternative measures of model goodness The coefficient of determination (\\(R^2\\)) can be calculated whenever it makes sense to calculate MSE. It equals: \\[ R^2 = 1 - \\frac{\\text{MSE of your model}}{\\text{MSE of the model that always predicts } \\bar{y}}. \\] This number lies between 0 and 1, where a 1 represents perfect prediction on the set that you’re computing \\(R^2\\) with. When we have a distributional assumption (such as Gaussian errors), we can calculate the likelihood – or more often, the negative log likelihood (\\(\\ell\\)). If the density/mass function of \\(y_i\\) is \\(f_i\\), and we have \\(n\\) observations, then the negative log likelihood is \\[ \\ell = -\\sum_{i=1}^{n} \\log(f_i(y_i)). \\] 12.5.4 Feature and model selection: setup For supervised learning, we seek a model that gives us the lowest generalization error as possible. This involves two aspects: Reduce the irreducible error. This involves feature engineering and feature selection: finding and choosing predictors that give us as much information about the response as we can get. Reduce the reducible error (= bias &amp; variance) This involves modelling and tuning, so that we can extract the information that the predictors hold about the response as best as we can. The better our model, the lower our reducible error is. This has been the main focus of BAIT 509, via models such as loess, kNN, random forests, SVM, etc. Recall for (2) that we avoid overfitting by tuning (choosing hyperparameters, such as \\(k\\) in kNN) to optimize generalization error. We estimate generalization error either using the validation set approach, cross validation, or the out-of-bag approach for bagging. The same thing applies to choosing features/predictors and choosing models, although model selection has a few extra components that should be considered. 12.5.5 Model selection The question here is, what supervised learning method should you use? There are a few things you should consider. Quantitative choice Suppose you’ve gone ahead and fit your best random forest model, kNN model, linear regression model, etc. Which do you choose? You should have estimated the generalization error for each model (for example, on a validation set) – choose the one that gives the lowest error. You might find that some models have roughly the same error. In this case, feel free to use all of these to make predictions. You can either look at all predictions, or take an average of the model outputs (called model averaging). Considering all models may be quite informative, though – for example, if all models are suggesting the same thing for a new case, then the decision is clearer than if they all say different things. Qualitative choice Sometimes, after exploring the data, it makes sense to add model assumptions. For example, perhaps your response looks linear in your predictors. If so, it may be reasonable to assume linearity, and fit a linear regression model. Note that adding assumptions like this generally reduce the variance in your model fit – but is prone to bias if the assumption is far from the truth. As usual, adding assumptions is about reducing the bias-variance tradeoff. Human choice (interpretability) Sometimes it’s helpful for a model to be interpretable. For example, the slopes in linear regression hold meaning; odds ratios in logistic regression hold meaning; nodes in a decision tree have meaning. If this is the case, then interpretability should also be considered. 12.5.6 Feature (predictor) selection Recall that, when tuning a supervised learning method (such as choosing \\(k\\) in kNN), we can make the training error arbitrarily small – but this results in overfitting the training data. The same thing applies to the number of predictors you add. Here’s an example. I’ll generate 100 observations of 1 response and 99 predictor variables totally randomly, fit a linear regression model with all the predictors, and calculate MSE: set.seed(38) dat &lt;- as.data.frame(matrix(rnorm(100*100), ncol=100)) names(dat)[1] &lt;- &quot;y&quot; fit &lt;- lm(y~., data=dat) mean((dat$y - predict(fit))^2) ## [1] 7.519591e-29 The MSE is 0 (up to computational precision) – the response is perfectly predicted on the training set. If we consider the number of predictors as a tuning parameter, then we can optimize this by estimating generalization error, as usual. But there are approaches that we can use that’s specific to feature selection, that we’ll discuss next. You are not expected to apply these for your project! This is just for your information. 12.5.6.1 Specialized metrics for feature selection You are not required to use this method for your project. Using these specialized metrics, we don’t need to bother holding out data to estimate generalization error: they have a penalty built into them based on the number of predictors that the model uses. The adjusted \\(R^2\\) is a modified version of \\(R^2\\). The AIC and BIC are modified versions of the negative log likelihood. There are others, like Mallows’ \\(C_p\\). Optimize these on the training data – they’re designed to (try to) prevent overfitting. But, with \\(p\\) predictors, we’d have \\(2^p\\) models to calculate these statistics for! That’s a lot of models when \\(p\\) is not even all that large. Here’s how the number of models grows as \\(p\\) increases: For 10 predictors, that’s 1000 models. For 20 predictors, that’s over 1,000,000. 12.5.6.2 Greedy Selection You are not required to use this method for your project. Instead of fitting all models, we can take a “greedy approach”. This may not result in the optimal model, but the hope is that we get close. One of three methods are typically used: Forward Selection The idea here is to start with the null model: no predictors. Then, add one predictor at a time, each time choosing the best one in terms of generalization error OR in terms of one of the specialized measures discussed above. Sometimes, a hypothesis test is used to determine whether the addition of the predictor is significant enough. Backward Selection The idea here is to start with the full model: all predictors. Then, gradually remove predictors that are either insignificant according to a hypothesis test, or that gives the greatest reduction in one of the specialized measures discussed above. Stepwise Selection The idea here is to combine forward and backward selection. Instead of only adding or only removing predictors, we can consider either at each iteration: adding or removing. 12.5.6.3 Regularization You are not required to use this method for your project. When training a model, we can write the training procedure as the optimization of a loss function. For example, in regression, we want to minimize the sum of squared errors. Regularization adds a penalty directly to this loss function, that grows as the number of predictors grows. This is in contrast to the specialized measures (like adjusted \\(R^2\\)) that adds the penalty to the error term after the model is fit. There are different types of regularization, but typically those that involve an L1 regularizer are used in feature selection. "],
["describing-relationships.html", "Describing Relationships", " Describing Relationships "],
["theres-meaning-in-parameters.html", "Chapter 13 There’s meaning in parameters 13.1 The types of parametric assumptions 13.2 The value of making parametric assumptions 13.3 ANOVA", " Chapter 13 There’s meaning in parameters 13.1 The types of parametric assumptions A model can be parametric (i.e., containing parameters) in roughly two ways: 13.1.1 1. When defining a model function. For example, in linear regression, we assume that the model function is linear. We might also make assumptions about the conditional variance. This tends to be the meaning of “parametric” in Computer Science. 13.1.2 2. When defining probability distributions. For example, we might assume that residuals are Gaussian, or perhaps some other distribution. This tends to be the meaning of “parametric” in Statistics. 13.2 The value of making parametric assumptions There are arguably two reasons one might bother making a parametric assumption. They are: Reduced error. Interpretability. 13.2.1 Value #1: Reduced Error One value of making parametric assumptions is that we might achieve reduced error. As long as we don’t introduce as many parameters as there are observations (or more), here’s what generally happens when we make an assumption: The model variance decreases. You can think of the reason behind this in two ways: we’re adding information to our data set; or we don’t need to estimate as many quantities. The bias increases the “more incorrect” your assumption is. This is because we’re identifying a framework that is almost surely not true. Recall that mean squared error has both (squared) bias and model variance as components. The hope is that your model is “correct enough” so that the increase in bias is small in comparison to the decrease in variance, resulting in an overall decrease in error. Challenge: run a simulation to convince yourself of this – first in the univariate setting (where bias and variance are more interpretable), then in the regression setting. For more information on the bias-variance tradeoff, check out Section 2.2.2 of the ISLR book. 13.2.2 Value #2: Interpretation Sometimes making a parametric assumption does not reduce the overall error by much, even when the assumption is true. This would not be appealing if all you care about is prediction performance. But if you want to gain some insight into relationships between your predictors and response, then introducing a parameter so that it has meaning will help with this task. For example, assuming the mean response is linear in the predictors gives us meaning behind the slope parameter, as it corresponds to the expected change in response associated with a difference of 1 unit of the corresponding predictor. The bonus here is that we don’t always need to think of the parameters as being strictly correct. Your assumptions will never hold exactly, so as long as the assumption is not completely unreasonable, the parameters at least give you a sense of what’s going on in your data. 13.3 ANOVA Caution: in a highly developmental stage! See Section 1.1. (From 2018-2019 DSCI 561 lab1) Remember that to make a hypothesis test, we need to first come up with some “distance metric” (called the test statistic) to measure discrepency from the null hypothesis. For example, to test whether two groups have different population means, a t-test bases its distance metric / test statistic on the difference between the two sample means – the further this metric is from 0, the more evidence we have that the null hypothesis is not true. There’s another way to measure discrepency from the null hypothesis that all group means are equal – a way that allows for any number of groups to be compared all at once (not just two). But, it operates under the assumption that the variance of the data in each group is the same. Here’s the idea. If the population means of each group truly are the same, then we can estimate that common variance in two ways: using the “overall” variance (ignoring the groups altogether), OR by averaging the variances of each group. The distance metric / test statistic is based off of the ratio of these two values (which is more meaningful than looking at the difference between the two). This is ANOVA – ANalysis Of VAriance. For the pregnancy (two-group) dataset, calculate: the overall variance of the response, and store it in var_tot; the average of the variances for each group, and store it in var_grp; and how many times larger is (1) compared to (2)? i.e., calculate (1)/(2). Store it in my_ratio. Take a moment to reflect as to whether you think this is a big difference or not (no need to write anything). The actual test statistic of ANOVA has minor adjustments to the ratio you just calculated, although is based on the same concepts. Since (1) &gt;= (2), the ratio will always be &gt;1. So instead, the numerator is based on the difference between (1) and (2) (this results in a variance sometimes called the “treatment variance”). The variance estimates don’t always use n-1 – they adjust for the number of groups, based on the concept of “degrees of freedom”. Still, the larger the ratio (test statistic), the less evidence we have to support the null hypothesis. Your task: run the ANOVA in R using the aov() function for the pregnancy data. Quality code uses broom::tidy(). Then, store the p-value in preg_aov_p, the test statistic in preg_aov_F, and the “Species” and “Residuals” degrees of freedom (df) as a length-2 vector in the variable preg_aov_df. If the null hypothesis is true, then the sampling distribution of this test statistic is (to a good approximation) a specific F-distribution. This “good approximation” is thanks to the CLT, and is true as long as the sample size isn’t small (and it’s always true if the data are Gaussian). "],
["the-meaning-of-interaction.html", "Chapter 14 The meaning of interaction", " Chapter 14 The meaning of interaction Interaction terms, and when relationships change given a variable. "],
["scales-and-the-restricted-range-problem.html", "Chapter 15 Scales and the restricted range problem 15.1 Problems 15.2 Solutions 15.3 GLM’s in R 15.4 Options for Logistic Regression", " Chapter 15 Scales and the restricted range problem Caution: in a highly developmental stage! See Section 1.1. link functions and alternative parameter interpretations (categorical data too) In Regression I, the response was allowed to take on any real number. But what if the range is restricted? suppressPackageStartupMessages(library(tidyverse)) ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 Wage &lt;- ISLR::Wage NCI60 &lt;- ISLR::NCI60 baseball &lt;- Lahman::Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) cow &lt;- suppressMessages(read_csv(&quot;data/milk_fat.csv&quot;)) esoph &lt;- as_tibble(esoph) %&gt;% mutate(agegp = as.character(agegp)) titanic &lt;- na.omit(titanic::titanic_train) 15.1 Problems Here are some common examples. Positive values: river flow. Lower limit: 0 Percent/proportion data: proportion of income spent on housing in Vancouver. Lower limit: 0 Upper limit: 1. Binary data: success/failure data. Only take values of 0 and 1. Count data: number of male crabs nearby a nesting female Only take count values (0, 1, 2, …) Here is an example of the fat content of a cow’s milk, which was recorded over time. Data are from the paper “Transform or Link?”. Let’s consider data as of week 10: (plot_cow &lt;- cow %&gt;% filter(week &gt;= 10) %&gt;% ggplot(aes(week, fat*100)) + geom_point() + theme_bw() + labs(y = &quot;Fat Content (%)&quot;) + ggtitle(&quot;Fat content of cow milk&quot;)) Let’s try fitting a linear regression model. plot_cow + geom_smooth(method = &quot;lm&quot;, se = FALSE) Notice the problem here – the regression lines extend beyond the possible range of the response. This is mathematically incorrect, since the expected value cannot extend outside of the range of Y. But what are the practical consequences of this? In practice, when fitting a linear regression model when the range of the response is restricted, we lose hope for extrapolation, as we obtain logical fallacies if we do. In this example, a cow is expected to produce negative fat content after week 35! Despite this, a linear regression model might still be useful in these settings. After all, the linear trend looks good for the range of the data. 15.2 Solutions How can we fit a regression curve to stay within the bounds of the data, while still retaining the interpretability that we have with a linear model function? Remember, non-parametric methods like random forests or loess will not give us interpretation. Here are some options: Transform the data. Transform the linear model function: link functions Use a scientifically-backed parametric function. 15.2.1 Solution 1: Transformations One solution that might be possible is to transform the response so that its range is no longer restricted. The most typical example is for positive data, like river flow. If we log-transform the response, then the new response can be any real number. All we have to do is fit a linear regression model to this transformed data. One downfall is that we lose interpretability, since we are estimating the mean of \\(\\log(Y)\\) (or some other transformation) given the predictors, not \\(Y\\) itself! Transforming the model function by exponentiating will not fix this problem, either, since the exponential of an expectation is not the expectation of an exponential. Though, this is a mathematical technicality, and might still be a decent approximation in practice. Also, transforming the response might not be fruitful. For example, consider a binary response. No transformation can spread the two values to be non-binary! 15.2.2 Solution 2: Link Functions Instead of transforming the data, why not transform the model function? For example, instead of taking the logarithm of the response, perhaps fit the model \\[ E(Y|X=x) = \\exp(\\beta_0 + \\beta x) = \\alpha \\exp(\\beta x) \\]. Or, in general, \\[ g(E(Y|X=x)) = X^T \\beta \\] for some increasing function \\(g\\) called the link function. This has the added advantage that we do not need to be able to transform the response. Two common examples of link functions: \\(\\log\\), for positive response values. Parameter interpretation: an increase of one unit in the predictor is associated with an \\(\\exp(\\beta)\\) times increase in the mean response, where \\(\\beta\\) is the slope parameter. \\(\\text{logit}(x)=\\log(x/(1-x))\\), for binary response values. Parameter interpretation: an increase of one unit in the predictor is associated with an \\(\\exp(\\beta)\\) times increase in the odds of “success”, where \\(\\beta\\) is the slope parameter, and odds is the ratio of success to failure probabilities. 15.2.3 Solution 3: Scientifically-backed functions Sometimes there are theoretically derived formulas for the relationship between response and predictors, which have parameters that carry some meaning to them. 15.3 GLM’s in R This document introduces the glm() function in R for fitting a Generlized Linear Model (GLM). We’ll work with the titanic_train dataset in the titanic package. str(titanic) ## &#39;data.frame&#39;: 714 obs. of 12 variables: ## $ PassengerId: int 1 2 3 4 5 7 8 9 10 11 ... ## $ Survived : int 0 1 1 1 0 0 0 1 1 1 ... ## $ Pclass : int 3 1 3 1 3 1 3 3 2 3 ... ## $ Name : chr &quot;Braund, Mr. Owen Harris&quot; &quot;Cumings, Mrs. John Bradley (Florence Briggs Thayer)&quot; &quot;Heikkinen, Miss. Laina&quot; &quot;Futrelle, Mrs. Jacques Heath (Lily May Peel)&quot; ... ## $ Sex : chr &quot;male&quot; &quot;female&quot; &quot;female&quot; &quot;female&quot; ... ## $ Age : num 22 38 26 35 35 54 2 27 14 4 ... ## $ SibSp : int 1 1 0 1 0 0 3 0 1 1 ... ## $ Parch : int 0 0 0 0 0 0 1 2 0 1 ... ## $ Ticket : chr &quot;A/5 21171&quot; &quot;PC 17599&quot; &quot;STON/O2. 3101282&quot; &quot;113803&quot; ... ## $ Fare : num 7.25 71.28 7.92 53.1 8.05 ... ## $ Cabin : chr &quot;&quot; &quot;C85&quot; &quot;&quot; &quot;C123&quot; ... ## $ Embarked : chr &quot;S&quot; &quot;C&quot; &quot;S&quot; &quot;S&quot; ... ## - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int 6 18 20 27 29 30 32 33 37 43 ... ## ..- attr(*, &quot;names&quot;)= chr &quot;6&quot; &quot;18&quot; &quot;20&quot; &quot;27&quot; ... Consider the regression of Survived on Age. Let’s take a look at the data with jitter: ggplot(titanic, aes(Age, Survived)) + geom_jitter(height=0.1, alpha=0.25) + scale_y_continuous(breaks=0:1, labels=c(&quot;Perished&quot;, &quot;Survived&quot;)) + theme_bw() Recall that the linear regression can be done with the lm function: res_lm &lt;- lm(Survived ~ Age, data=titanic) summary(res_lm) ## ## Call: ## lm(formula = Survived ~ Age, data = titanic) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.4811 -0.4158 -0.3662 0.5789 0.7252 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.483753 0.041788 11.576 &lt;2e-16 *** ## Age -0.002613 0.001264 -2.067 0.0391 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4903 on 712 degrees of freedom ## Multiple R-squared: 0.005963, Adjusted R-squared: 0.004567 ## F-statistic: 4.271 on 1 and 712 DF, p-value: 0.03912 In this case, the regression line is 0.4837526 + -0.0026125 Age. A GLM can be fit in a similar way, using the glm function – we just need to indicate what type of regression we’re doing (binomial? poission?) and the link function. We are doing bernoulli (binomial) regression, since the response is binary (0 or 1); lets choose a probit link function. res_glm &lt;- glm(factor(Survived) ~ Age, data=titanic, family=binomial(link=&quot;probit&quot;)) The family argument takes a function, indicating the type of regression. See ?family for the various types of regression allowed by glm(). Let’s see a summary of the GLM regression: summary(res_glm) ## ## Call: ## glm(formula = factor(Survived) ~ Age, family = binomial(link = &quot;probit&quot;), ## data = titanic) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.1477 -1.0363 -0.9549 1.3158 1.5929 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.037333 0.107944 -0.346 0.7295 ## Age -0.006773 0.003294 -2.056 0.0397 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 964.52 on 713 degrees of freedom ## Residual deviance: 960.25 on 712 degrees of freedom ## AIC: 964.25 ## ## Number of Fisher Scoring iterations: 4 We can make predictions too, but this is not as straight-forward as in lm() – here are the “predictions” using the predict() generic function: pred &lt;- predict(res_glm) qplot(titanic$Age, pred) + labs(x=&quot;Age&quot;, y=&quot;Default Predictions&quot;) Why the negative predictions? It turns out this is just the linear predictor, -0.0373331 + -0.0067733 Age. The documentation for the predict() generic function on glm objects can be found by typing ?predict.glm. Notice that the predict() generic function allows you to specify the type of predictions to be made. To make predictions on the mean (probability of Survived=1), indicate type=&quot;response&quot;, which is the equivalent of applying the inverse link function to the linear predictor. Here are those predictions again, this time indicating type=&quot;response&quot;: pred &lt;- predict(res_glm, type=&quot;response&quot;) qplot(titanic$Age, pred) + labs(x=&quot;Age&quot;, y=&quot;Mean Estimates&quot;) Look closely – these predictions don’t actually fall on a straight line. They follow an inverse probit function (i.e., a Gaussian cdf): mu &lt;- function(x) pnorm(res_glm$coefficients[1] + res_glm$coefficients[2] * x) qplot(titanic$Age, pred) + labs(x=&quot;Age&quot;, y=&quot;Mean Estimates&quot;) + stat_function(fun=mu, colour=&quot;blue&quot;) + scale_x_continuous(limits=c(-200, 200)) 15.3.1 broom::augment() We can use the broom package on glm objects, too. But, just like we had to specify type=&quot;response&quot; when using the predict() function in order to evaluate the model function, so to do we have to specify something in the broom::augment() function. Here, the type.predict argument gets passed to the predict() generic function (actually, the predict.glm() method). This means that indicating type.predict=&quot;response&quot; will evaluate the model function: res_glm %&gt;% broom::augment(type.predict = &quot;response&quot;) %&gt;% head() ## # A tibble: 6 x 10 ## .rownames factor.Survived. Age .fitted .se.fit .resid .hat .sigma ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0 22 0.426 0.0209 -1.05 0.00179 1.16 ## 2 2 1 38 0.384 0.0211 1.38 0.00188 1.16 ## 3 3 1 26 0.415 0.0190 1.33 0.00149 1.16 ## 4 4 1 35 0.392 0.0196 1.37 0.00160 1.16 ## 5 5 0 35 0.392 0.0196 -0.997 0.00160 1.16 ## 6 7 0 54 0.343 0.0345 -0.917 0.00528 1.16 ## # … with 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; 15.4 Options for Logistic Regression Some popular interpretable quantities (IQ’s) which compare exposure risk \\(\\pi_{E}\\) to baseline (unexposed) risk \\(\\pi_{B}\\) are the risk difference, \\(\\pi_{E}-\\pi_{B}\\), the reciprocal risk difference, or number needed to treat (NNT) (or sometimes number needed to harm), the relative risk, \\(\\pi_{E}/\\pi_{B}\\), and the odds ratio, These IQ’s consider all other factors to be equal. 15.4.1 Models A first inclination may be to model the mean as one would in the case of multiple linear regression that is, as a linear combination of the covariates. The link function \\(g\\) is the identity, and the model becomes (EQUATION). Kovalchik and others (2013) refer to this as the “Binomial Linear Model”, or BLM, though it is more commonly known as the “Linear Probability Model”, or LPM (see, for example, Aldrich and Nelson, 1984; Amemiya, 1977; Horrace and Oaxaca, 2006). In this paper, the model is referred to as the LPM. Before proceeding with any further discussion, the validity of this model must be enforced. The Bernoulli distribution requires \\(0\\leq\\pi(X)\\leq1\\) for all \\(x\\) \\(\\boldsymbol{x}\\in XX\\) to be a valid distribution. Validity can be ensured by restricting the parameter space of \\(\\left(\\beta_{0},\\boldsymbol{\\beta}\\right)\\) to . However, the parameter space can be severely restricted depending on the covariate space. For example, if predictor \\(k\\) of is unbounded, then the only allowable value for \\(\\beta_{k}\\) is zero. In other words, any covariate in the LPM that has an unbounded range cannot technically be included in the LPM. Further, even if component \\(k\\) is bounded, if it has a large range, then the slope is restricted to be small. One reason why the LPM is used, despite the above restrictions, is for access to constant interpretable quantities that is, IQ’s discussed in section 1 which do not depend on other covariates. In an LPM, the risk difference by increasing \\(X_{k}\\) by one unit is simply given by \\(\\beta_{k}\\), and the NNT is \\(1/\\beta_{k}\\). However, the relative risk and odds ratio are non-constant, as they are functions of the other covariates. Since the LPM is just a multiple linear regression model, the regression parameters can be estimated without bias by ordinary least squares (OLS). However, we do not necessarily have homoskedastic errors, since \\(Var(Y|X=x)\\) differs with the covariates. As such, the efficiency of the OLS estimator can be improved by the weighted least squares estimator with weights \\(1/\\sigma\\left(\\boldsymbol{x}\\right)\\). Since these weights are unknown, an iterative algorithm is used, which calculates weights using the fitted probabilities from parameter estimates of the previous step to compute a new “re-weighted” estimator. Iterating this beginning with the OLS estimator converges to the iteratively re-weighted least squares (IRLS) estimator. Amemiya (1977) shows that the IRLS is identical to the maximum likelihood estimator (MLE). An alternative model which is sometimes confused for the LPM (for example, see Horrace and Oaxaca, 2006) is to allow for an arbitrary parameter space by taking \\(\\pi(x)\\) to be zero when \\(\\eta\\) is less than zero, and unity otherwise. This model, which I call the “truncated LPM” (TLPM), is (EQUATION) where the inverse-link function \\(T\\) is the ramp function (actually, \\(T\\) is not quite an inverse-link function because it is non-invertible, but this is unimportant). However, as one can see by the differing link function, this is not the LPM, although it is often mistaken for the LPM. Horrace and Oaxaca (2006) mistake the TLPM for the LPM, and in doing so, show that estimation of the model parameters through OLS or IRLS provide biased and inconsistent estimators. This is a good reason why the TLPM should not be used unless a different method of estimation is considered. To rid the parameter space of restrictions, one may consider link functions similar to the ramp function (preferably smooth) to ensure \\(0\\leq\\pi(x)\\leq1\\). Popular choices are logit, probit, the inverse Gumbel distribution function, or the angular function (Cox and Snell, 1989). Each of these link functions ensures a valid probability for an arbitrary parameter space. The logit link function is a popular choice because it has the best interpretability. It models the log-odds as a linear function of the covariates that is, (EQUATION). This model is known as the logistic regression model, and can be written equivalently as , where is the inverse logit function. The logistic model stands out over models with other link functions because a constant IQ can be obtained from it the odds ratio by increasing \\(X_{k}\\) by one unit is simply \\(\\exp\\left(\\beta_{k}\\right)\\). However, of the interpretable quantities discussed in Section 1, the odds ratio is the most difficult to interpret. Though, if both risks are smallthe “rare disease assumption” with risks under \\(0.1\\) then the odds ratio is a good approximation to the relative risk, which is easier to interpret. The lack of an easy interpretable constant IQ is why some researchers will opt for the LPM instead of the logistic model when the rare disease assumption is invalid. Indeed, this is one major reason behind the study done by Kovalchik and others (2013). One other method to decide is through Goodness of Fit criteria, which was the other deciding factor of Kovalchik and others. Conveniently, the log odds appears in the likelihood of the logistic model, which simplifies some computations. This leads to the MLE which solves the equation . However, occasionally it is possible that no MLE exists when there is a \\(\\left(b_{0},\\boldsymbol{b}\\right)\\in\\mathcal{F}\\) such that \\(b_{0}+\\boldsymbol{x}_{i}^{T}\\boldsymbol{b}&gt;0\\) has \\(Y_{i}=1\\) and \\(b_{0}+\\boldsymbol{x}_{i}^{T}\\boldsymbol{b}&lt;0\\) has \\(Y_{i}=0\\) for each \\(i=1,\\ldots,n\\) (Albert and Anderson, 1984). This is called the case of “complete separation”, and the likelihood has no maximum, so a “perfect fit” is made by infinitely pushing the covariate data to the tails of the expit curve. This is not an issue with the LPM model. "],
["improving-estimation-through-distributional-assumptions.html", "Chapter 16 Improving estimation through distributional assumptions", " Chapter 16 Improving estimation through distributional assumptions "],
["when-we-only-want-interpretation-on-some-predictors.html", "Chapter 17 When we only want interpretation on some predictors 17.1 Non-identifiability in GAMS", " Chapter 17 When we only want interpretation on some predictors Caution: in a highly developmental stage! See Section 1.1. 17.1 Non-identifiability in GAMS Here is some help on Lab 4 Exercise 1(b). Exercise 1(b) is intended to get you to think about what the \\(h\\) functions in a Generalized Additive Model (GAM) are. An interpretation of the \\(h\\) functions can only make sense in light of the non-identifiability issue of GAM’s, so that’s discussed first. Then, hints are given for the first two questions in Exercise 1(b). 17.1.1 Non-identifiability What is “non-identifiability”, exactly? It can happen for any model that’s not carefully specified (not just GAM’s). Let’s look at an example first. In simple linear regression, why not write the model \\[ Y = \\beta_0 + \\alpha_0 + \\beta_1 X + \\varepsilon, \\] where \\(\\mathbb{E}(\\varepsilon)=0\\)? It’s because three parameters are too many to describe a line. In other words, . For example, the model \\(Y=1+X+\\varepsilon\\) can be written with \\[ \\beta_0 = 0, \\alpha_0 = 1, \\beta_1 = 1, \\] or \\[ \\beta_0 = -1, \\alpha_0 = 2, \\beta_1 = 1, \\] etc. In fact, as long as \\(\\alpha_0 = 1 - \\beta_0\\), and \\(\\beta_1=1\\), we get the same regression line. In general, and roughly speaking, when more than one parameter selection gives you the same model, there’s a non-identifiability issue. It leads to problems in estimation and estimator properties. It also leads to an problem: the parameters don’t have a meaning, since they can represent more than one thing in the model. This is even true in non-parametric cases, such as the GAM. Let’s look at a two-predictor GAM: \\[ Y = \\beta_0 + h_1\\left(X_1\\right) + h_2\\left(X_2\\right) + \\varepsilon, \\] where \\(\\beta_0\\) is any real number, \\(h_1\\) and \\(h_2\\) are any smooth functions, and \\(\\mathbb{E}(\\varepsilon)=0\\). As it is, this model is non-identifiable: if you pick a \\(\\beta_0\\), \\(h_1\\), and \\(h_2\\), I can find another set of \\(\\beta_0\\), \\(h_1\\), and \\(h_2\\) that gives the same regression surface. How? I can just add a constant \\(c\\) to your \\(\\beta_0\\), and subtract that constant from your, say, \\(h_1\\) (i.e., “vertically shift” your \\(h_1\\) function downwards by \\(c\\)). So, the “parameters” (which includes the \\(h\\) functions) in a GAM are non-identifiable – the \\(h\\) functions can be vertically shifted, and \\(\\beta_0\\) can just compensate for these shifts to give the same regression surface. To make the model identifiable, we force the \\(h\\) functions to be vertically centered at zero. Here’s how: we ensure that after transforming the \\(j\\)’th predictor to \\(h_j\\left(X_j\\right)\\), the resulting data are centered at 0. Mathematically, we ensure that \\[ \\frac{1}{n}\\sum_{i=1}^{n}h_j\\left(x_{ij}\\right) = 0 \\] for each predictor \\(j\\), where \\(x_{ij}\\) for \\(i=1,\\ldots,n\\) are the observations. 17.1.2 Question 1b Notation: Let’s call \\(\\hat{\\beta}_0\\) the estimate of \\(\\beta_0\\), and the the functions \\(\\hat{h}_1\\) and \\(\\hat{h}_2\\) the estimates of \\(h_1\\) and \\(h_2\\), respectively. The prediction on observation \\(i\\), denoted \\(\\hat{Y}_i\\), is \\[ \\hat{Y}_i = \\hat{\\beta}_0 + \\hat{h}_1\\left(x_{i1}\\right) + \\hat{h}_1\\left(x_{i2}\\right). \\] This will help with the first question: Suppose the gam fit is called fit. Why is mean(predict(fit)) the same as the estimate of the intercept? Here’s a hint: predict(fit) gives you the vector \\(\\hat{Y}_1, \\ldots, \\hat{Y}_n\\). Then, mean averages them. The question is asking you to indicate why we have \\[ \\frac{1}{n}\\sum_{i=1}^{n}\\hat{Y}_i = \\hat{\\beta}_0. \\] The answer uses Equation The next question asks you to think about how you’d recover an \\(h\\) function. It asks: For each \\(h\\) function, write an R function that evaluates the \\(h\\) function over a grid of values, without calling the plot function on the fit. Show that the function works by evaluating it over a small grid of values. Suppose you want to evaluate function \\(\\hat{h}_1\\) at some generic point \\(x_0\\). You can do this using the predict function, and somehow specifying \\(x_0\\) in the newdata argument (in place of “predictor 1”). But predict will give you all three components of the model, added together: the \\(\\hat{\\beta}_0\\) part, plus the \\(\\hat{h}_1\\) part (evaluated at whatever is in the “predictor 1” column), plus the \\(\\hat{h}_2\\) part (evaluated at whatever is in the “predictor 2” column). Your job is to “isolate” the \\(\\hat{h}_1\\) part, evaluated at \\(x_0\\). We can subtract out \\(\\hat{\\beta}_0\\), which is specified in the model output. But you can’t just subtract out the \\(\\hat{h}_2\\) part, because we don’t know it. Your job is to use a property of \\(\\hat{h}_2\\) (hint: Equation ) to remove it. You can also think of it this way: if mean(predict(fit)) “zeroes-out” both \\(h\\) functions, how can you modify the prediction data so that one of the \\(h\\) functions doesn’t zero-out, but instead evaluates at some desired point? "],
["special-cases.html", "Special cases", " Special cases "],
["regression-when-data-are-censored-survival-analysis.html", "Chapter 18 Regression when data are censored: survival analysis", " Chapter 18 Regression when data are censored: survival analysis "],
["regression-in-the-presence-of-outliers-robust-regression.html", "Chapter 19 Regression in the presence of outliers: robust regression 19.1 Robust Regression in R", " Chapter 19 Regression in the presence of outliers: robust regression Caution: in a highly developmental stage! See Section 1.1. 19.1 Robust Regression in R DSCI 562 Lab 4 Tutorial There are many R packages out there to assist with robust estimation. It depends on the task at hand. We’ll go over a few. For robust linear regression, there are a few options. There’s the rlm function in the MASS package. It works in a similar way as the lm function. Can also use the functions predict, residuals, coefficients, etc. on the output. I like this option because it allows for different psi functions besides the Huber. library(MASS) (fit6 &lt;- rlm(mpg ~ disp + wt, data=mtcars)) ## Call: ## rlm(formula = mpg ~ disp + wt, data = mtcars) ## Converged in 7 iterations ## ## Coefficients: ## (Intercept) disp wt ## 34.60475194 -0.01640037 -3.39317550 ## ## Degrees of freedom: 32 total; 29 residual ## Scale estimate: 3.15 (fit7 &lt;- rlm(mpg ~ disp + wt, data=mtcars, psi=psi.bisquare)) ## Call: ## rlm(formula = mpg ~ disp + wt, data = mtcars, psi = psi.bisquare) ## Converged in 7 iterations ## ## Coefficients: ## (Intercept) disp wt ## 34.64727079 -0.01692585 -3.36447640 ## ## Degrees of freedom: 32 total; 29 residual ## Scale estimate: 3.22 The package robustbase has the function lmrob for linear models, but also has glmrob for GLM’s. Similarly, the robust package has similar functions lmRob and glmRob. A robust version of GAM’s can be obtained with the robustgam function in the robustgam package. A robust version of LME’s can be obtained with the rlmer function in the robustlmm package. 19.1.1 Heavy Tailed Regression For a heavy tailed extension of lm, one can use the tlm function in the hett package. The package heavy has some regression techniques using heavy tailed distributions. "],
["regression-in-the-presence-of-extremes-extreme-value-regression.html", "Chapter 20 Regression in the presence of extremes: extreme value regression", " Chapter 20 Regression in the presence of extremes: extreme value regression Caution: in a highly developmental stage! See Section 1.1. suppressPackageStartupMessages(library(tidyverse)) ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 The problem with estimating extreme quantiles in the “usual” sense: Here is a histogram of 100 observations generated from a Student’s t(1) distribution (it’s heavy-tailed): set.seed(4) y &lt;- rt(100, df=1) qplot(y) + theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Here are estimates of high and low quantiles, compared to the actual. You can see the discrepency grows quickly. Extreme-low quantiles are too high, whereas extreme-high quantiles are too low. As a rule of thumb, it’s best to stay below \\(\\tau=0.95\\) or above \\(\\tau=0.05\\). If you really want estimates of these extreme quantiles, you’ll need to turn to Extreme Value Theory to make an assumption on the tail of the distribution of the data. One common approach is to fit a generalized Pareto distribution to the upper portion of the data, after which you can extract high quantiles. "],
["regression-when-data-are-ordinal.html", "Chapter 21 Regression when data are ordinal", " Chapter 21 Regression when data are ordinal "],
["regression-when-data-are-missing-multiple-imputation.html", "Chapter 22 Regression when data are missing: multiple imputation 22.1 Mean Imputation 22.2 Multiple Imputation", " Chapter 22 Regression when data are missing: multiple imputation Caution: in a highly developmental stage! See Section 1.1. (DSCI 562 Tutorial) suppressPackageStartupMessages(library(tidyverse)) ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 suppressPackageStartupMessages(library(mice)) Let’s take a closer look at mean imputation vs. multiple imputation. 22.1 Mean Imputation Let’s consider a simple linear regression example, with one explanatory variable. We’ll generate 100 data points, and make 10 of the response values missing. set.seed(13) x &lt;- rnorm(100) y &lt;- -1 + 2 * x + rnorm(100) y[1:10] &lt;- NA Here are the data: x ## [1] 0.55432694 -0.28027194 1.77516337 0.18732012 1.14252615 ## [6] 0.41552613 1.22950656 0.23667967 -0.36538277 1.10514427 ## [11] -1.09359397 0.46187091 -1.36098453 -1.85602715 -0.43985541 ## [16] -0.19394690 1.39643151 0.10066325 -0.11443881 0.70222523 ## [21] 0.26254267 1.83616330 0.35740242 -1.04541013 0.62018413 ## [26] 0.14935453 -1.45931685 -2.02704380 -1.05695776 -0.72814372 ## [31] -0.00821067 0.84779738 -0.38349150 -0.52651151 -0.27322596 ## [36] -0.60574161 -0.33286731 -0.24153755 -0.86277540 -0.84697075 ## [41] 0.10034035 1.59003353 0.56649488 1.61447949 -0.46865016 ## [46] -0.72610140 -1.02333900 -1.93781553 0.27714729 1.40835367 ## [51] 0.27312919 0.75552507 -0.34901841 -0.54619076 0.23436199 ## [56] -0.29782822 -0.84047613 0.82651036 1.48369123 0.69967564 ## [61] -1.26157415 0.29827174 -0.14780711 -0.88892233 1.01306586 ## [66] -0.92052508 -0.57389450 1.15036548 1.14382456 -0.23944276 ## [71] -1.08680215 -0.06144699 -0.51669734 -1.90767369 0.10715648 ## [76] -1.17737519 1.74542691 -0.39869853 0.44243942 0.45027946 ## [81] -0.07606216 0.29751322 -1.19435471 -1.99687548 1.38851305 ## [86] -0.08248357 0.39251449 -1.08276971 1.60212039 1.00406897 ## [91] 0.37989570 -0.56550536 -1.21377810 -1.36430159 -1.41613295 ## [96] -0.25557803 -1.22542595 0.21383426 0.06722356 0.85663511 y ## [1] NA NA NA NA NA ## [6] NA NA NA NA NA ## [11] -4.053704654 1.559887706 -4.743071625 -5.523818780 -1.930473677 ## [16] -1.500485206 2.645513317 -0.051326812 -0.839409352 0.218430884 ## [21] 0.850566209 3.126514212 -0.368543711 -3.162228622 -1.105128040 ## [26] -0.902167648 -4.154972172 -5.111932100 -4.145855361 -3.322836591 ## [31] -0.168810384 0.819016643 -1.365461091 -1.130697319 0.240141538 ## [36] -2.390779077 -2.058218000 -0.761012345 -1.767026541 -3.989408584 ## [41] -0.459957593 1.032304370 -0.957928133 0.235330176 -1.507042163 ## [46] -0.745023600 -4.628929629 -5.232634575 -2.144358973 0.664736992 ## [51] -3.298505466 2.225627728 -2.926853644 -2.406454479 -0.601188334 ## [56] -2.433257875 -2.434091710 -1.922107514 1.971684397 2.745313865 ## [61] -3.459211491 0.705772270 -1.719688562 -2.123423177 1.619808115 ## [66] -3.826059870 -1.864032144 1.330204157 2.087342480 -2.782158129 ## [71] -0.403227138 -1.468562228 -1.026899366 -2.000743246 0.042009453 ## [76] -0.473418979 2.841051621 -2.703355211 0.700489075 -0.592450346 ## [81] -1.683073694 0.229914942 -5.462264420 -5.187776409 1.969480413 ## [86] -2.450285233 -0.005289845 -3.465216049 1.160366954 1.827202816 ## [91] 0.761346358 -1.696197511 -2.962489498 -4.827255075 -3.221340010 ## [96] -1.494866659 -2.714534509 0.673879626 -1.598181064 0.641959448 Here’s the scatterplot with the missing data removed, and the corresponding linear regression fit: p &lt;- qplot(x, y) + geom_smooth(method=&quot;lm&quot;, se=FALSE) p ## Warning: Removed 10 rows containing non-finite values (stat_smooth). ## Warning: Removed 10 rows containing missing values (geom_point). The mean imputation method replaces the NA’s with an estimate for the mean of \\(Y\\). The simplest case is to use the sample average of the response. The imputed observations are shown in red, and the resulting lm fit is also in red. ybar &lt;- mean(y, na.rm=TRUE) datrm &lt;- na.omit(data.frame(x=x, y=y)) datimp &lt;- data.frame(x=x[1:10], y=ybar) p + geom_point(data=datimp, colour=&quot;red&quot;) + geom_smooth(data=rbind(datrm, datimp), method=&quot;lm&quot;, se=FALSE, colour=&quot;red&quot;) ## Warning: Removed 10 rows containing non-finite values (stat_smooth). ## Warning: Removed 10 rows containing missing values (geom_point). Notice that the new regression line is flatter. Another mean-imputation method is to replace the NA’s with an alternative mean estimate: the regression predictions. fit2 &lt;- lm(y ~ x, na.action=na.omit) yhat &lt;- predict(fit2, newdata=data.frame(x=x[1:10])) datimp2 &lt;- data.frame(x=x[1:10], y=yhat) p + geom_point(data=datimp2, colour=&quot;red&quot;) + geom_smooth(data=rbind(datrm, datimp2), method=&quot;lm&quot;, se=FALSE, colour=&quot;red&quot;, size=0.5) ## Warning: Removed 10 rows containing non-finite values (stat_smooth). ## Warning: Removed 10 rows containing missing values (geom_point). The regression line has not changed. This method seems smarter, but it still has consequences, since the imputed data suggests that the dataset is bound closer to the regression line than reality. So the residual variance is biased to be smaller. These are both mean imputation methods. So, in your Lab 2 assignment, you can use any mean imputation method – your explanation of the comparison will just depend on what you choose. 22.2 Multiple Imputation Recall that multiple imputation is a technique for handling missing data. It replaces the missing data with many plausible values, to obtain mutliple data sets. An analysis is done on each data set, and the results are combined. A very powerful R package to assist with multiple imputation is the mice package. Some key things that it does: Displays patterns in missing data. Imputes data to obtain multiple data sets. Pools multiple analyses into one. We’ll look at the airquality dataset in R. head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 22.2.1 Patterns Where are the NAs? md.pattern(airquality) ## Wind Temp Month Day Solar.R Ozone ## 111 1 1 1 1 1 1 0 ## 35 1 1 1 1 1 0 1 ## 5 1 1 1 1 0 1 1 ## 2 1 1 1 1 0 0 2 ## 0 0 0 0 7 37 44 A “1” indicates that an observation is present, and a “0” indicates absense. The periphery of the matrix are counts: to the right, are the number of NAs in the row; at the bottom, are the number of NAs in each column; to the left, are the number of observations having a missing data pattern indicated in the matrix. So we can see that there are 7 missing Solar Radiation observations, and 37 missing Ozone observations. We could check that in another way as follows: sum(is.na(airquality$Solar.R)) ## [1] 7 sum(is.na(airquality$Ozone)) ## [1] 37 22.2.2 Multiple Imputation There are many methods of doing an imputation. But generally, they use other columns in the data set to do prediction on the missing data. The function to do this is mice. Let’s impute 50 data sets using the “Predictive Mean Matching” method. (dats &lt;- mice(airquality, m=50, method=&quot;pmm&quot;, seed=123, printFlag=FALSE)) ## Class: mids ## Number of multiple imputations: 50 ## Imputation methods: ## Ozone Solar.R Wind Temp Month Day ## &quot;pmm&quot; &quot;pmm&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## Ozone Solar.R Wind Temp Month Day ## Ozone 0 1 1 1 1 1 ## Solar.R 1 0 1 1 1 1 ## Wind 1 1 0 1 1 1 ## Temp 1 1 1 0 1 1 ## Month 1 1 1 1 0 1 ## Day 1 1 1 1 1 0 The m argument is the number of imputed datasets. method is the method (you can check out the other methods in the “Details” part of the documentation of mice). Because there’s a random component to the imputation, seed indicates the seed to initiate the random number generator – useful for reproducibility! Finally, I didn’t want mice to be verbose with its output, so I silenced it with printFlag=FALSE. dats isn’t just a list of 50 datasets. It has more information bundled in it. The info is bundled in an object of type “mids”: class(dats) ## [1] &quot;mids&quot; But we can extract the data sets. Want to see the fourth imputed data set? Here it is: head(mice::complete(dats, 4)) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 8 127 14.3 56 5 5 ## 6 28 314 14.9 66 5 6 22.2.3 Pooling The mice package allows you to pool many types of regression analyses. Let’s try a simple linear regression to predict Ozone from Solar.R, Wind, and Temp. You’ll need to use base R’s with function. fits &lt;- with(dats, lm(Ozone ~ Solar.R + Wind + Temp)) If you were to print fits to the screen, it would look like a list of 50 regression fits – one for each of the imputed data sets. But it’s not. Take a look: names(fits) ## [1] &quot;call&quot; &quot;call1&quot; &quot;nmis&quot; &quot;analyses&quot; Like dats, fits has more info in it. But it does have the 50 regression fits. And they can be pooled using the pool function: (fit &lt;- pool(fits)) ## Class: mipo m = 50 ## estimate ubar b t dfcom ## (Intercept) -61.90301171 3.811506e+02 1.307680e+02 5.145339e+02 149 ## Solar.R 0.05910719 4.108874e-04 1.037848e-04 5.167480e-04 149 ## Wind -3.11988214 3.146474e-01 1.181007e-01 4.351101e-01 149 ## Temp 1.59210031 4.685921e-02 1.183499e-02 5.893090e-02 149 ## df riv lambda fmi ## (Intercept) 94.76599 0.3499492 0.2592314 0.2743853 ## Solar.R 106.27508 0.2576387 0.2048591 0.2194121 ## Wind 91.16698 0.3828499 0.2768557 0.2922145 ## Temp 106.27817 0.2576162 0.2048448 0.2193977 summary(fit) ## estimate std.error statistic df p.value ## (Intercept) -61.90301171 22.68334009 -2.729008 94.76599 7.435912e-03 ## Solar.R 0.05910719 0.02273209 2.600165 106.27508 1.064336e-02 ## Wind -3.11988214 0.65962874 -4.729755 91.16698 6.945757e-06 ## Temp 1.59210031 0.24275688 6.558415 106.27817 2.021784e-09 And there are the results of the pooled fit. This pooling works for more than just lm! "],
["regression-under-many-groups-mixed-effects-models.html", "Chapter 23 Regression under many groups: mixed effects models 23.1 Motivation for LME 23.2 Mixed Effects Models in R: tutorial", " Chapter 23 Regression under many groups: mixed effects models Caution: in a highly developmental stage! See Section 1.1. suppressPackageStartupMessages(library(tidyverse)) ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 23.1 Motivation for LME Let’s take a look at the esoph data set, to see how the number of controls ncontrols affects the number of cases ncases of cancer for each age group agegp. Here’s what the data look like (with a tad bit of vertical jitter): It seems each age group has a different relationship. Should we then fit regression lines for each group separately? Here’s what we get, if we do: But, each group has so few observations, making the regression less powerful: ## # A tibble: 6 x 2 ## agegp n ## &lt;ord&gt; &lt;int&gt; ## 1 25-34 15 ## 2 35-44 15 ## 3 45-54 16 ## 4 55-64 16 ## 5 65-74 15 ## 6 75+ 11 Question: can we borrow information across groups to strengthen regression, while still allowing each group to have its own regression line? Here’s another scenario: suppose we want to know the effect of ncontrols on the average person. Then, we would only include one common slope parameter for all individuals. Even if each individual “has their own unique slope”, this model is still sensible because the common slope can be interpreted as the average effect. The problem with this model is that the typical estimates of standard error on our regression coefficients will be artificially small due to correlation in the data induced by the grouping. Here is a simulation that compares the “actual” SE (or at least an approximation of it) and the SE reported by lm: # library(tidyverse) # library(broom) # set.seed(1000) # ## Number of groups # g &lt;- 10 # ## Number of observations per group # ng &lt;- 10 # ## Initiate slope and SE estimates # beta1hat &lt;- numeric(0) # se &lt;- numeric(0) # for (i in 1:1000) { # ## Generate intercept and slope from a joint Normal distribution # beta0 &lt;- rnorm(g) # beta1 &lt;- 1 + beta0 + rnorm(g) # ## Generate iid data from within each group # esoph &lt;- tibble(group=LETTERS[1:g], beta0, beta1) %&gt;% # mutate(x = map(beta0, ~ rnorm(ng))) %&gt;% # unnest() %&gt;% # group_by(group) %&gt;% # mutate(eps = rnorm(length(x)), # y = beta0 + beta1 * x + eps) # ## Fit a linear regression, forcing a common slope # fit &lt;- lm(y ~ x + group, data=esoph) %&gt;% # tidy() # beta1hat[i] &lt;- fit$estimate[2] # se[i] &lt;- fit$std.error[2] # } # ## Actual SE: # sd(beta1hat) # ## SE given from the lm fit: # mean(se) # # ## Here&#39;s a plot of the last sample generated: # ggplot(esoph, aes(x, y)) + # geom_point(aes(colour=group), alpha=0.5) + # theme_bw() Question: How can we account for the dependence in the data? Both questions can be addressed using a Linear Mixed Effects (LME) model. An LME model is just a linear regression model for each group, with different slopes and intercepts, but the collection of slopes and intercepts is assumed to come from some normal distribution. 23.1.1 Definition With one predictor (\\(X\\)), we can write an LME as follows: \\[ Y = \\left(\\beta_0 + b_0\\right) + \\left(\\beta_1 + b_1\\right) X + \\varepsilon, \\] where the error term \\(\\varepsilon\\) has mean zero, and the \\(b_0\\) and \\(b_1\\) terms are normally distributed having a mean of zero, and some unknown variances and correlation. The \\(\\beta\\) terms are called the fixed effects, and the \\(b\\) terms are called the random effects. Since the model has both types of effects, it’s said to be a mixed model – hence the name of “LME”. Note that we don’t have to make both the slope and intercept random. For example, we can remove the \\(b_0\\) term, which would mean that each group is forced to have the same (fixed) intercept \\(\\beta_0\\). Also, we can add more predictors (\\(X\\) variables). 23.1.2 R Tools for Fitting Two R packages exist for working with mixed effects models: lme4 and nlme. We’ll be using the lme4 package (check out this discussion on Cross Valiesophed for a comparison of the two packages). Let’s fit the model. We need to indicate a formula first in the lmer function, and indicate the data set we’re using. fit &lt;- lme4::lmer(ncases ~ ncontrols + (ncontrols | agegp), data=esoph) Let’s take a closer look at the formula, which in this case is ncases ~ ncontrols + (ncontrols | agegp). On the left of the ~ is the response variable, as usual (just like for lm). On the right, we need to specify both the fixed and random effects. The fixed effects part is the same as usual: ncontrols indicates the explanatory variables that get a fixed effect. Then, we need to indicate which explanatory variables get a random effect. The random effects can be indicated in parentheses, separated by +, followed by a |, after which the variable(s) that you wish to group by are indicated. So | can be interpreted as “grouped by”. Now let’s look at the model output: summary(fit) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: ncases ~ ncontrols + (ncontrols | agegp) ## Data: esoph ## ## REML criterion at convergence: 388.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.6510 -0.3710 -0.1301 0.3683 4.8056 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## agegp (Intercept) 1.694453 1.30171 ## ncontrols 0.005729 0.07569 0.26 ## Residual 3.732899 1.93207 ## Number of obs: 88, groups: agegp, 6 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 1.63379 0.59994 2.723 ## ncontrols 0.04971 0.03676 1.352 ## ## Correlation of Fixed Effects: ## (Intr) ## ncontrols 0.038 The random and fixed effects are indicated here. Under the “Random effects:” section, we have the variance of each random effect, and the lower part of the correlation matrix of these random effects. Under the “Fixed effects:” section, we have the estimates of the fixed effects, as well as the uncertainty in the estimate (indicated by the Std. Error). We can extract the collection of slopes and intercepts for each group using the coef function: (par_coll &lt;- coef(fit)[[1]]) ## (Intercept) ncontrols ## 25-34 0.2674067 -0.002914520 ## 35-44 0.7227280 -0.001127293 ## 45-54 2.2834139 0.036587885 ## 55-64 3.5108403 0.064242966 ## 65-74 1.8699415 0.171918181 ## 75+ 1.1484332 0.029581764 Let’s put these regression lines on the plot: ## Warning: Column `agegp` joining character vector and factor, coercing into ## character vector ## agegp (Intercept) ncontrols.x alcgp tobgp ncases ncontrols.y ## 1 25-34 0.2674067 -0.002914520 0-39g/day 0-9g/day 0 40 ## 2 25-34 0.2674067 -0.002914520 0-39g/day 10-19 0 10 ## 3 25-34 0.2674067 -0.002914520 0-39g/day 20-29 0 6 ## 4 25-34 0.2674067 -0.002914520 0-39g/day 30+ 0 5 ## 5 25-34 0.2674067 -0.002914520 40-79 0-9g/day 0 27 ## 6 25-34 0.2674067 -0.002914520 40-79 10-19 0 7 ## 7 25-34 0.2674067 -0.002914520 40-79 20-29 0 4 ## 8 25-34 0.2674067 -0.002914520 40-79 30+ 0 7 ## 9 25-34 0.2674067 -0.002914520 80-119 0-9g/day 0 2 ## 10 25-34 0.2674067 -0.002914520 80-119 10-19 0 1 ## 11 25-34 0.2674067 -0.002914520 80-119 30+ 0 2 ## 12 25-34 0.2674067 -0.002914520 120+ 0-9g/day 0 1 ## 13 25-34 0.2674067 -0.002914520 120+ 10-19 1 1 ## 14 25-34 0.2674067 -0.002914520 120+ 20-29 0 1 ## 15 25-34 0.2674067 -0.002914520 120+ 30+ 0 2 ## 16 35-44 0.7227280 -0.001127293 0-39g/day 0-9g/day 0 60 ## 17 35-44 0.7227280 -0.001127293 0-39g/day 10-19 1 14 ## 18 35-44 0.7227280 -0.001127293 0-39g/day 20-29 0 7 ## 19 35-44 0.7227280 -0.001127293 0-39g/day 30+ 0 8 ## 20 35-44 0.7227280 -0.001127293 40-79 0-9g/day 0 35 ## 21 35-44 0.7227280 -0.001127293 40-79 10-19 3 23 ## 22 35-44 0.7227280 -0.001127293 40-79 20-29 1 14 ## 23 35-44 0.7227280 -0.001127293 40-79 30+ 0 8 ## 24 35-44 0.7227280 -0.001127293 80-119 0-9g/day 0 11 ## 25 35-44 0.7227280 -0.001127293 80-119 10-19 0 6 ## 26 35-44 0.7227280 -0.001127293 80-119 20-29 0 2 ## 27 35-44 0.7227280 -0.001127293 80-119 30+ 0 1 ## 28 35-44 0.7227280 -0.001127293 120+ 0-9g/day 2 3 ## 29 35-44 0.7227280 -0.001127293 120+ 10-19 0 3 ## 30 35-44 0.7227280 -0.001127293 120+ 20-29 2 4 ## 31 45-54 2.2834139 0.036587885 0-39g/day 0-9g/day 1 46 ## 32 45-54 2.2834139 0.036587885 0-39g/day 10-19 0 18 ## 33 45-54 2.2834139 0.036587885 0-39g/day 20-29 0 10 ## 34 45-54 2.2834139 0.036587885 0-39g/day 30+ 0 4 ## 35 45-54 2.2834139 0.036587885 40-79 0-9g/day 6 38 ## 36 45-54 2.2834139 0.036587885 40-79 10-19 4 21 ## 37 45-54 2.2834139 0.036587885 40-79 20-29 5 15 ## 38 45-54 2.2834139 0.036587885 40-79 30+ 5 7 ## 39 45-54 2.2834139 0.036587885 80-119 0-9g/day 3 16 ## 40 45-54 2.2834139 0.036587885 80-119 10-19 6 14 ## 41 45-54 2.2834139 0.036587885 80-119 20-29 1 5 ## 42 45-54 2.2834139 0.036587885 80-119 30+ 2 4 ## 43 45-54 2.2834139 0.036587885 120+ 0-9g/day 4 4 ## 44 45-54 2.2834139 0.036587885 120+ 10-19 3 4 ## 45 45-54 2.2834139 0.036587885 120+ 20-29 2 3 ## 46 45-54 2.2834139 0.036587885 120+ 30+ 4 4 ## 47 55-64 3.5108403 0.064242966 0-39g/day 0-9g/day 2 49 ## 48 55-64 3.5108403 0.064242966 0-39g/day 10-19 3 22 ## 49 55-64 3.5108403 0.064242966 0-39g/day 20-29 3 12 ## 50 55-64 3.5108403 0.064242966 0-39g/day 30+ 4 6 ## 51 55-64 3.5108403 0.064242966 40-79 0-9g/day 9 40 ## 52 55-64 3.5108403 0.064242966 40-79 10-19 6 21 ## 53 55-64 3.5108403 0.064242966 40-79 20-29 4 17 ## 54 55-64 3.5108403 0.064242966 40-79 30+ 3 6 ## 55 55-64 3.5108403 0.064242966 80-119 0-9g/day 9 18 ## 56 55-64 3.5108403 0.064242966 80-119 10-19 8 15 ## 57 55-64 3.5108403 0.064242966 80-119 20-29 3 6 ## 58 55-64 3.5108403 0.064242966 80-119 30+ 4 4 ## 59 55-64 3.5108403 0.064242966 120+ 0-9g/day 5 10 ## 60 55-64 3.5108403 0.064242966 120+ 10-19 6 7 ## 61 55-64 3.5108403 0.064242966 120+ 20-29 2 3 ## 62 55-64 3.5108403 0.064242966 120+ 30+ 5 6 ## 63 65-74 1.8699415 0.171918181 0-39g/day 0-9g/day 5 48 ## 64 65-74 1.8699415 0.171918181 0-39g/day 10-19 4 14 ## 65 65-74 1.8699415 0.171918181 0-39g/day 20-29 2 7 ## 66 65-74 1.8699415 0.171918181 0-39g/day 30+ 0 2 ## 67 65-74 1.8699415 0.171918181 40-79 0-9g/day 17 34 ## 68 65-74 1.8699415 0.171918181 40-79 10-19 3 10 ## 69 65-74 1.8699415 0.171918181 40-79 20-29 5 9 ## 70 65-74 1.8699415 0.171918181 80-119 0-9g/day 6 13 ## 71 65-74 1.8699415 0.171918181 80-119 10-19 4 12 ## 72 65-74 1.8699415 0.171918181 80-119 20-29 2 3 ## 73 65-74 1.8699415 0.171918181 80-119 30+ 1 1 ## 74 65-74 1.8699415 0.171918181 120+ 0-9g/day 3 4 ## 75 65-74 1.8699415 0.171918181 120+ 10-19 1 2 ## 76 65-74 1.8699415 0.171918181 120+ 20-29 1 1 ## 77 65-74 1.8699415 0.171918181 120+ 30+ 1 1 ## 78 75+ 1.1484332 0.029581764 0-39g/day 0-9g/day 1 18 ## 79 75+ 1.1484332 0.029581764 0-39g/day 10-19 2 6 ## 80 75+ 1.1484332 0.029581764 0-39g/day 30+ 1 3 ## 81 75+ 1.1484332 0.029581764 40-79 0-9g/day 2 5 ## 82 75+ 1.1484332 0.029581764 40-79 10-19 1 3 ## 83 75+ 1.1484332 0.029581764 40-79 20-29 0 3 ## 84 75+ 1.1484332 0.029581764 40-79 30+ 1 1 ## 85 75+ 1.1484332 0.029581764 80-119 0-9g/day 1 1 ## 86 75+ 1.1484332 0.029581764 80-119 10-19 1 1 ## 87 75+ 1.1484332 0.029581764 120+ 0-9g/day 2 2 ## 88 75+ 1.1484332 0.029581764 120+ 10-19 1 1 So, each group still gets its own regression line, but tying the parameters together with a normal distribution gives us a more powerful regression. 23.2 Mixed Effects Models in R: tutorial Caution: in a highly developmental stage! See Section 1.1. Two R packages exist for working with mixed effects models: lme4 and nlme. We’ll be using the lme4 package (check out this discussion on Cross Valiesophed for a comparison of the two packages). In Lab 1, we compared linear regression (function lm) with GLM’s (function glm). In Lab 2, we consider adding a random effect to either of these: A linear model with random effects is a Linear Mixed-Effects Model, and is fit using the lmer function. A generalized linear model with random effects is a Generalized Linear Mixed-Effects Model, and is fit using the glmer function. We’ll work with the esoph data set, to see how the number of controls ncontrols affects the number of cases ncases based on age group agegp. Here’s what the data look like (with a tad bit of vertical jitter): p &lt;- ggplot(esoph, aes(ncontrols, ncases)) + geom_jitter(aes(colour=agegp), height=0.25) p Since the response is a count variable, we’ll go ahead with a Poisson regression – a Generalized Linear Mixed-Effects Model. The model is \\[ Y_{ij} \\mid X_{ij} = x_{ij} \\sim \\text{Poisson}\\left(\\lambda_{ij}\\right) \\] for each observation \\(i\\) on the \\(j\\)’th age group, where \\(Y_{ij}\\) is the number of cases, \\(X_{ij}\\) is the number of controls, and \\(\\lambda_{ij}\\) is the conditional mean of \\(Y_{ij}.\\) We model \\(\\lambda_{ij}\\) as \\[ \\log\\left(\\lambda_{ij}\\right) = \\left(\\beta_0 + b_{0j}\\right) + \\left(\\beta_1 + b_{1j}\\right) x_{ij}, \\] where \\(b_{0j}\\) and \\(b_{1j}\\) are joint (bivariate) normally distributed with zero mean. What does this model mean? First, it means that the mean is exponential in the explanatory variable, since we chose a \\(\\log\\) link function. Second, each age group (\\(j\\)) gets its own mean curve, via its own linear predictor. But we’re saying that these linear predictors are related: the collection of slopes and intercepts across age groups are centered around \\(\\beta_0\\) and \\(\\beta_1\\) (respectively, called the fixed effects), and the slope and intercept of each age group departs from this center according to some Gaussian random noise (the \\(b\\) terms, called the random effects). Let’s fit the model. Then we’ll go through the formula, and the output. fit &lt;- lme4::glmer(ncases ~ ncontrols + (1 + ncontrols | agegp), data=esoph, family=poisson) summary(fit) ## Generalized linear mixed model fit by maximum likelihood (Laplace ## Approximation) [glmerMod] ## Family: poisson ( log ) ## Formula: ncases ~ ncontrols + (1 + ncontrols | agegp) ## Data: esoph ## ## AIC BIC logLik deviance df.resid ## 315.1 327.5 -152.5 305.1 83 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.8527 -0.7919 -0.3286 0.4776 3.8037 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## agegp (Intercept) 1.2343732 1.11102 ## ncontrols 0.0003231 0.01797 0.66 ## Number of obs: 88, groups: agegp, 6 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.135467 0.483821 0.280 0.779 ## ncontrols 0.006613 0.013945 0.474 0.635 ## ## Correlation of Fixed Effects: ## (Intr) ## ncontrols 0.174 To specify the formula, the fixed effects part is the same as usual: ncases ~ ncontrols gives you ncases = beta0 + beta1 * ncontrols. Note that the intercept is put in there by default. Then, we need to indicate which explanatory variables are getting the random effects – including the intercept this time (with a 1), if you want it (in this case, we do). The random effects can be indicated in parentheses, separated by +, followed by a |, after which the variable(s) that you wish to group by are indicated. So | can be interpreted as “grouped by”. The output of the model fit is similar to what you’ve seen before (in glm for example), but the “random effects” part is new. That gives us the estimates of the joint normal distribution of the random effects – through the variances, and correlation matrix to the right (only the lower-diagonal of the correlation matrix is given, because that matrix is symmetric anyway). Let’s see what the intercepts and slopes for each age group are, and let’s plot the estimated mean curves: (coef_fit &lt;- coef(fit)$agegp) ## (Intercept) ncontrols ## 25-34 -1.7354568 -0.015913381 ## 35-44 -0.4511795 -0.001500806 ## 45-54 0.8834953 0.010877993 ## 55-64 1.3497975 0.011718494 ## 65-74 0.8568544 0.031875270 ## 75+ 0.1396592 0.006661840 ## Colours with stat_function are not nice to deal with. Do manually. p + stat_function(aes(colour=&quot;25-34&quot;), fun = function(x) exp(coef_fit[1,1] + coef_fit[1,2]*x)) + stat_function(aes(colour=&quot;35-44&quot;), fun = function(x) exp(coef_fit[2,1] + coef_fit[2,2]*x)) + stat_function(aes(colour=&quot;45-54&quot;), fun = function(x) exp(coef_fit[3,1] + coef_fit[3,2]*x)) + stat_function(aes(colour=&quot;55-64&quot;), fun = function(x) exp(coef_fit[4,1] + coef_fit[4,2]*x)) + stat_function(aes(colour=&quot;65-74&quot;), fun = function(x) exp(coef_fit[5,1] + coef_fit[5,2]*x)) + stat_function(aes(colour=&quot;75+&quot;), fun = function(x) exp(coef_fit[6,1] + coef_fit[6,2]*x)) A (response-) residual plot is somewhat sensible to look at here: plot(fit) Looks fairly centered at zero, so the shape of the mean curves are satisfactory. "],
["regression-on-an-entire-distribution-probabilistic-forecasting.html", "Chapter 24 Regression on an entire distribution: Probabilistic Forecasting 24.1 Probabilistic Forecasting: What it is 24.2 Review: Univariate distribution estimates 24.3 Probabilistic Forecasts: subset-based learning methods 24.4 Discussion Points 24.5 When are they not useful?", " Chapter 24 Regression on an entire distribution: Probabilistic Forecasting Caution: in a highly developmental stage! See Section 1.1. suppressPackageStartupMessages(library(tidyverse)) ## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2 ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 ## Warning: package &#39;dplyr&#39; was built under R version 3.5.2 ## Warning: package &#39;stringr&#39; was built under R version 3.5.2 baseball &lt;- Lahman::Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) Up until now, we’ve only seen different ways of using a predictor to give us more information the mean and mode of the response. The world holds a huge emphasis on the mean and mode, but these are not always what’s important. Two alternatives are: Probabilistic forecasting Quantile Regression (numeric response only) 24.1 Probabilistic Forecasting: What it is The idea here is to put forth an entire probability distribution as a prediction. Let’s look at an example. Suppose there are two baseball teams, one that gets 1000 total hits in a year, and another that gets 1500. Using “total hits in a year” as a predictor, we set out to predict the total number of runs of both teams. Here’s the top snippet of the data: baseball ## # A tibble: 2,835 x 2 ## runs hits ## &lt;int&gt; &lt;int&gt; ## 1 401 426 ## 2 302 323 ## 3 249 328 ## 4 137 178 ## 5 302 403 ## 6 376 410 ## 7 231 274 ## 8 351 384 ## 9 310 375 ## 10 617 747 ## # … with 2,825 more rows Let’s not concern ourselves with the methods yet. Using a standard regression technique, here are our predictions: r &lt;- 20 datsub &lt;- filter(baseball, (hits&gt;=1000-r &amp; hits&lt;=1000+r) | (hits&gt;=1500-r &amp; hits&lt;=1500+r)) %&gt;% mutate(approx_hits = if_else(hits&gt;=1000-r &amp; hits&lt;=1000+r, 1000, 1500)) datsub %&gt;% group_by(approx_hits) %&gt;% summarize(expected_runs=round(mean(runs))) %&gt;% rename(hits=approx_hits) %&gt;% select(hits, expected_runs) ## # A tibble: 2 x 2 ## hits expected_runs ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1000 558 ## 2 1500 768 Using a probabilistic forecast, here are our predictions: Don’t you think this is far more informative than the mean estimates in the above table? The probabilistic forecast/prediction contains the most amount of information about the response as possible (based on a set of predictors), because it communicates the entire belief of what \\(Y\\) values are most plausible, given values of the predictor. Predictions/forecasts here are called predictive distributions. From @gneiting_raftery: Indeed, over the past two decades, probabilistic forecasting has become routine in such applications as weather and climate prediction (Palmer 2002; Gneiting and Raftery 2005), computational finance (Duffle and Pan 1997), and macroeconomic forecasting (Garratt, Lee, Pesaran, and Shin 2003; Granger 2006). 24.2 Review: Univariate distribution estimates Let’s review how to estimate a univariate probability density function or probability mass function. 24.2.1 Continuous response Here’s a random sample of 10 continuous variables, ordered from smallest to largest, stored in the variable x: Recall that we can use histograms to estimate the density of the data. The idea is: Cut the range of the data into “bins” of a certain width. For these data, the range is 40. Let’s set up four bins of width 10: -19.8 to -9.8, -9.8 to 0.2, etc. Count the number of observations that fall into each bin. For our setup, the number of observations falling into the four bins, in order, are: 3,2,2,3. Make a bar plot (with no space between the bars), where the bar width corresponds to the bins, and the bar height corresponds to the number of observations in that bin. For our setup, we have: ggplot(data.frame(x=x), aes(x)) + geom_histogram(binwidth=10, center=min(x)+5, fill=&quot;orange&quot;, colour=&quot;black&quot;) + theme_bw() (Note: this is not a true density, since the area under the curve is not 1, but the shape is what matters) You’d have to play with the binwidth to get a histogram that looks about right (not too jagged, not too coarse). For the above example, there are too few data to make a good estimate. Let’s now generate 1000 observations, and make a histogram using qplot from R’s ggplot2 package, with a variety of binwidths – too small, too large, and just right. x &lt;- rnorm(1000, sd=10) qplot(x, binwidth=1) # too small qplot(x, binwidth=10) # too big qplot(x, binwidth=3.5) # just right Advanced method: There’s a technique called the kernel density estimate that works as an alernative to the histogram. The idea is to put a “little mound” (a kernel) on top of each observation, and add them up. Instead of playing with the binwidth, you can play with the “bandwidth” of the kernels. Use geom=&quot;density&quot; in qplot, and use bw to play with the bandwidth: qplot(x, geom=&quot;density&quot;, bw=2.5) 24.2.2 Discrete Response When the response is discrete (this includes categorical), the approach is simpler: Calculate the proportion of observations that fall into each category. Make a bar chart, placing a bar over each category, and using the proportions as the bar heights. Here are ten observations, stored in x: x ## [1] 1 0 0 0 2 0 1 2 3 0 The proportions are as follows: props &lt;- tibble(Value=x) %&gt;% group_by(Value) %&gt;% summarize(Proportion=length(Value)/length(x)) You can plot these proportions with qplot, specifying geom=&quot;col&quot;: qplot(x=Value, y=Proportion, data=props, geom=&quot;col&quot;) You can use ggplot2 to calculate the proportions, but it’s more complex. It’s easier to plot the raw counts, instead of proportions – and that’s fine, you’ll still get the same shape. Using qplot again, let’s make a plot for 1000 observations (note that I indicate that my data are discrete by using the factor function): set.seed(2) x &lt;- rpois(1000, lambda=1) qplot(factor(x)) Here’s the code to get proportions instead of counts: qplot(factor(x), mapping=aes(y=..prop..), group=1) 24.3 Probabilistic Forecasts: subset-based learning methods 24.3.1 The techniques The local methods and classification/regression trees that we’ve seen so far can be used to produce probabilistic forecasts. For local methods, let’s ignore the complications of kernel weighting and local polynomials. These methods result in a subset of the data, for which we’re used to taking the mean or mode. Instead, use the subsetted data to plot a distribution. For kNN, form a histogram/density plot/bar plot using the \\(k\\) nearest neighbours. For the moving window (loess), form a histogram/density plot/bar plot using the observations that fall in the window. For tree-based methods, use the observations within a leaf to form a histogram/density plot/bar plot for that leaf. The above baseball example used a moving window with a radius of 20 hits. Visually, you can see the data that I subsetted within these two narrow windows, for hits of 1000 and 1500: ggplot(baseball, aes(hits, runs)) + geom_point(colour=&quot;orange&quot;, alpha=0.1) + geom_vline(xintercept=c(1000+c(-r,r), 1500+c(-r,r)), linetype=&quot;dashed&quot;) + theme_bw() + labs(x=&quot;Number of Hits (X)&quot;, y=&quot;Number of Runs (Y)&quot;) 24.3.2 Exercise Install the Lahman package, which contains the Teams dataset. Build a null model probabilistic forecast of “number of runs” (R column). Build a probabilistic forecast, using kNN, of “number of runs” for a team that has 1500 hits (H column) and 70 wins (W column). Don’t forget to scale the predictors! Do the same thing, but using linear regression. What additional assumption(s) is/are needed here? 24.3.3 Bias-variance tradeoff Let’s examine the bias-variance / overfitting-underfitting tradeoff with kNN-based probabilistic forecasts. I’ll run a simulation like so: Generate data from a bivariate Normal distribution, so that \\(X \\sim N(0, 100)\\), and \\(Y = X + N(0, 100)\\). Training data will contain 500 observations, for which a kNN probabilistic forecast will be built when \\(X=25\\). Try both a small (k=15) and large (k=100) value of \\(k\\). For each value of \\(k\\), we’ll generate 20 training data sets. Here are the 20 estimates for the values of \\(k\\). The overall mean of the distributions are indicated by a vertical dashed line. Notice that: When \\(k\\) is large, our estimates are biased, because the distributions are not centered correctly. But, the estimates are more consistent. When \\(k\\) is small, our estimates are less biased, because the distributions overall have a mean that is close to the true mean. But the variance is high – we get all sorts of distribution shapes here. A similar thing happens with a moving window, with the window width parameter. For tree-based methods, the amount that you partition the predictor space controls the bias-variance tradeoff. 24.3.4 Evaluating Model Goodness To choose a balance between bias and variance, we need a measure of prediction goodness. When predicting the mean, the MSE works. When predicting the mode, the classification error works. But what works for probabilistic forecasts? This is an active area of research. The idea is to use a proper scoring rule – a way of assigning a score based on the forecast distribution and the outcome only, that also encourages honesty. We won’t go into details – see [@gneiting_raftery] for details. At the very least, one should check that the forecast distributions are “calibrated” – that is, the actual outcomes are spread evenly amongst the forecasts. You can check this by applying the forecast cdf to the corresponding outcome – the resulting sample should be Uniform(0,1). Note that this is built-in to at least some proper scoring rules. For this course, we won’t be picky about how you choose your tuning parameters. Just look for a subset that you think has “enough” observations in it so that the distribution starts to take some shape, but not so much that it starts to shift. 24.4 Discussion Points For (1) and (2) below, you’re choosing between two candidates to hire. Discuss the pros and cons of choosing one candidate over the other in the following situations. Both are predicted to have the same productivity score of 75, but have the following probabilistic forecasts. It’s hard to make a decision here. On the one hand, we can be fairly certain that the actual productivity of candidate A will be about 75, but there’s more of a gamble with candidate B. There’s a very real chance that B’s productivity is actually quite a bit higher than A – for example, a productivity of 80 is plausible for B, but not for A. On the other hand, there’s also a very real chance that B’s productivity is actually quite a bit lower than A, for the same reason. Your decision would depend on whether you would want to take a risk or not. On the other hand, in reality, this is only one tool out of many other aspects of the candidate that you would consider. It might be a good idea to chat with B to get a better sense of what their productivity might actually be. Two “non-overlapping” forecasts: In this case, B is very very likely to have higher productivity than A, because all “plausible” productivity values for B are higher than all “plausible” productivity values of A. Again, this is just one tool you might use to make a decision. You’ve formed a probabilistic forecast for a particular value of the predictors, displayed below as a density. You then collect test data for that same value of the predictor, indicated as the points below the density. What is the problem with the probabilistic forecast? The forecast is biased, because the actual values are occuring near the upper tail of the distribution – they should be scattered about the middle, with a higher density of points occuring near 0. If using local methods, we’d have to reduce \\(k\\) or the window width to decrease bias (to remove “further” data that are less relevant); if using a tree-based method, you could grow the tree deeper to lower the bias. 24.5 When are they not useful? Probabilistic forecasts are useful if you’re making a small amount of decisions at a time. For example: Predicting which hockey team will win the Stanley Cup Looking at the 2-day-ahead prediction of river flow every day to decide whether to take flood mitigation measures. But they are not appropriate when making decisions en-masse. For example: A bus company wants to know how long it takes a bus to travel between stops, for all stops and all busses. You want to predict future behaviour of customers. "],
["regression-when-order-matters-time-series-and-spatial-analysis.html", "Chapter 25 Regression when order matters: time series and spatial analysis 25.1 Timeseries in (base) R 25.2 Spatial Example 25.3 A Model for River Rock Size 25.4 Statistical Objectives 25.5 Three Concepts 25.6 Estimation 25.7 Statistical Objective 1: Downstream Fining Curve 25.8 Statistical Objective 2: River Profile 25.9 Confidence Intervals of the River Profile", " Chapter 25 Regression when order matters: time series and spatial analysis Caution: in a highly developmental stage! See Section 1.1. 25.1 Timeseries in (base) R To add: times() function to extract times from a ts object. How to deal with the start and end arguments when declaring a ts object. tsibble. This tutorial demonstrates timeseries objects, and stl decomposition. Let’s make a periodic time series with a trend. The data can be contained in a vector: p &lt;- 10 n &lt;- 20*p dat &lt;- 100 + sqrt(1:n) + 5*sin(1:n * 2*pi/p) + rnorm(n) It’s sometimes useful to make an object of type timeseries. Do this with the ts function in R. But, if there’s a cycle, we’ll need to indicate that in the frequency argument, which is the number of observations per cycle. In this case, the period is 10. (datts &lt;- ts(dat, frequency = p)) ## Time Series: ## Start = c(1, 1) ## End = c(20, 10) ## Frequency = 10 ## [1] 103.32889 106.35013 107.56493 103.90212 100.44689 100.02040 99.23977 ## [8] 99.90411 97.80701 103.04150 106.35105 107.54984 108.83248 107.28402 ## [15] 104.74773 99.04764 98.83221 99.00899 102.65845 105.03137 105.70640 ## [22] 109.51352 110.09315 107.64833 105.11708 102.74025 100.98054 100.02756 ## [29] 102.57547 105.38420 109.28836 111.67652 110.79192 109.73707 105.71219 ## [36] 102.52666 101.19743 100.97894 104.35514 106.08092 109.75667 110.64612 ## [43] 110.29309 109.57699 107.70139 105.07604 101.30022 101.73367 105.26116 ## [50] 107.37251 109.83265 113.43234 111.72398 110.63875 107.43417 105.13136 ## [57] 100.99518 102.16019 105.10312 108.37546 110.74573 112.74182 112.81721 ## [64] 111.79372 108.44135 104.31747 102.96235 103.99485 104.63046 108.23121 ## [71] 112.70844 112.60766 114.51234 112.62513 110.42844 105.23155 105.18638 ## [78] 104.41402 105.48675 109.18306 111.56031 116.14705 113.82024 111.36483 ## [85] 109.70297 106.98137 105.36169 104.23700 105.93138 109.86967 112.81569 ## [92] 113.54574 114.99730 112.79152 107.65767 108.65163 105.81281 105.98343 ## [99] 106.28639 110.37149 114.01048 114.63328 116.05280 112.07958 109.50329 ## [106] 108.57387 104.97237 105.94653 108.26723 110.45921 112.70742 114.22830 ## [113] 114.03421 113.29996 108.59782 107.27047 105.88529 106.29127 108.42358 ## [120] 110.68839 115.04421 116.58580 115.79269 113.99425 111.45906 108.32603 ## [127] 106.13172 107.40980 110.04083 113.07690 116.02524 115.97180 118.54013 ## [134] 115.54017 111.46706 109.52001 107.51757 107.35546 108.65786 111.61801 ## [141] 115.57068 116.63465 116.86443 115.38791 111.22910 111.65136 107.54475 ## [148] 106.43798 110.13672 110.94354 114.09149 115.29062 117.36761 115.45760 ## [155] 110.61333 109.66325 107.49430 106.89842 108.69599 112.34607 114.89838 ## [162] 117.92475 118.06219 116.80134 110.62594 109.27855 109.48547 108.38689 ## [169] 109.08309 111.90395 115.71773 117.44988 118.47755 116.06137 111.37075 ## [176] 110.48622 109.81735 108.94256 111.51192 112.55261 116.78280 116.27828 ## [183] 118.25285 117.90620 113.42742 110.53220 108.48663 107.08068 112.21581 ## [190] 115.01808 116.84095 121.74166 119.46233 116.41862 113.42014 111.16250 ## [197] 107.89265 109.47532 110.18262 113.34804 You can plot this object too. You’ll get a nice looking time series plot: plot(datts) And now you can decompose the trend, seasonal component, and error terms with stl. Note that stl requires a timeseries object! Be sure to put s.window=&quot;periodic&quot; in the stl function to use the periodicity of the timeseries object. Notice that there are options to change the bandwidths of the loess estimation, along with the degree of the local polynomial, with the _.degree and _.window arguments. fit &lt;- stl(datts, s.window=&quot;periodic&quot;) The estimates are contained in the $time.series part of the output: head(fit$time.series) ## seasonal trend remainder ## [1,] 2.9368607 101.0823 -0.6902522 ## [2,] 4.7313532 101.3257 0.2931270 ## [3,] 5.0377200 101.5690 0.9581829 ## [4,] 3.0777250 101.8033 -0.9789428 ## [5,] -0.5403572 102.0377 -1.0504015 ## [6,] -2.7452879 102.2631 0.5025873 25.2 Spatial Example Rocks were sampled at 54 sites along the river within a period of two days. The procedure to take one sample involves taking an underwater photo at a glide site along the river, and by using computer software, obtaining the lengths of the intermediate axes of each rock over 8mm in the photo area. To ensure accuracy of measurements, 25 photos of the same area are taken and combined. 25.3 A Model for River Rock Size The variable \\(x\\) refers to a location of some distance downstream the river for example, distance downstream the Meadows campground. For the 54 sample sites, the locations are denoted by \\(x_{1},\\ldots,x_{54}\\). There are three concepts related to rock size. 25.3.1 1. Average rock size: This is the average size of sampled rocks at a location, had that location been sampled. For location \\(x\\), the value of the sample average rock size is denoted \\(m\\left(x\\right)\\). These values are known for 54 sample sites, which are denoted for brevity as \\(m_{1},\\ldots,m_{54}\\), represented as the dots in Figure 25.3.2 2. Mean rock size: At a particular river location, this can be thought of as the average rock size in the bed load in the hypothetical situation where the river flows forever in the same condition as during the sampling period. At any location, this quantity is unknown, and will thus be referred to as a “mean” instead of an “average” (an average is a known calculable quantity). At a location \\(x\\), the mean rock size will be denoted \\(M\\left(x\\right)\\), and is represented by the solid line in Figure 25.3.3 3. Downstream fining curve: This can be thought of as an “overall trend” for rock size from upstream to downstream. The mean rock size \\(M\\left(x\\right)\\) should “on average” follow this curve. For location \\(x\\), the value of the downstream fining curve will be denoted \\(T\\left(x\\right)\\), represented as the dashed line in Figure . At any location, this is an unknown quantity. 25.4 Statistical Objectives Using the three concepts of rock size in Section, the following statistical objectives can be pursued to address the scientific objectives: Estimate the downstream fining curve \\(T\\left(x\\right)\\) (i.e. the dashed line in Figure) for the range of the study area; Estimate the mean rock size \\(M\\left(x\\right)\\) (i.e. the solid line in Figure) for the range of the study area, along with confidence bands These objectives are addressed in Sections, but first some preliminaries are needed, discussed in Section. 25.4.1 Preliminaries: Variance and Correlation Most of the techniques introduced in Sections require three descriptions of rock size, which are defined in Section. The procedure for fitting these descriptions to the data is discussed in Section . For a detailed review of these concepts, see Chapter 2 in reference @Geostatistics, for example. 25.5 Three Concepts 25.5.1 Error Variance \\(\\sigma_{E}^{2}\\left(x\\right)\\) At location \\(x\\), the error variance is the variability of the average rock size \\(m\\left(x\\right)\\) in comparison to the true mean rock size \\(M\\left(x\\right)\\). In other words, it is the variance of \\(\\left(m\\left(x\\right)-M\\left(x\\right)\\right)\\). 25.5.2 Mean Variance \\(\\sigma_{M}^{2}\\) At location \\(x\\), the mean variance is the variability of the true mean \\(M\\left(x\\right)\\) around the downstream fining curve \\(T\\left(x\\right)\\) (i.e. how “tightly” \\(M\\left(x\\right)\\) follows \\(T\\left(x\\right)\\)). It may be reasonable to assume that this variance is constant, unless there is evidence that this variability changes significantly throughout the study area. In what follows we will make this assumption; denote the variance of the true mean rock size as \\(\\sigma_{M}^{2}\\). 25.5.3 Mean Correlation \\(\\rho\\left(d\\right)\\) The mean correlation measures how closely related two mean rock sizes are at different locations (and equivalently, the average rock sizes at those locations). The mean rock sizes at two locations immediately next to each other are expected to be closely related, whereas mean rock sizes at two locations that are far apart may not be related. A key assumption used in Sections is isotropy, that is, the correlation between mean rock sizes at two sites only depends on the distance between the sites, and not on the actual locations along the river. This assumption is of course not exactly true. For example, the relationship between rocks at sites upstream and downstream the dam should be different than the relationship between two equally-spaced sites located without the dam between them. However, the assumption of isotropy should be viewed as an approximation to reality. If \\(d\\) is the distance between two sites, then \\(\\rho\\left(d\\right)\\) denotes the correlation between mean rock sizes at those sites. In technical terms, \\(\\rho\\left(d\\right)=\\text{Corr}\\left(M\\left(x\\right),M\\left(x+d\\right)\\right)\\) for all \\(x\\), \\(x+d\\) in the study area. 25.6 Estimation The quantities introduced in Section \\(\\sigma_{E}^{2}\\left(x\\right)\\), \\(\\sigma_{M}^{2}\\), and \\(\\rho\\left(d\\right)\\) are unknown and need to be estimated, because most of the techniques in Sections use them. There is a relatively simple way to do this estimation in the case that the error variance, \\(\\sigma_{E}^{2}\\left(x\\right)\\), is constant, which is discussed in Section . To account for the changing error variance, an estimation technique is suggested in Section, which may require some manual computations depending on the capabilities of the software you end up using. 25.6.1 Constant Error Variance Although the error variance \\(\\sigma_{E}^{2}\\left(x\\right)\\) is influenced by sample size \\(n\\left(x\\right)\\) and distribution variance \\(\\sigma^{2}\\left(x\\right)\\), approximating the error variance as a constant should be acceptable if the error variances are small compared to the mean variance, \\(\\sigma_{M}^{2}\\). Then, any differences in \\(\\sigma_{E}^{2}\\left(x\\right)\\) amongst different locations would be minuscule relative to \\(\\sigma_{M}^{2}\\). Because your sample sizes are quite large (at least 100), the assumption of small error variances might be reasonable. However, if that assumption is not close to the truth, then the error bars described in Section would be overly wide at locations where the error variance is small (and mean rock size is small), and vice-versa. Since the error variance is assumed constant here, we will denote it \\(\\sigma_{E}^{2}\\), without the \\(\\left(x\\right)\\), since it is assumed not to change with \\(x\\). One tool that incorporates all the quantities in Section is the variogram, often denoted by \\(\\gamma\\left(d\\right)\\). For locations separated by a distance of \\(d\\), it is defined as half the variance of the difference of the sample averages at those sites. Using symbols, the variogram is defined as \\[\\gamma\\left(d\\right)=\\frac{1}{2}\\text{Var}\\left\\{ m\\left(x\\right)-m\\left(x+d\\right)\\right\\} ,\\] where “Var” means “variance of”. Working out the math, the variogram can be expressed as \\[\\gamma\\left(d\\right)=\\sigma_{M}^{2}\\left[1-\\rho\\left(d\\right)\\right]+\\sigma_{E}^{2},\\label{eq:Variogram}\\] which contains all the quantities we need to estimate. Thus, estimating \\(\\sigma_{E}^{2}\\), \\(\\sigma_{M}^{2}\\), and \\(\\rho\\left(d\\right)\\) amounts to estimating the variogram. A plot of the variogram against \\(d\\) might look something like that in Figure . The bottom dashed line is called a nugget, and the top dashed line is called a sill. The nugget equals \\(\\sigma_{E}^{2}\\) and the difference between the sill and the nugget equals \\(\\sigma_{M}^{2}\\). To estimate a variogram, an empirical variogram is typically used (a “data version” of the variogram). An empirical variogram can be viewed as a scatterplot, where one point is plotted for each pair of sample sites. In particular, for two different site numbers \\(i\\) and \\(j\\), a point is plotted with a vertical value of \\(0.5\\left(m_{i}-m_{j}\\right)^{2}\\) and a horizontal value of \\(\\left|x_{i}-x_{j}\\right|\\), where \\(\\left|\\cdot\\right|\\) refers to the absolute value. Then nonlinear regression is used to fit a variogram model to the data in this scatterplot. In R, you will need to specify the “model type”. The model type refers to the form of the mean correlation, \\(\\rho\\left(d\\right)\\). In Figure , the form is exponential, but there are many other forms one can choose, including gaussian, matern, or spherical (see Section 2.5 in Reference @Geostatistics for a discussion of these models). There is no such thing as a “correct” model, but some may be good approximations to the truth. You can assess whether a model is a good approximation by visually checking whether a plot of the fitted variogram is “close” to the empirical (data) variogram. For a more formal assessment, you can choose the model that results in the smallest Akaike Information Criterion (AIC) value, which quantifies a compromise between model simplicity and goodness-of-fit. Yet another option is to assess the residual plot, where a “residual” here is the difference between a point on the empirical variogram and the theoretical variogram. Although this residual plot is slightly different from “traditional” residual plots (since the points are not independent), ensuring the residuals are roughly centered around zero is still useful for assessing the model fit (you were wondering where an “analysis of residuals” could be used in your analysis this is one place). If several models seem to be a good approximation, then it is best to choose the simplest one (the exponential form is one of the simplest, and most popular). 25.6.2 Non-Constant Error Variance If you fit a variogram model to your data as discussed in Section , and you find that the nugget is fairly large in comparison to the sill, then it is probably not realistic to approximate the error variance by a constant. This is because the fluctuations in the error variance \\(\\sigma_{E}^{2}\\left(x\\right)\\) (which we know exist) would no longer be small relative to the mean variance \\(\\sigma_{M}^{2}\\). In the case that you would like to account for the differences in error variance, the variogram then depends on the actual locations being compared, as opposed to just the distance between the locations. For sites \\(x\\) and \\(y\\), the variogram can be expressed as \\[\\gamma\\left(x,y\\right)=\\sigma_{M}^{2}\\left[1-\\rho\\left(\\left|x-y\\right|\\right)\\right]+\\frac{1}{2}\\left[\\sigma_{E}^{2}\\left(x\\right)+\\sigma_{E}^{2}\\left(y\\right)\\right].\\] Compared to Equation , the term on the right is no longer constant. This means that each point in the empirical variogram (as described at the end of Section ) would have a different nugget and sill; in other words, they do not come from a common variogram. As such, direct regression cannot be used to estimate the parameters. However, if we knew the values of \\(\\sigma_{E}^{2}\\left(x\\right)\\) at the sample sites, then you could follow these steps to estimate \\(\\sigma_{M}^{2}\\) and \\(\\rho\\left(d\\right)\\): For all (different) pairs of sites \\(i\\) and \\(j\\), create a modified empirical variogram with dependent variable \\(0.5\\left(m_{i}-m_{j}\\right)^{2}-0.5\\left[\\sigma_{E}^{2}\\left(x_{i}\\right)-\\sigma_{E}^{2}\\left(x_{j}\\right)\\right]\\) and independent variable \\(\\left|x_{i}-x_{j}\\right|\\) ; Choose a form for \\(\\rho\\left(d\\right)\\) this could be exponential, gaussian, etc. (see Section 2.5 in Reference @Geostatistics for a discussion of these models); Fit a nonlinear regression model using the mean structure \\(\\gamma\\left(d\\right)=\\sigma_{M}^{2}\\left[1-\\rho\\left(d\\right)\\right]\\) (see Reference @Nonlinear [@Regression] for details on nonlinear regression); Nonlinear regression will provide estimates for \\(\\sigma_{M}^{2}\\) and \\(\\rho\\left(d\\right)\\). As before, an analysis of residuals would be useful here to assess the goodness-of-fit of the regression. It only remains to estimate \\(\\sigma_{E}^{2}\\left(x\\right)\\) at the sample sites. The simplest case is to use the sample variance of the individual rock sizes at a site, then dividing by the total number of rocks counted in that sample to get the estimate. One might believe that the variances of rock size at two sites with equal means are approximately equal. If this belief is close to the truth, then there is a more efficient way to estimate the error variances by “pooling” all of the data. This can be done by simple regression of the sample variances of the individual rock sizes (computed by the excel functions or ) against the sample averages. Then \\(\\sigma_{E}^{2}\\left(x\\right)\\) can be estimated as the value of the regression curve evaluated at \\(x\\), then dividing by the sample size at that site. 25.7 Statistical Objective 1: Downstream Fining Curve regression of \\(m_{1},\\ldots,m_{54}\\) against \\(x_{1},\\ldots,x_{54}\\). there are two other things you might want to change in the regression. 25.7.1 Regression Form there might be a theoretical form that more accurately describes the curve, such as an exponential model: \\[T\\left(x\\right)=ae^{-bx}.\\] If such a theoretical model exists, you may need to consider nonlinear regression, as discussed in Reference @Nonlinear [@Regression]. If there is no theoretical form, then using a straight line here is probably a reasonable choice as a rough approximation. 25.8 Statistical Objective 2: River Profile The goal of this section is to estimate the mean rock size \\(M\\left(x\\right)\\) at a location \\(x\\) along the study site of the river. Doing this for all locations in the study site, you can obtain a “river profile” of mean rock size. Kriging is a method of estimating mean rock size \\(M\\left(x\\right)\\) that takes the variances and correlation of the data into consideration. In the case when there is no error variance, Kriging will smoothly “connect the dots” of your data \\(m_{1},\\ldots,m_{54}\\). There are many types of Kriging, but there are two that are directly relevant for this project Simple Kriging and Universal Kriging. These are discussed in Sections and when approximating the error variance \\(\\sigma_{E}^{2}\\left(x\\right)\\) by a constant, and in Section when allowing the error variance to be non-constant. For a summary of many types of Kriging, see Reference @arcGIS_Kriging. For an extensive overview of Kriging, see Chapter 3 in Reference @Geostatistics. The idea behind ** Kriging is to estimate mean rock size \\(M\\left(x\\right)\\) by using a “weighted average” of \\(m_{1},\\ldots,m_{54}\\). That is, instead of using the regular average \\(\\frac{1}{54}m_{1}+\\cdots+\\frac{1}{54}m_{54}\\), Kriging uses \\(w_{1}\\left(x\\right)m_{1}+\\cdots+w_{54}\\left(x\\right)m_{54}\\) to estimate \\(M\\left(x\\right)\\), where the weights \\(w_{1}\\left(x\\right),\\ldots,w_{54}\\left(x\\right)\\) are nonnegative numbers adding to 1 that depend on the location \\(x\\). The weights are chosen to minimize variability in the estimate (technically, the “mean squared error”). 25.8.1 Simple Kriging Simple Kriging is Kriging when the downstream fining curve, \\(T\\left(x\\right)\\), is known. Of course, the downstream fining curve is unknown, but you could get an estimate of it, as discussed in Section . Regarding the implementation of Kriging, if you are not prompted to specify \\(T\\left(x\\right)\\), then whichever software you use will likely assume \\(T\\left(x\\right)=0\\). In this case, you would need to adjust for this by subtracting \\(T\\left(x_{1}\\right),\\ldots,T\\left(x_{54}\\right)\\) from your data \\(m_{1},\\ldots,m_{54}\\) before running the Kriging procedure, then add \\(T\\left(x\\right)\\) to your result. 25.8.2 Universal Kriging Universal Kriging is Kriging when the downstream fining curve, \\(T\\left(x\\right)\\), is a straight line. In this case, there is no need to estimate \\(T\\left(x\\right)\\) separately as in Section the estimation is “built-in” to the procedure. It is best to use Universal Kriging if you choose \\(T\\left(x\\right)\\) to be linear and if you do not estimate \\(T\\left(x\\right)\\) using GLS. 25.8.3 Kriging under Non-Constant Error Variance To implement Kriging, it is likely that most software would use the variogram estimate in place of the estimates of \\(\\sigma_{E}^{2}\\left(x\\right)\\), \\(\\sigma_{M}^{2}\\), and \\(\\rho\\left(d\\right)\\). However, if you want to account for \\(\\sigma_{E}^{2}\\left(x\\right)\\) being different for different locations, then the Kriging procedure would need to use the individual estimates of \\(\\sigma_{E}^{2}\\left(x\\right)\\), \\(\\sigma_{M}^{2}\\), and \\(\\rho\\left(d\\right)\\) instead of the variogram itself (recall Section for estimating these quantities under the non-constant error variance consideration). See Chapter 3 in Reference @Geostatistics to find formulas for the Kriging estimator. 25.9 Confidence Intervals of the River Profile Various software that have Kriging capabilities most likely also have the ability to construct confidence intervals when prompted. A confidence interval at a location \\(x\\) is an interval that covers the true mean rock size, \\(M\\left(x\\right)\\), with approximately some pre-specified chance (such as 95%). When the intervals are plotted over a range of locations, they form a confidence “band” around the river profile. However, be aware that the confidence intervals in the case of Simple Kriging will be “too narrow”, unless the estimate of the downstream fining curve is quite precise. A confidence interval that is “too narrow” has the implication that the amount of confidence you claim (such as 95%) is actually more than the actual confidence level. This is because the Simple Kriging procedure treats the downstream fining curve \\(T\\left(x\\right)\\) as known, and does not incorporate the uncertainty involved in estimating \\(T\\left(x\\right)\\). It is likely that the confidence intervals provided by the software are based on the assumption that the data are normally distributed. Since your data \\(m_{1},\\ldots,m_{54}\\) are averages, and there are many rocks in each site, this assumption of normality is probably reasonable due to the Central Limit Theorem even though the original rock size distributions are not normally distributed. However, this relies on the assumption that the presence of each rock from a sampling site is not influenced from other rocks (i.e. they are independent). It is not clear whether this is true, but the influence from other rocks might be small enough to make independence a reasonable assumption. As usual, if you decide to account for the non-constant nature of the error variance \\(\\sigma_{E}^{2}\\left(x\\right)\\), then you may need to compute the variance of the mean estimate. These formulas are found in Chapter 3 of Reference @Geostatistics. You can compute a 95% confidence interval using the variance of the mean estimate (denoting this variance as \\(V\\left(x\\right)\\) and the estimated mean as \\(\\hat{M}\\left(x\\right)\\)) by \\(\\hat{M}\\left(x\\right)\\pm1.96\\sqrt{V\\left(x\\right)}\\). 1 Chilès, J.P., and Delfiner, P. (2012) Geostatistics: Modeling Spatial Uncertainty. Second Edition. Wiley. http://help.arcgis.com/en/arcgisdesktop/10.0/help/index.html\\#//00310000003q000000 http://www2.sas.com/proceedings/sugi27/p213-27.pdf Huet, S., Bouvier, A., Poursat, M.-A., and Jolivet, E. (2003) Statistical Tools for Nonlinear Regression: A Practical Guide with S-Plus and R Examples. Second Edition. Springer. Anton, H. (2010) Elementary Linear Algebra. Tenth Edition. Wiley. "]
]
