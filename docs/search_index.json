[["index.html", "Interpreting Regression Chapter 1 Preamble 1.1 Caution 1.2 Preamble 1.3 A focus on Interpretation", " Interpreting Regression Vincenzo Coia 2021-10-25 Chapter 1 Preamble 1.1 Caution This book is in its very preliminary stages. Content will be moving around and updated. Currently, much of the book is a stitching together of previous pieces of my writing that I think might be relevant to this book. These chapters will be updated to consider the different context, audience, and content organization that’s best for this book. 1.2 Preamble This book explores statistics from a problem-first perspective, a topic I like to call “statistical data science”. More broadly, data science is concerned with drawing insights from data, often relying on the interpretation of quantities – whether that quantity be a prediction, a regression coefficient, or a mean squared error. The part of the data science process that provides precise interpretation is the topic of this book. The book aims to cover fundamental topics in statistical data science, spanning a variety of regression models that are often seen in practice, and tying them together. Of course, there are many, many more advanced statistical topics that will not be covered in this book, including spatial statistics, state space models, etc. This book is written for data analysts who need to make decisions with data in hand, and for statistics and data science students who want to learn a unifying landscape of statistical methods. The practicality of the book comes by presenting a toolkit for making decisions on key model components, so that a model can be designed rather than simply chosen from a list of possibilities. Because many of the models taught in statistics courses can be arrived at through specific combinations of these decisions, the book provides a framework whereby seemingly different statistical methods can be related back to the bigger picture framework. Texts in the traditional statistical sciences tend to use a lot of math. For these books, this is necessary, because statistical science tends to start with model assumptions, and work towards deriving results – a process that requires mathematical precision. Take, for instance, linear regression. The topic is introduced by writing a linear equation relating a response to covariates, after which attention is turned to estimating the regression coefficients. Confidence intervals and hypothesis tests can then be derived, along with diagnostic checks on the model assumptions. Because statistical data science is not concerned with deriving results, but rather simply using them, mathematics becomes less relevant. As such, this book will not contain much math. Whenever equations are included, they are interpreted using simple language and diagrams. Or, sometimes equations will be included for those interested in seeing precise formulations, but are not needed to understand concepts. Another consequence coming from treating statistics from a problem-first approach is that model assumptions never truly hold in practice. In practice, it’s more realistic to ask whether an assumption is a realistic approximation to the truth. This question is in stark contrast to traditional statistical methods, which try to determine whether an assumption is true for your data. In reality, making an approximation results in a tradeoff, which unfortunately cannot be optimized based on your data. This book aims to provide guidance as to how to make a decision regarding model approximations. Methods are demonstrated using the R Project for Statistical Computing, freely available online. This is because R has an extensive selection of packages available for statistical analysis, amongst which we will focus on the tidyverse and tidymodels meta-packages, and the distplyr package for working with distributions. Even if you have not taken a statistics course before, I hope you find this book accessible, because fundamental concepts like probability and distributions are defined from scratch. Even if you are somewhat familiar with statistics, you may find these fundamental concepts worth revisiting, since you’ll be challenged to re-think why well-known concepts such as the mean and variance are prominent, for example. 1.3 A focus on Interpretation Regression analysis can help solve two main types of problems: Interpreting the relationship between variables. For example, we might want to know how someone’s age influences the click-through rate on an online advertisement, taking into account other factors like their location and the type of ad presented. Or, perhaps we would like to find the factors that are most highly related to fatal car crashes. Or, how does age influence the chance of pregnancy? How does time of insemination after a spike in Luteinizing hormone affect the chance of pregnancy, and how is this different for people over 40? Predicting a new outcome. For example, we might want to predict the click-through rate of an online advertisement, given someone’s age and location, and the type of ad presented. Or, we might want to predict the flow of a river given a certain amount of rainfall and snowmelt. Although both problem types are considered, this book primarily focusses on building models for interpretation. A focus on prediction lies in the realm of machine learning, and a large part of the decision making here involves optimizing a model’s fit to new data – a problem for which there are many resources available. This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. "],["interpreting-a-random-quantity.html", "Interpreting a Random Quantity", " Interpreting a Random Quantity How can we get a handle on an outcome that seems random? Although the score of a Canucks game, a stock price, or river flow is uncertain, their outcomes can still be described. For example, we can discover a range of likely possibilities, or what the most likely outcome is, or even get a sense of a worst-case scenario. This part of the book describes how to do just that, by shedding light on concepts of probability and univariate analysis as they apply to data science. "],["probability-when-an-outcome-is-unknown.html", "Chapter 2 Probability: When an Outcome is Unknown 2.1 Probability Distributions 2.2 Continuous random variables (10 min) 2.3 Density Functions (20 min) 2.4 Summary and take-aways", " Chapter 2 Probability: When an Outcome is Unknown Status: needs some elaborating and some good examples. See Section 1.1. Imagine what life would be like if nothing was certain. If you let go of a ball, sometimes it falls, stays put, or adopts some other motion. If you wrap yourself in a blanket, sometimes it warms you up, and sometimes it cools you down. Or, taking pain medicine sometimes eases your pain, and sometimes makes it worse. Indeed, we rely on known cause-and-effect relationships in order to operate day to day. And whatever cause-and-effect relationships we don’t know, we can learn them to allow us to expand our ability to operate in this world of ours. Don’t know how to ski? Just learn the motions that will result in you descending a snowy mountain with control and a lot of fun. But there are many things that are in fact uncertain. Maybe your income relies on how many clients you get, so that you don’t even know your income next month. Maybe you don’t know whether breast feeding your baby will result in less colic than bottle feeding, because you’ve seen your friends’ colicky babies in both cases. Does uncertainty mean we should resort to claiming ignorance? That you have no idea what your income will be in a month? That it’s impossible to know whether the way your baby is fed impacts colic? These might sound like realistic claims holding a rudimentary cause-and-effect mindset, but the reality is that we often can in fact garner information from uncertain outcomes if we adopt a probabilistic mindset. When the result of an outcome is known, it is referred to as deterministic, whereas an unknown outcome is referred to as stochastic. To understand a stochastic outcome and what influences it, it’s key to understand probability distributions, which are the topic of this chapter. 2.1 Probability Distributions Before abstracting probability distributions to real examples, it’s perhaps best to explore probability distributions using simple examples where probabilities are known. A probability distribution is a complete indication of how much probability is associated with certain outcomes. If we roll a 6-sided die, since each of the six outcomes (the numbers 1 through 6) are equally likely, each outcome has a probability of 1/6. The distribution of the dice roll is the specification of probabilities for each outcome: Outcome Probability 1 1/6 2 1/6 I like to play Mario Kart 8, a racing game with some “combat” involved using items. In the game, you are given an item at random whenever you get an “item box”. Suppose you’re playing the game, and so far have gotten the following items in total: Item Name Count Banana 7 Bob-omb 3 Coin 37 Horn 1 Shell 2 Total: 50 Attribution: images from pngkey. 2.1.1 Probability What’s the probability that your next item is a coin? How would you find the actual probability? From this, how might you define probability? In general, the probability of an event \\(A\\) occurring is denoted \\(P(A)\\) and is defined as \\[\\frac{\\text{Number of times event } A \\text{ is observed}}{\\text{Total number of events observed}}\\] as the number of events goes to infinity. 2.1.2 Probability Distributions So far, we’ve been discussing probabilities of single events. But it’s often useful to characterize the full “spectrum” of uncertainty associated with an outcome. The set of all outcomes and their corresponding probabilities is called a probability distribution (or, often, just distribution). The outcome itself, which is uncertain, is called a random variable. (Note: technically, this definition only holds if the outcome is numeric, not categorical like our Mario Kart example, but we won’t concern ourselves with such details) When the outcomes are discrete, the distributions are called probability mass functions (or pmf’s for short). 2.1.3 Examples of Probability Distributions Mario Kart Example: The distribution of items is given by the above table. Ship example: Suppose a ship that arrives at the port of Vancouver will stay at port according to the following distribution: Length of stay (days) Probability 1 0.25 2 0.50 3 0.15 4 0.10 The fact that the outcome is numeric means that there are more ways we can talk about things, as we will see. 2.2 Continuous random variables (10 min) What is the current water level of the Bow River at Banff, Alberta? How tall is a tree? What about the current atmospheric temperature in Vancouver, BC? These are examples of continuous random variables, because there are an uncountably infinite amount of outcomes. Discrete random variables, on the other hand, are countable, even if there are infinitely many outcomes, because each outcome can be accounted for one-at-a-time by some pattern. Example: The positive integers are discrete/countable: just start with 1 and keep adding 1 to get 1, 2, 3, etc., and that covers all possible outcomes. Positive real numbers are not countable because there’s no way to cover all possibilities by considering one outcome at a time. It turns out that it’s trickier to interpret probabilities for continuous random variables, but it also turns out that they’re in general easier to work with. Not all random variables with infinitely many outcomes are continuous. Take, for example, a Poisson random variable, that can take values \\(0, 1, 2, \\ldots\\) with no upper limit. The difference here is that a smaller range of values does have a finite amount of variables. By the way, this type of infinity is called “countably infinite”, and a continuous random variable has “uncountably infinite” amount of outcomes. 2.2.0.1 Advanced and Optional: Is anything actually continuous? In practice, we can never measure anything on a continuous scale, since any measuring instrument must always round to some precision. For example, your kitchen scale might only measure things to the nearest gram. But, these variables are well approximated by a continuous variable. As a rule of thumb, if the difference between neighbouring values isn’t a big deal, consider the variable continuous. Example: You’d like to get a handle on your monthly finances, so you record your total monthly expenses each month. You end up with 20 months worth of data: ## [1] &quot;$1903.68&quot; &quot;$3269.61&quot; &quot;$6594.05&quot; &quot;$1693.94&quot; &quot;$2863.71&quot; &quot;$3185.01&quot; ## [7] &quot;$4247.04&quot; &quot;$2644.27&quot; &quot;$8040.42&quot; &quot;$2781.11&quot; &quot;$3673.23&quot; &quot;$4870.13&quot; ## [13] &quot;$2449.53&quot; &quot;$1772.53&quot; &quot;$7267.11&quot; &quot;$938.67&quot; &quot;$4625.33&quot; &quot;$3034.81&quot; ## [19] &quot;$4946.4&quot; &quot;$3700.16&quot; Since a difference of $0.01 isn’t a big deal, we may as well treat this as a continuous random variable. Example: Back in the day when Canada had pennies, you liked to play “penny bingo”, and wrote down your winnings after each day of playing the game with your friends. Here are your net winnings: ## [1] 0.01 -0.01 0.02 0.01 0.04 0.02 -0.03 -0.01 0.05 0.04 Since a difference of $0.01 is a big deal, best to treat this as discrete. 2.3 Density Functions (20 min) In the discrete case, we were able to specify a distribution by indicating a probability for each outcome. Even when there’s an infinite amount of outcomes, such as in the case of a Poisson distribution, we can still place a non-zero probability on each outcome and have the probabilities sum to 1 (thanks to convergent series). But an uncountable amount of outcomes cannot be all accounted for by a sum (i.e., the type of sum we denote by \\(\\sum\\)), and this means that continuous outcomes must have probability 0. Example: The probability that the temperature in Vancouver tomorrow will be 18 degrees celcius is 0. In fact, any temperature has a probability of 0 of occurring. While individual outcomes have zero probability, ranges can have non-zero probability. We can use this idea to figure out how “dense” the probability is at some areas of the outcome space. For example, if a randomly selected tree has a 0.05 probability of being within 0.1m of 5.0m, then as a rate, that’s about 0.05/(0.1m) = 0.5 “probability per meter” here. Taking the limit as the range width \\(\\rightarrow 0\\), we obtain what’s called the density at 5m. The density as a function over the outcome space is called the probability density function (pdf), usually abbreviated to just the density, and denoted \\(f\\). Sometimes we specify the random variable in the subscript, just to be clear about what random variable this density represents – for example, \\(f_X\\) is the density of random variable \\(X\\). You’ll see that the density is like a “continuous cousin” of the probability mass function (pmf) in the case of discrete random variables. We’ll also see in a future lecture that there are some random variables for which neither a density nor a pmf exist. We can use the density to calculate probabilies of a range by integrating the density over that range: \\[P(a &lt; X &lt; b) = \\int_a^b f(x) \\text{d}x.\\] This means that, integrating over the entire range of possibilities should give us 1: \\[\\int_{-\\infty}^\\infty f(x) \\text{d}x = 1\\] This integral corresponds to the entire area under the density function. 2.3.1 Example: “Low Purity Octane” You just ran out of gas, but luckily, right in front of a gas station! Or maybe not so lucky, since the gas station is called “Low Purity Octane”. They tell you that the octane purity of their gasoline is random, and has the following density: What’s the probability of getting 25% purity? That is, \\(P(\\text{Purity} = 0.25)\\)? The density evaluates to be &gt;1 in some places. Does this mean that this is not a valid density? Why is the density in fact valid? Is it possible for the density to be negative? Why or why not? What’s the probability of getting gas that’s \\(&lt;50\\%\\) pure? That is, \\(P(\\text{Purity} &lt; 0.5)\\)? What’s the probability of getting gas that’s \\(\\leq 50\\%\\) pure? That is, \\(P(\\text{Purity} \\leq 0.5)\\)? What’s the support of this random variable? That is, the set of all outcomes that have non-zero density? You decide to spend the day at Low Purity Octane, measuring the octane purity for each customer. You end up getting 100 observations, placing each measurement along the x-axis. Which of the following plots would be more likely, and why? 2.3.2 Example: Monthly Expenses It turns out your monthly expenses have the following density, with your 20 observations plotted below it: 2.4 Summary and take-aways These distributions might not seem practical, but they certainly are. Even in cases where we are considering many other pieces of information aside from the response, we are still dealing with univariate distributions – we’ll see later that the only difference is that they are no longer marginal distributions (they are conditional). It’s important to consider whether you are interested in the result of a single observation, or whether you are more interested in outcomes observed in the long run. While Statistics uses repeated observations in the long run to identify patterns and distributions, how you use those distributions will depend on your interest. "],["distribution-properties-quantities-we-can-interpret.html", "Chapter 3 Distribution Properties: Quantities we can Interpret 3.1 Probabilistic Quantities 3.2 Measures of central tendency and uncertainty 3.3 What is the mean, anyway? 3.4 Quantiles 3.5 Continuous Distribution Properties 3.6 Heavy-Tailed Distributions", " Chapter 3 Distribution Properties: Quantities we can Interpret Caution: in a highly developmental stage! See Section 1.1. Status: Topics are mostly all here, but needs framing. Also needs some consolidation. Concepts: Probabilistic quantities and their interpretation Prediction as choosing a probabilistic quantity to put forth. Irreducible error 3.1 Probabilistic Quantities Sometimes confusingly called “parameters”. Explain the quantities by their interpretation/usefulness, using examples. Mean: what number would you want if you have 10 weeks worth of data, and you want to estimate the total after 52 weeks (one year)? Mode Quantiles: Measures of discrepency/“distance” (for prediction): difference ratio Measures of spread: Variance IQR Coefficient of Variance (point to its usefulness on a positive ratio scale) Information measures When you want information about an unknown quantity, it’s up to you what you decide to use. The mean is the most commonly sought when the unknown is numeric. I suspect this is the case for two main reasons: It simplifies computations. It’s what’s taught in school. 3.2 Measures of central tendency and uncertainty There are two concepts when communicating an uncertain outcome: Central tendency: a “typical” value of the outcome. Uncertainty: how “random” the outcome is. There are many ways to measure these two concepts. They’re defined using a probability distribution, but just as probability can be defined as the limit of a fraction based on a sample, these measures often have a sample version (aka empirical version) from which they are derived. As such, let’s call \\(X\\) the random outcome, and \\(X_1, \\ldots, X_n\\) a set of \\(n\\) observations that form a sample (see the terminology page for alternative uses of the word sample). 3.2.1 Mode and Entropy No matter what scale a distribution has, we can always calculate the mode and entropy. And, when the outcome is categorical (like the Mario Kart example), we are pretty much stuck with these as our choices. The mode of a distribution is the outcome having highest probability. A measure of central tendency. The sample version is the observation you saw the most. Measured as an outcome, not as the probabilities. The entropy of a distribution is defined as \\[-\\displaystyle \\sum_x P(X=x)\\log(P(X=x)).\\] A measure of uncertainty. Probably the only measure that didn’t originate from a sample version (comes from information theory). Measured as a transformation of probabilities, not as the outcomes – so, hard to interpret on its own. Cannot be negative; zero-entropy means no randomness. 3.2.2 Mean and Variance When our outcome is numeric, we can take advantage of the numeric property and calculate the mean and variance: The mean (aka expected value, or expectation) is defined as \\[\\displaystyle \\sum_x x\\cdot P(X=x).\\] A measure of central tendency, denoted \\(E(X)\\). Its sample version is \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i,\\) which gets closer and closer to the true mean as \\(n \\rightarrow \\infty\\) (this is in fact how the mean is originally defined!) Useful if you’re wanting to compare totals of a bunch of observations (just multiply the mean by the number of observations to get a sense of the total). Probably the most popular measure of central tendency. Note that the mean might not be a possible outcome! The variance is defined as \\[E[(X-E(X))^2],\\] or this works out to be equivalent to the (sometimes) more useful form, \\[E[X^2]-E[X]^2.\\] A measure of uncertainty, denoted \\(\\text{Var}(X)\\). Yes! This is an expectation – of the squared deviation from the mean. Its sample version is \\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar{X})^2\\), or sometimes \\(s^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar{X})^2\\) – both get closer and closer to the true variance as \\(n \\rightarrow \\infty\\) (you’ll be able to compare the goodness of these at estimating the true variance in DSCI 552 next block). Like entropy, cannot be negative, and a zero variance means no randomness. Unlike entropy, depends on the actual values of the random variable. The standard deviation is the square root of the variance. Useful because it’s measured on the same scale as the outcome, as opposed to variance, which takes on squared outcome measurements. Note: you may have heard of the median – we’ll hold off on this until later. 3.3 What is the mean, anyway? Imagine trying to predict your total expenses for the next two years. You have monthly expenses listed for the past 12 months. What’s one simple way of making your prediction? Calculate the average expense from the past 12 months, and multiply that by 24. In general, a mean (or expected value) can be interpreted as the long-run average. However, the mean tends to be interpreted as a measure of central tendency, which has a more nebulous interpretation as a “typical” outcome, or an outcome for which most of the data will be “nearest”. 3.4 Quantiles It’s common to “default” to using the mean to make decisions. But, the mean is not always appropriate (I wrote a blog post about this): Sometimes it makes sense to relate the outcome to a coin toss. For example, find an amount for which next month’s expenditures will either exceed or be under with a 50% chance. Sometimes a conservative/liberal estimate is wanted. For example, a bus company wants conservative estimates so that most busses fall within the estimated travel time. In these cases, we care about quantiles, not the mean. Estimating them is called quantile regression (as opposed to mean regression). Recall what quantiles are: the \\(\\tau\\)-quantile (for \\(\\tau\\) between 0 and 1) is the number that will be exceeded by the outcome with a \\((1-\\tau)\\) chance. In other words, there is a probability of \\(\\tau\\) that the outcome will be below the \\(\\tau\\)-quantile. \\(\\tau\\) is referred to as the quantile level, or sometimes the quantile index. For example, a bus company might want to predict the 0.8-quantile of transit time – 80% of busses will get to their destination within that time. 3.5 Continuous Distribution Properties With continuous random variables, it becomes easier to expand our “toolkit” of the way we describe a distribution / random variable. As before, each property always has a distribution-based definition that gives us an exact/true value, and sometimes has an empirically-based (data-based) definition that gives us an approximate value, but that approaches the true value as more and more observations are collected. 3.5.1 Mean, Variance, Mode, and Entropy (5 min) These are the properties of a distribution that we’ve already seen, but they do indeed extend to the continuous case. Mode and entropy can be defined, but since these ignore the numeric property of continuous random variables, they tend to not be used. Also, these properties don’t really have a natural empirical version. Mode: The outcome having the highest density. That is, \\[\\text{Mode} = {\\arg \\max}_x f(x).\\] Entropy: The entropy can be defined by replacing the sum in the finite case with an integral: \\[\\text{Entropy} = \\int_x f(x) \\log f(x) \\text{d}x.\\] Instead, we prefer to describe a continuous random variable using properties that inform us about distances. The mean and variance are two such measures of central tendency and uncertainty, where the only difference with a continuous random variable is in the distribution-based definition, where the sum becomes an integral. Mean: The distribution-based definition is \\[E(X) = \\int_x x \\, f(x) \\text{d}x.\\] You may later learn that this is a point that is “as close as possible” to a randomly generated outcome, in the sense that its expected squared distance is as small as possible. Ends up being the “center of mass” of a probability density function, meaning that you could “balance” the density function on this single point without it “toppling over due to gravity”. Probably best interpreted as the long-run sample average (empirical mean). Variance: The distribution-based definition is \\[\\text{Var}(X) = E \\left( (X - \\mu_X)^2 \\right) = \\int_x (x - \\mu_X) ^ 2 \\, f(x) \\text{d}x,\\] where \\(\\mu_X = E(X)\\). While the mean minimizes the expected squared distance to a randomly generated outcome, this is the expected squared distance. Going back to the octane purity example from Low Purity Octane gas station: The mode is 1 (the highest purity possible!). The entropy works out to be \\[\\int_0^1 2x \\log(2x) \\text{d}x \\doteq 0.1931.\\] The mean ends up being not a very good purity (especially as compared to the mode!), and is \\[\\int_0^1 2x^2 \\text{d}x = \\frac{2}{3}.\\] The variance ends up being \\[\\int_0^1 2 x \\left(x - \\frac{2}{3}\\right)^2 \\text{d}x = \\frac{1}{18}.\\] 3.5.2 Median (5 min) The median is the outcome for which there’s a 50-50 chance of seeing a greater or lesser value. So, its distribution-based definition satisfies \\[P(X \\leq \\text{Median}(X)) = 0.5.\\] Its empirically-based definition is the “middle value” after sorting the outcomes from left-to-right. Similar to the mean, you may later learn that the median is a point that is “as close as possible” to a randomly generated outcome, in the sense that its expected absolute distance is as small as possible. The median is perhaps best for making a single decision about a random outcome. Making a decision is simplest when the possibilities are reduced down to two equally likely outcomes, and this is exactly what the median does. For example, if the median time it takes to complete a hike is 2 hours, then you know that there’s a 50-50 chance that the hike will take over 2 hours. If you’re instead told that the mean is 2 hours, this only tells us that the total amount of hiking time done by a bunch of people will be as if everyone takes 2 hours to do the hike – this is still useful for making a decision about whether or not you should do the hike, but is more convoluted. Using the purity example at Low Purity Octane, the median is about 0.7071: 3.5.3 Quantiles (5 min) More general than a median is a quantile. The definition of a \\(p\\)-quantile \\(Q(p)\\) is the outcome that has a \\(1-p\\) probability of exceedance, or equivalently, for which there’s a probability \\(p\\) of getting a smaller outcome. So, its distribution-based definition satisfies \\[P(X \\leq Q(p)) = p.\\] The median is a special case, and is the 0.5-quantile. An empirically-based definition of the \\(p\\)-quantile is the \\(np\\)’th largest (rounded up) observation in a sample of size \\(n\\). Some quantiles have a special name: The 0.25-, 0.5-, and 0.75-quantiles are called quartiles. Sometimes named the first, second, and third quartiles, respectively. The 0.01-, 0.02, …, and 0.99-quantiles are called percentiles. Sometimes the \\(p\\)-quantile will be called the \\(100p\\)’th percentile; for example, the 40th percentile is the 0.4-quantile. Less commonly, there are even deciles, as the 0.1, 0.2, …, and 0.9-quantiles. For example, the 0.25-quantile of octane purity at Low Purity Octane is 0.5, since the area to the left of 0.5 is 0.25: 3.5.4 Prediction Intervals (5 min) It’s often useful to communicate an interval for which a random outcome will fall in with a pre-specified probability \\(p\\). Such an interval is called a \\(p \\times 100\\%\\) Prediction Interval. Usually, we set this up in such a way that there’s a \\(p/2\\) chance of exceeding the interval, and \\(p/2\\) chance of undershooting the interval. You can calculate the lower limit of this interval as the \\((1 - p)/2\\)-Quantile, and the upper limit as the \\(1 - (1 - p)/2\\)-Quantile. Example: a 90% prediction interval for the purity of gasoline at “Low Purity Octane” is [0.2236, 0.9746], composed of the 0.05- and 0.95-quantiles. 3.5.5 Skewness (5 min) Skewness measures how “lopsided” a distribution is, as well as the direction of the skew. If the density is symmetric about a point, then the skewness is 0. If the density is more “spread-out” towards the right / positive values, then the distribution is said to be right-skewed (positive skewness). If the density is more “spread-out” towards the left / negative values, then the distribution is said to be left-skewed (negative skewness). It turns out that for symmetric distributions, the mean and median are equivalent. But otherwise, the mean tends to be further into the skewed part of the distribution. Using the monthly expense example, the mean monthly expense is $3377.87, which is bigger than the median monthly expense of $2980.96. Formally, skewness can be defined as \\[\\text{Skewness} = E \\left( \\left( \\frac{X - \\mu_X}{\\sigma_X} \\right) ^ 3 \\right),\\] where \\(\\mu_X = E(X)\\) and \\(\\sigma_X = \\text{SD}(X)\\). For example, the octane purity distribution is left-skewed, and has a skewness of \\[\\int_0^1 2 x \\left(\\sqrt{18} (x - 2/3) \\right) ^ 3 \\text{d}x \\doteq -0.5657.\\] 3.5.6 Examples For the following situations, which quantity is most appropriate, and why? You want to know your monthly expenses in the long run (say, for forecasting net gains after many months). How do you communicate total expense? You want to ensure you put enough money aside on a given month to ensure you’ll have enough money to pay your bills. How much should you put aside? How should you communicate the cost of a typical house in North Vancouver? 3.6 Heavy-Tailed Distributions Consider the weekly returns of the Singapore Straights (STI) market, depicted by the following histogram. You’ll notice some extreme values that are far from the “bulk” of the data. Traditional practice was to view these extremes as “outliers” that are a nuisance for analysis, and therefore should be removed. But this can actually be detrimental to the analysis, because these outliers are real occurences that should be anticipated. Instead, Extreme Value Analysis is a practice that tries to get a sense of how big and how frequently extremes will happen. 3.6.1 Sensitivity of the mean to extremes Indeed, the empirical (arithmetic) mean is sensitive to outliers: consider the sample average of 100 observations coming from a N(0,1) distribution: set.seed(6) n &lt;- 50 x &lt;- rnorm(n) mean(x) ## [1] 0.08668773 Here’s that mean depicted on a histogram of the data: Now consider calculating the mean by replacing the last observation with 50 (a very large number): x[n] &lt;- 50 mean(x) ## [1] 1.082927 This is a big difference made by a single observation! Let’s take a look at the histogram now (outlier not shown). The “old” mean is the thin vertical line: There are robust and/or resistant ways of estimating the mean that are less sensitive to the outliers. But what’s more interesting when you have extreme values in your data is to get a sense of how frequently extremes will happen, and the mean won’t give you that sense. 3.6.2 Heavy-tailed Distributions Distributions known as heavy-tailed distributions give rise to extreme values. These are distributions whose tail(s) decay like a power decay. The slower the decay, the heavier the tail is, and the more prone extreme values are. For example, consider the member of the Pareto Type I family of distributions with survival function \\(S(x) = 1/x\\) for \\(x \\geq 1\\). Here is this distribution compared to an Exponential(1) distribution (shifted to start at \\(x=1\\)): Notice that the Exponential survival function becomes essentially zero very quickly, whereas there’s still lots of probability well into the tail of the Pareto distribution. Also note that if a distribution’s tail is “too heavy”, then its mean will not exist! For example, the above Pareto distribution has no mean. 3.6.3 Heavy-tailed distribution families Here are some main families that include heavy-tailed distributions: Family of Generalized Pareto distributions Family of Generalized Extreme Value distributions Family of Student’s t distributions The Cauchy distribution is a special case of this. 3.6.4 Extreme Value Analysis There are two key approaches in Extreme Value Analysis: Model the tail of a distribution using a theoretical model. That is, choose some x value, and model the distribution beyond that point. It turns out a Generalized Pareto distribution is theoretically justified. The peaks over thresholds method models the extreme observations occurring in a defined window of time. For example, the largest river flows each year. It turns out a Generalized Extreme Value distribution is theoretically justified here. 3.6.5 Multivariate Student’s t distributions Just like there’s a multivariate Gaussian distribution, there’s also a multivariate Student’s t distribution. And in fact, its contours are elliptical, too! Here’s a comparison of a bivariate Gaussian and a bivariate Student’s t distribution, both of which are elliptical. One major difference is that a sample from a bivariate Gaussian distribution tends to be tightly packed, whereas data from a bivariate Student’s t distribution is prone to data deviating far from the main “data cloud”. And here are samples coming from these two distributions. Notice how tightly bundled the Gaussian distribution is compared to the t distribution! "],["explaining-an-uncertain-outcome-interpretable-quantities.html", "Chapter 4 Explaining an uncertain outcome: interpretable quantities", " Chapter 4 Explaining an uncertain outcome: interpretable quantities Caution: in a highly developmental stage! See Section 1.1. So far, we’ve been saying that a pmf or a pdf is a distribution. Actually, there are more ways we can depict a distribution aside from the pmf/pdf. This section takes a deeper dive into alternative ways a probability distribution can be depicted, and their usefulness. Keep in mind that all of these depictions capture everything about a distribution, which means that if one of them is given, then the other ones can be derived. A note on depictions of multivariate distributions: There is such thing as a multivariate cdf. It comes in handy in copula theory, which is an optional question in a lab assignment. But otherwise, it’s not as useful as a multivariate density, so we won’t cover it. And, there’s no such thing as a multivariate quantile function. 4.0.1 Cumulative Density Functions (cdf’s) / Distribution Functions The cdf is usually denoted by \\(F\\), and is defined as \\[F(x) = P(X \\leq x).\\] We can calculate this using a density \\(f\\) by \\[F(x) = \\int_{-\\infty}^x f(t) \\, \\text{d}t.\\] Unlike the pdf/pmf, the cdf always exists for any random variable. It just doesn’t exist for categorical variables, because there’s no such thing as “less than” or “greater than”. For discrete random variables, the cdf is still a continuous function, but has a jump-discontinuity at the discrete values. Here are the cdf’s of the octane purity, monthly expenses, and length of stay (from last time): For the discrete cdf, a hollow point is a limiting point – the cdf does not evaluate to that point. Note that usually jump discontinuities in a cdf are connected with a straight vertical line, which we will do from now on after this plot. In order for a function \\(F\\) to be a valid cdf, the function needs to satisfy the following requirements: Must never decrease. It must never evalute to be &lt;0 or &gt;1. \\(F(x) \\rightarrow 0\\) as \\(x \\rightarrow -\\infty\\) \\(F(x) \\rightarrow 1\\) as \\(x \\rightarrow \\infty\\). The empirical cdf (ecdf) for a sample of size \\(n\\) treats the sample as if they are discrete values, each with probability \\(1/n\\). Like the cdf of a discrete random variable, the ecdf is also a “step function”. Here is the empirical cdf for the sample of 20 monthly expenses: 4.0.1.1 Exercise (10 min) On the board, let’s calculate the cdf’s of the following two distributions (that you’ve seen in lab): \\[X \\sim \\text{Discrete Uniform}(0, 4)\\] \\[Y \\sim \\text{Continuous Uniform}(0, 4)\\] 4.0.1.2 Evaluating Properties using the cdf (5 min) It turns out that the mean can be calculated in a fairly simple way from the cdf. It’s the area above the cdf and to the right of \\(x = 0\\), minus the area below the cdf and to the left of \\(x = 0\\). In-class exercise: the cdf of octane purity is \\[ F_{\\text{Purity}}(x) = \\begin{cases} 0, \\: x &lt; 0\\\\ x^2, \\: 0 \\leq x \\leq 1, \\\\ 1, \\: x &gt; 1. \\end{cases} \\] What is \\(P(0.5 &lt; \\text{Octane} &lt; 0.75)\\)? What is \\(P(0.5 &lt; \\text{Octane} \\leq 0.75)\\)? What is \\(P(\\text{Octane} &gt; 0.75)\\)? What is the median? 0.25-quantile? True or False: knowing the density of a distribution means that we also know the cdf; but knowing the cdf does not imply knowing the density. 4.0.2 Survival Function (2 min) The survival function \\(S\\) is just the cdf “flipped upside down”. For random variable \\(X\\), the survival function is defined as \\[S(x) = P(X &gt; x) = 1 - F(x).\\] The name comes from Survival Analysis (covered in DSCI 562), where \\(X\\) is interpreted as a “time of death”, so that the survival function is the probability of surviving beyond \\(x\\). Aside from Survival Analysis, the survival function is also useful for Extreme Value Theory. Here are the survival functions of our three examples: 4.0.3 Quantile Function (5 min) The quantile function \\(Q\\) takes a probability \\(p\\) and maps it to the \\(p\\)-quantile. It turns out that this is the inverse of the cdf! \\[Q(p) = F^{-1}(p)\\] Note that this function does not exist outside of \\(0 \\leq p \\leq 1\\)! This is unlike the other functions (density, cdf, and survival function), which exist on all real numbers. Here are the quantile functions of the examples we are working with: 4.0.4 Other ways of depicting a distribution (Optional) (1 min) There are even more ways to depict a distribution that we won’t be going into, that you might have heard of. Denote \\(X\\) as a random variable. Some are: Moment generating function (useful in mathematical statistics): \\[M(t) = E(e^{Xt})\\] Characteristic function (useful in mathematical statistics): \\[\\chi(t) = E(e^{Xti}),\\] where \\(i^2=1\\). Hazard function (useful in survival analysis; wait for DSCI 562): \\[h(t) = \\frac{f(t)}{S(t)}\\] "],["simulation-when-calculations-are-difficult.html", "Chapter 5 Simulation: When calculations are difficult", " Chapter 5 Simulation: When calculations are difficult Caution: in a highly developmental stage! See Section 1.1. Status: Needs framing from the context of this book if so (as opposed to DSCI 551, its original context). Should prepare for bootstrap. So far, we’ve seen many quantities that help us communicating an uncertain outcome: probability probability mass function odds mode entropy mean variance / standard deviation Sometimes, it’s not easy to compute these things. In these situations, we can use simulation to approximate these and other quantities. This is today’s topic. Let’s set up the workspace for this lecture: suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(testthat)) 5.0.1 Learning Objectives From this lecture, students are expected to be able to: Generate a random sample from a discrete distribution in R Reproduce the same random sample each time you re-run your code in R by setting the seed. Evaluate whether or not a set of observations are iid. Use simulation to approximate distribution properties (like mean and variance) using empirical quantities, especially for random variables involving multiple other random variables. 5.0.2 Review Activity (15 min) True or False? In general, 9 parameters must be specified in order to fully describe a distribution with 9 outcomes. A Binomial distribution only has one mean, but there are many Binomial distributions that have the same mean. A Poisson distribution only has one mean, but there are many Poisson distributions that have the same mean. A Binomial distribution is also a Bernoulli distribution, but a Bernoulli distribution is not a Binomial distribution. 5.0.3 Random Samples: Terminology (5 min) A random sample is a collection of random outcomes/variables. Using symbols, a random sample of size \\(n\\) is usually depicted as \\(X_1, \\ldots, X_n\\). We think of data as being a random sample. Some examples of random samples: the first five items you get in a game of Mario Kart the outcomes of ten dice rolls the daily high temperature in Vancouver for each day in a year. A random sample is said to be independent and identically distributed (or iid) if each pair of observations are independent, and each observation comes from the same distribution. We’ll define “independent” next class, but for now, you can think of this as meaning “not influencing each other”. Sometimes, when an outcome is said to be random, this can either mean the outcome has some distribution (with non-zero entropy), or that is has the distribution with maximum entropy. To avoid confusion, the word stochastic refers to the former (as having some uncertain outcome). For example, if a die is weighted so that “1” appears very often, would you call this die “random”? Whether or not you do, it’s always stochastic. The opposite of stochastic is deterministic: an outcome that will be known with 100% certainty. 5.0.4 Seeds (5 min) Computers can’t actually generate truly random outcomes. Instead, they use something called pseudorandom numbers. As an example of a basic algorithm that produces pseudo-random numbers between 0 and 1, consider starting with your choice of number \\(x_0\\) between 0 and 1, and iterating the following equation: \\[x_{i+1} = 4 x_i (1 - x_i).\\] The result will appear to be random numbers between 0 and 1. Here is the resulting sequence when we start with \\(x_0 = 0.3\\) and iterate 1000 times: Although this sequence is deterministic, it behaves like a random sample. But not entirely! All pseudorandom number generators have some pitfalls. In the case above, one pitfall is that neighbouring pairs are not independent from each other (by definition of the way the sequence was set up!). There are some sophisticated algorithms that produce outcomes that more closely resemble a random sample, so most of the time, we don’t have to worry about the sample not being truly random. The seed (or random state) in a pseudo-random number generator is some pre-specified initial value that determines the generated sequence. As long as the seed remains the same, the resulting sample will also be the same. In the case above, this is \\(x_0 = 0.3\\). In R, if we don’t explicitly set the seed, then the seed will be chosen for us. In R, we can set the seed using the set.seed() function. The seed gives us an added advantage over truly random numbers: it allows our analysis to be reproducible! If we explicitly set a seed, then someone who re-runs the analysis will get the same results. 5.0.5 Generating Random Samples: Code Here, we’ll look at some R functions that help us generate a random sample. We’re still focussing on discrete distributions, here. 5.0.5.1 From Finite Number of Categories (5 min) In R, we can generate a random sample from a distribution with a finite number of outcomes using the sample() function: Put the outcomes as a vector in the first argument, x. Put the desired sample size in the argument size. Put replace = TRUE so that sampling can happen with replacement. Put the probabilities of the outcomes as a vector respective to x in the argument prob. Just a warning: if these probabilities do not add up to 1, R will not throw an error. Instead, R automatically adjusts the probabilities so that they add up to 1. Here’s an example of generating 10 items using the Mario Kart item distribution from Lecture 1. Notice that the seed is set, so that every time these lecture notes are rendered, the same results are obtained. set.seed(1) outcomes &lt;- c(&quot;banana&quot;, &quot;bob-omb&quot;, &quot;coin&quot;, &quot;horn&quot;, &quot;shell&quot;) probs &lt;- c(0.12, 0.05, 0.75, 0.03, 0.05) n &lt;- 10 sample(outcomes, size = n, replace = TRUE, prob = probs) ## [1] &quot;coin&quot; &quot;coin&quot; &quot;coin&quot; &quot;bob-omb&quot; &quot;coin&quot; &quot;bob-omb&quot; &quot;shell&quot; ## [8] &quot;coin&quot; &quot;coin&quot; &quot;coin&quot; 5.0.5.2 From Distribution Families (5 min) In R, we can generate data from a distribution belonging to some parametric family using the rdist() function, where “dist” is replaced with a short-form of the distribution family’s name. We can access the corresponding pmf with ddist(). The following table summarizes the functions related to the distribution famlies we’ve seen so far: Family R function Binomial rbinom() Geometric rgeom() Negative Binomial rnbinom() Poisson rpois() Here’s how to use these functions: Sample size: For R, put this in the argument n, which comes first. Each parameter has its own argument. Sometimes, like in R’s rnbinom(), there are more parameters than needed, giving the option of different parameterizations. Be sure to only specify the exact number of parameters required to isolate a member of the distribution family! Example: Generate 10 observations from a binomial distribution with probability of success 0.6 and 5 trials. Using R: rbinom(10, size = 5, prob = 0.6) ## [1] 4 4 2 3 2 3 2 0 3 2 The Negative Binomial family is an example of a function in R that allows for a different parameterization. Notice that specifying too many or too few parameters results in an error (remember, we need to specify two parameters): rnbinom(10, size = 5) ## Error in rnbinom(10, size = 5): argument &quot;prob&quot; is missing, with no default rnbinom(10, size = 5, prob = 0.6, mu = 4) ## Error in rnbinom(10, size = 5, prob = 0.6, mu = 4): &#39;prob&#39; and &#39;mu&#39; both specified 5.0.6 Running Simulations So far, we’ve seen two ways to calculate quantities that help us communicate uncertainty (like means and probabilities): The distribution-based approach (using the distribution), resulting in true values. The empirical approach (using data), resulting in approximate values that improve as the sample size increases. For example, the true mean of a random variable \\(X\\) can be calculated as \\(E(X) = \\sum_x x P(X = x)\\) using each pair of outcome and outcome’s probability, or can be approximated using the empirical approach from a random sample \\(X_1, \\ldots, X_n\\) by \\(E(X) \\approx (1/n) \\sum_{i=1}^n X_i\\). This means that we can approximate these quantities by generating a sample! An analysis that uses a randomly generated data set is called a simulation. 5.0.6.1 Code for empirical quantities (0 min) For your reference, here are some hints for calculating empirical quantities using R. We’ll be going over these below in the “Basic Simulation” section. mean() calculates the sample average. var() calculates the sample variance (the \\(n-1\\) version, not \\(n\\)), and sd() its square root for the standard deviation. For a single probability, remember that a mean is just an average. Just calculate the mean of a condition. For an entire pmf, use the table() function, or more conveniently, the janitor::tabyl() function. For the mode, either get it manually using the table() or janitor::tabyl() function, or you can use DescTools::Mode(). 5.0.6.2 Basic Simulation (10 min) Consider playing games with probability of success \\(p=0.7\\) until you experience \\(k=5\\) successes, and counting the number of failures. This random variable (say \\(X\\)) has a Negative Binomial distribution. You can find an R script containing the code for the basic simulation in the students’ repo. Let’s demonstrate both a distribution-based and empirical approach to computing the variance and pmf. First, let’s obtain our random sample (of, say, 10000 observations). set.seed(88) k &lt;- 5 p &lt;- 0.7 n &lt;- 10000 rand_sample &lt;- rnbinom(n, size = 5, prob = 0.7) head(rand_sample, 100) ## [1] 1 1 6 2 0 3 3 3 2 1 2 5 1 1 1 3 1 1 1 2 2 1 1 7 1 1 3 5 0 4 0 5 1 1 4 1 1 ## [38] 1 2 6 3 2 5 3 1 2 0 2 2 1 1 4 0 0 5 5 2 7 0 0 1 0 3 1 3 2 0 2 2 0 3 1 0 5 ## [75] 4 0 1 3 2 1 2 1 1 2 2 1 0 1 4 4 2 2 4 1 2 4 3 4 1 1 Mean (1 - p) * k / p # True, distribution-based ## [1] 2.142857 mean(rand_sample) # Approximate, empirical ## [1] 2.1654 Variance (1 - p) * k / p^2 # True, distribution-based ## [1] 3.061224 var(rand_sample) # Approximate, empirical ## [1] 3.060549 Standard deviation sqrt((1 - p) * k / p^2) # True, distribution-based ## [1] 1.749636 sd(rand_sample) # Approximate, empirical ## [1] 1.749442 Probability of seeing 0 mean(rand_sample == 0) # Approximate, empirical ## [1] 0.163 dnbinom(0, size = k, prob = p) # True, distribution-based ## [1] 0.16807 pmf ## Code without using the tidyverse: pmf &lt;- janitor::tabyl(rand_sample) # Empirical pmf$n &lt;- NULL pmf &lt;- setNames(pmf, c(&quot;x&quot;, &quot;empirical&quot;)) pmf$actual &lt;- dnbinom(pmf$x, size = k, prob = p) # True ## Code using the tidyverse: pmf &lt;- janitor::tabyl(rand_sample) %&gt;% select(x = rand_sample, empirical = percent) %&gt;% mutate(actual = dnbinom(x, size = k, prob = p)) pmf %&gt;% mutate(actual = round(actual, 4), # Empirical empirical = round(empirical, 4)) %&gt;% # True DT::datatable(rownames = FALSE) Here’s a plot of the pmf: pmf %&gt;% gather(key = &quot;method&quot;, value = &quot;Probability&quot;, empirical, actual) %&gt;% ggplot(aes(x, Probability)) + facet_wrap(~ method) + geom_col(fill = &quot;maroon&quot;) + theme_bw() Entropy It turns out to be hard to calculate the actual entropy, so we will only compute the empirical: - sum(pmf$empirical * log(pmf$empirical)) ## [1] 1.858891 Mode ## Actual pmf %&gt;% filter(actual == max(actual)) %&gt;% pull(x) ## [1] 1 ## Empirical pmf %&gt;% filter(empirical == max(empirical)) %&gt;% pull(x) ## [1] 1 Distribution-based calculations on empirical pmf What do you think you’ll get if you use the definition of mean, variance, etc. on the empirical distribution? You get the empirical values! Here’s an example with the mean – notice that they are identical. sum(pmf$x * pmf$empirical) ## [1] 2.1654 mean(rand_sample) ## [1] 2.1654 Law of Large Numbers To demonstrate that the a larger sample size improves the approximation of the empirical quantities, let’s see how the sample average changes as we collect more and more data: tibble(i = 1:n, mean = cumsum(rand_sample) / i) %&gt;% ggplot(aes(i, mean)) + geom_hline(yintercept = (1 - p) * k / p, colour = &quot;maroon&quot;) + geom_line() + labs(x = &quot;Sample size&quot;, y = &quot;Empirical mean&quot;) + theme_bw() You can try this for yourself with Chapter 1 -“expectation” in Seeing Theory. 5.0.7 Multi-Step Simulations (10 min) The simulation above was not all that useful, since we could calculate basically anything. Where it gets more interesting is when we want to calculate things for a random variable that transforms and/or combines multiple random variables. The idea is that some random variables will have a distribution that depends on other random variables, but in a way that’s explicit. For example, consider a random variable \\(T\\) that we can obtain as follows. Take \\(X \\sim \\text{Poisson}(5)\\), and then \\(T = \\sum_{i = 1}^{X} D_i\\), where each \\(D_i\\) are iid with some specified distribution. In this case, to generate \\(T\\), you would first need to generate \\(X\\), then generate \\(X\\) values of \\(D_i\\), then sum those up to get \\(T\\). This is the example we’ll see here, but in general, you can have any number of dependencies, each component of which you would have to generate. Consider an example that a Vancouver port faces with “gang demand”. Whenever a ship arrives to the port of Vancouver, they request a certain number of “gangs” (groups of people) to help unload the ship. Let’s suppose the number of gangs requested by a ship has the following distribution: gang &lt;- 1:4 p &lt;- c(0.2, 0.4, 0.3, 0.1) tibble( gangs = gang, p = p ) %&gt;% ggplot(aes(gangs, p)) + geom_col(fill = &quot;maroon&quot;) + labs(x = &quot;Number of Gangs&quot;, y = &quot;Probability&quot;) + theme_bw() The following function sums up simulated gangs requested by a certain number of ships, with the above probability distribution as a default. As an example, check out the simulated gang request from 10 ships: #&#39; Generate gang demand #&#39; #&#39; Simulates the number of gangs requested, if each ship #&#39; requests a random number of gangs. #&#39; #&#39; @param n_ships Number of ships that are making demands #&#39; @param gangs Possible gang demands made by a ship. #&#39; @param prob Probabilities of gang demand corresponding to &quot;gangs&quot; #&#39; #&#39; @return Number representing the total gang demand demand_gangs &lt;- function(n_ships, gangs = gang, prob = p) { if (length(gangs) == 1) { gangs &lt;- c(gangs, gangs) prob &lt;- c(1,1) } requests &lt;- sample( gangs, size = n_ships, replace = TRUE, prob = prob ) sum(requests) } test_that(&quot;demand_gangs output is as expected&quot;, { expect_identical(demand_gangs(0), 0L) expect_gte(demand_gangs(1), min(gang)) expect_lte(demand_gangs(1), max(gang)) expect_gte(demand_gangs(10), 10*min(gang)) expect_lte(demand_gangs(10), 10*max(gang)) expect_identical(length(demand_gangs(10)), 1L) expect_identical(demand_gangs(10, gangs = 2, prob = 1), 20) }) ## Test passed 😀 demand_gangs(10) ## [1] 24 Now suppose that the number of ships that arrive on a given day follows the Poisson distribution with a mean of 5. What’s the distribution of total gang request on a given day? Let’s simulate the process to find out: Generate arrival quantities for many days from the Poisson(5) distribution. For each day, simulate total gang request for the simulated number of ships. You now have your random sample – compute things as you normally would. Let’s try this, obtaining a sample of 10000 days: n_days &lt;- 10000 ## Step 1: generate a bunch of ships arriving each day arrivals &lt;- rpois(n_days, lambda = 5) ## Step 2: Simulate total gang request on each day. total_requests &lt;- purrr::map_int(arrivals, demand_gangs) ## Step 3: Compute things like pmf, mean, variance: tibble(x = total_requests) %&gt;% ggplot(aes(x, y = ..prop..)) + geom_bar() + labs(x = &quot;Total Gang Request&quot;, y = &quot;Probability&quot;) + theme_bw() tibble( mean = mean(total_requests), variance = var(total_requests) ) %&gt;% knitr::kable() mean variance 11.5182 30.79875 5.0.8 Generating Continuous Data Until now, we’ve sidestepped the actual procedure for how a random outcome is actually generated. For the discrete case, we could get by with the “drawing from a hat” analogy. But this won’t get us far in the continuous case, because each outcome has 0 probability of occuring. The idea is to convert a random number between 0 and 1 into an outcome. Going back to the discrete case, using the Mario Kart example, we can break the interval [0, 1] into sub-intervals with widths equal to their probabilities. Visually, this might look like the following: We can make a similar plot for a Poisson(3) random variable (the y-axis is truncated because we can’t plot all infinite outcomes): Indeed, this plot is nothing other than the quantile function! This idea extends to all random variables. If we want to generate an observation of a random variable \\(Y\\) with quantile function \\(Q_Y\\), just follow these two steps: Generate a number \\(U\\) completely at random between 0 and 1. Calculate the observation as \\(Y = Q_Y(U)\\). For continuous random variables only, the opposite of this result also has important implications: if \\(Y\\) is a continuous random variable with cdf \\(F_Y\\), then \\[F_Y(Y) \\sim \\text{Unif}(0,1).\\] This is important for p-values in hypothesis testing (DSCI 552+), transformations, and copulas (optional question on your lab assignment). "],["parametric-families-of-distributions.html", "Chapter 6 Parametric Families of Distributions 6.1 Concepts 6.2 Common Parametric Families 6.3 Relevant R functions (8 min) 6.4 Analyses under a Distributional Assumption", " Chapter 6 Parametric Families of Distributions Caution: in a highly developmental stage! See Section 1.1. Concepts: Common scales: Positive ratio scale, binary, (0,1) Different data generating processes give rise to various parametric families of distributions. We’ll explain a good chunk of them. These are useful in data analysis because they narrow down the things that need to be estimated. Improving estimator quality by parametric distributional assumptions and MLE suppressPackageStartupMessages(library(tidyverse)) 6.1 Concepts Aside from the Binomial family of distributions, there are many other families that come up in practice. Here are some of them. For a more complete list, check out Wikipedia’s list of probability distributions. In practice, it’s rare to encounter situations that are exactly described by a distribution family, but distribution families still act as useful approximations. Details about these distributions are specified abundantly online. My favourite resource is Wikipedia, which organizes a distribution family’s properties in a fairly consistent way – for example here is the page on the Binomial family. We won’t bother transcribing these details here, but instead focus on some highlights. What is meant by “family”? This means that there are more than one of them, and that a specific distribution from the family can be characterized the family’s parameters. This is what is meant by “parametric” – the distribution can be distilled down to a set of parameters. For example, to identify a Gaussian distribution, we need to know the mean and variance. Note that some families are characterized by parameters that do not necessarily have an interpretation (or at least an easy one) – for example, to identify a Beta distribution, we need to know the two shape parameters \\(\\alpha\\) and \\(\\beta\\). Technicality that you can safely skip: It’s not that there are “set” parameters that identify a distribution. For example, although a Beta distribution is identified by the two shape parameters \\(\\alpha\\) and \\(\\beta\\), the distribution can also be uniquely identified by its mean and variance. Or, by two quantiles. In general, we need as many pieces of information as there are parameters. This needs to be done in an identifiable way, however. For example, it’s not enough to identify an Exponential distibution by its skewness, because all Exponential distributions have a skewness of 2! The way in which we identify a distribution from parameters is called the parameterization of the distribution. So far in our discussion of distributions, we’ve been talking about properties of distributions in general. Again, this is important because a huge component of data science is in attempting to describe an uncertain outcome, like the number of ships that arrive to the port of Vancouver on a given day, or the identity of a rock. There are some common processes that give rise to probability distributions having a very specific form, and these distributions are very useful in practice. Let’s use the Binomial family of distributions as an example. 6.1.1 Binomial Distribution Process: Suppose you play a game, and win with probability \\(p\\). Let \\(X\\) be the number of games you win after playing \\(N\\) games. \\(X\\) is said to have a Binomial distribution, written \\(X \\sim \\text{Binomial} (N, p)\\). Example: (Demonstration on the board) Let’s derive the probability of winning exactly two games out of three. That is, \\(P(X=2)\\) when \\(N=3\\). pmf: A binomial distribution is characterized by the following pmf: \\[P(X=x) = {N \\choose x} p^x (1-p)^{N-x}.\\] Remember, \\(N \\choose x\\) is read as “N choose x”. You can think of it as the number of ways you can make a team of \\(x\\) people from a total of \\(N\\) people. You can calculate this in R with choose(N, x), and its formula is \\[{N \\choose x} = \\frac{N!}{x!(N-x)!}.\\] mean: \\(Np\\) variance: \\(Np(1-p)\\) Code: The pmf can be calculated in R with dbinom(); in python, scipy.stats.binom. Here is an example pmf for a Binomial(N = 5, p = 0.2) distribution: tibble(x = 0:5, probability = dbinom(x, size = 5, prob = 0.25)) %&gt;% ggplot(aes(x, probability)) + geom_col(fill = &quot;maroon&quot;) + ggtitle(&quot;Binomial(5, 0.2)&quot;) + theme_bw() 6.1.2 Families vs. distributions Specifying a value for both \\(p\\) and \\(N\\) results in a unique Binomial distribution. For example, the Binomial(N = 5, p = 0.2) distribution is plotted above. It’s therefore helpful to remember that there are in fact many Binomial distributions (actually infinite), one for each choice of \\(p\\) and \\(N\\). We refer to the entire set of probability distributions as the Binomial family of distributions. This means that it doesn’t actually make sense to talk about “the” Binomial distribution! This is important to remember as we add on concepts throughout MDS, such as the maximum likelihood estimator that you’ll see in a future course. 6.1.3 Parameters True or false: For a distribution with possible values {0, 1, 2, 3, 4, 5}, five probabilities need to be specified in order to fully describe the distribution. For a Binomial distribution with \\(N=5\\), five probabilities need to be specified in order to fully describe the distribution. Knowing \\(p\\) and \\(N\\) is enough to know the entire distribution within the Binomial family. That is, no further information is needed – we know all \\(N+1\\) probabilities based on only two numbers! Since \\(p\\) and \\(N\\) fully specify a Binomial distribution, we call them parameters of the Binomial family, and we call the Binomial family a parametric family of distributions. In general, a parameter is a variable whose specification narrows down the space of possible distributions (or to be even more general, the space of possible models). 6.1.4 Parameterization A Binomial distribution can be specified by knowing \\(N\\) and \\(p\\), but there are other ways we can specify the distribution. For instance, specifying the mean and variance is enough to specify a Binomial distribution. Demonstration: Which Binomial distribution has mean 2 and variance 1? (on the whiteboard) Exactly which variables we decide to use to identify a distribution within a family is called the family’s parameterization. So, the Binomial distribution is usually parameterized according to \\(N\\) and \\(p\\), but could also be parameterized in terms of the mean and variance. The “usual” parameterization of a distribution family is sometimes called the canonical parameterization. In general, there are many ways in which a distribution family can be parameterized. The parameterization you use in practice will depend on the information you can more easily obtain. 6.1.5 Distribution Families in Practice Why is it useful to know about distribution families? In general when we’re modelling something, like river flow or next month’s net gains or the number of ships arriving at port tomorrow, you have the choice to make a distributional assumption or not. That is, do you want to declare the random variable of interest as belonging to a certain distribution family, or do you want to allow the random variable to have a fully general distribution? Both are good options depending on the scenario, and later in the program, we’ll explore the tradeoff with both options in more detail. 6.2 Common Parametric Families 6.2.1 Geometric Process: Suppose you play a game, and win with probability \\(p\\). Let \\(X\\) be the number of attempts at playing the game before experiencing a win. Then \\(X\\) is said to have a Geometric distribution. Note: Sometimes this family is defined so that \\(X\\) includes the winning attempt. The properties of the distribution differ, so be sure to be deliberate about which one you use. Since there’s only one parameter, this means that if you know the mean, you also know the variance! Code: The pmf can be calculated in R with dgeom(); in python, scipy.stats.geom. 6.2.2 Negative Binomial Process: Suppose you play a game, and win with probability \\(p\\). Let \\(X\\) be the number of attempts at playing the game before experiencing \\(k\\) wins. Then \\(X\\) is said to have a Negative Binomial distribution. Two parameters. The Geometric family results with \\(k=1\\). Code: The pmf can be calculated in R with dnb(); in python, scipy.stats.nbinom. 6.2.3 Poisson Process: Suppose customers independently arrive at a store at some average rate. The total number of customers having arrived after a pre-specified length of time follows a Poisson distribution, and can be parameterized by a single parameter, usually the mean \\(\\lambda\\). A noteable property of this family is that the mean is equal to the variance. Examples that are indicative of this process: The number of ships that arrive at the port of Vancouver in a given day. The number of emails you receive in a given day. Code: The pmf can be calculated in R with dpois(); in python, scipy.stats.poisson. 6.2.4 Bernoulli A random variable that is either \\(1\\) (with probability \\(p\\)) or \\(0\\) (with probability \\(1-p\\)). Parameterized by \\(p\\). A special case of the Binomial family, with \\(N=1\\). 6.2.5 Uniform (3 min) A Uniform distribution has equal density in between two points \\(a\\) and \\(b\\) (for \\(a &lt; b\\)), and is usually denoted by \\[\\text{Unif}(a, b).\\] That means that there are two parameters: one for each end-point. Reference to a “Uniform distribution” usually implies continuous uniform, as opposed to discrete uniform. The density is \\[f(x) = \\frac{1}{b - a} \\text{ for } a \\leq x \\leq b.\\] Here are some densities from members of this family: tibble( x = seq(-3.5, 1.5, length.out = 1000), `Unif(0,1)` = dunif(x, min = 0, max = 1), `Unif(-3, 1)` = dunif(x, min = -3, max = 1), `Unif(-2, -1.5)` = dunif(x, min = -2, max = -1.5) ) %&gt;% pivot_longer(contains(&quot;Unif&quot;), names_to = &quot;distribution&quot;, values_to = &quot;density&quot;) %&gt;% ggplot(aes(x, density)) + facet_wrap(~ distribution) + geom_line() + theme_bw() 6.2.6 Gaussian / Normal (4 min) Probably the most famous family of distributions. Has a density that follows a “bell-shaped” curve. Is usually parameterized by its mean \\(\\mu\\) and variance \\(\\sigma^2\\) (or sometimes just the standard deviation). A Normal distribution is usually denoted as \\[N(\\mu, \\sigma^2).\\] The density is \\[f(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\\] Here are some densities from members of this family: expand_grid(mu = c(-3, 0, 3), sd = c(0.5, 1, 2)) %&gt;% mutate(f = map2( mu, sd, ~ tibble( x = seq(-8, 8, length.out = 1000), density = dnorm(x, mean = .x, sd = .y) ) )) %&gt;% unnest(f) %&gt;% mutate(mu = str_c(&quot;mean = &quot;, mu), var = str_c(&quot;variance = &quot;, sd^2)) %&gt;% ggplot(aes(x, density)) + facet_grid(mu ~ var) + geom_line() + theme_bw() 6.2.7 Log-Normal Family A random variable \\(X\\) as a Log-Normal distribution if \\(\\log X\\) is Normal. This family is often parameterized by the mean \\(\\mu\\) and variance \\(\\sigma^2\\) of \\(\\log X\\). The Log-Normal family is sometimes denoted, and this course will denote this family, as \\[LN(\\mu, \\sigma^2).\\] Here are some densities from members of this family: expand_grid(meanlog = c(-0.5, 0, 1), sdlog = c(0.5, sqrt(0.75), 1.5)) %&gt;% mutate(f = map2( meanlog, sdlog, ~ tibble( x = seq(0, 8, length.out = 1000), density = dlnorm(x, meanlog = .x, sdlog = .y) ) )) %&gt;% unnest(f) %&gt;% mutate(meanlog = str_c(&quot;mu = &quot;, meanlog), varlog = str_c(&quot;sigma^2 = &quot;, sdlog^2)) %&gt;% ggplot(aes(x, density)) + facet_grid(meanlog ~ varlog) + geom_line() + theme_bw() 6.2.8 Exponential Family The exponential family is for positive random variables, often interpreted as “wait time” for some event to happen. Characterized by a “memoryless” property, where after waiting for a certain period of time, the remaining wait time has the same distribution. The family is characterized by a single parameter, usually either the mean wait time, or its reciprocal, the average rate at which events happen. The densities from this family all decay starting at \\(x=0\\) for rate \\(\\beta\\): tibble(beta = c(1, 0.5, 0.25)) %&gt;% mutate(f = map( beta, ~ tibble( x = seq(0, 10, length.out = 1000), density = dexp(x, rate = .x) ) )) %&gt;% unnest(f) %&gt;% mutate(beta = str_c(&quot;beta = &quot;, beta)) %&gt;% ggplot(aes(x, density)) + facet_wrap(~ beta) + geom_line() + theme_bw() 6.2.9 Weibull Family A generalization of the Exponential family, which allows for an event to be more or less likely the longer you wait. Because of this flexibility and interpretation, this family is used heavily in survival analysis when modelling “time until an event”. This family is characterized by two parameters, a scale parameter \\(\\lambda\\) and a shape parameter \\(k\\) (where \\(k=1\\) results in the Exponential family). Here are some densities: expand_grid(k = c(0.5, 2, 5), lambda = c(0.5, 1, 1.5)) %&gt;% mutate(f = map2( k, lambda, ~ tibble( x = seq(0, 3, length.out = 1000), density = dweibull(x, shape = .x, scale = .y) ) )) %&gt;% unnest(f) %&gt;% mutate(k = str_c(&quot;k = &quot;, k), lambda = str_c(&quot;lambda = &quot;, lambda)) %&gt;% ggplot(aes(x, density)) + facet_grid(k ~ lambda) + geom_line() + theme_bw() + ylim(c(0, 5)) 6.2.10 Beta Family The Beta family of distributions is defined for random variables taking values between 0 and 1, so is useful for modelling the distribution of proportions. This family is quite flexible, and has the Uniform distribution as a special case. Characterized by two positive shape parameters, \\(\\alpha\\) and \\(\\beta\\). Examples of densities: expand_grid(alpha = c(0.5, 1, 2), beta = c(0.25, 1, 1.25)) %&gt;% mutate(f = map2( alpha, beta, ~ tibble( x = seq(0, 1, length.out = 1000), density = dbeta(x, shape1 = .x, shape2 = .y) ) )) %&gt;% unnest(f) %&gt;% mutate(alpha = str_c(&quot;alpha = &quot;, alpha), beta = str_c(&quot;beta = &quot;, beta)) %&gt;% ggplot(aes(x, density)) + facet_grid(beta ~ alpha) + geom_line() + theme_bw() + ylim(c(0, 4)) 6.2.11 Activity True or False? Knowing the probability of an event means that we also know the odds; but knowing the odds is not enough to calculate probability. A Binomial distribution only has one mean, but there are many Binomial distributions that have the same mean. A Poisson distribution only has one mean, but there are many Poisson distributions that have the same mean. 6.3 Relevant R functions (8 min) R has functions for many distribution families. We’ve seen a few already in the case of discrete families, but here’s a more complete overview. The functions are of the form &lt;x&gt;&lt;dist&gt;, where &lt;dist&gt; is an abbreviation of a distribution family, and &lt;x&gt; is one of d, p, q, or r, depending on exactly what about the distribution you’d like to calculate. Possible prefixes &lt;x&gt;: d: density function - we call this \\(p\\) p: cumulative distribution function (cdf) - we call this \\(F\\) q: quantile function (inverse cdf) r: random sample generation Some abbreviations &lt;dist&gt;: unif: Uniform (continuous) norm: Normal (continuous) lnorm: Log-Normal (continuous) geom: Geometric (discrete) pois: Poisson (discrete) binom: Binomial (discrete) etc. Examples: The uniform family: dunif(), punif(), qunif(), runif() The Gaussian family: dnorm(), pnorm(), qnorm(), rnorm() Demonstration: What’s the density of the N(2, 4) distribution evaluated at the point \\(x = 3\\)? What’s the cdf of the Unif(0, 1) distribution evaluated at the points \\(x = 0.25, 0.5, 0.75\\)? What’s the median of the Unif(0, 2) distribution? Generate a random sample from the N(0, 1) distribution of size 10. 6.4 Analyses under a Distributional Assumption Remember that the purpose of a univariate analysis on its own is to estimate parameters, communicate uncertainty about the estimates, and specify the distribution that “generated” the data. So far, we’ve seen how to do this using empirical methods, which don’t make any assumptions about how the data were obtained. But, if we suspect we know what distribution family generated the data, then this is potentially valuable information that can benefit the analysis. This chapter explains how to proceed with your analysis by making a distributional assumption, and when this might be a good idea. In the previous chapter, we estimated parameters like means and probabilities using empirical (or “sample versions”) of these parameters. Usually, these estimators perform well in the univariate setting, but there are some circumstances where they do not. There is a technique called Maximum Likelihood Estimation (MLE) that approximates the data distribution using a parametric family, and if done carefully, allows for significant improvements to estimation. This chapter first explains what MLE is and how to implement the technique, as well as why and when it would be of use. 6.4.1 Maximum Likelihood Estimation Maximum likelihood estimation is a way of estimating parameters by first estimating the data distribution from a specified parametric family. The steps are as follows. Make a distributional assumption: Choose a parametric family of distributions that you think is a decent approximation to the data distribution. Estimate: From that family, choose the distribution that fits the data “best”. Extract: Using the fitted distribution, extract the parameter(s) of interest (such as the mean and/or quantiles). Check the assumption: Check that the fitted distribution is a reasonable approximation to the data distribution (not required for estimation, but is good practice). Let’s look at these steps in turn, together with the following examples. Consider the following data set of damages caused by hurricanes, in billions of USD. The distribution of the 144 observations is depicted in the following histogram: suppressPackageStartupMessages(library(tidyverse)) data(damage, package = &quot;extRemes&quot;) house &lt;- suppressMessages(read_csv(&quot;data/house.csv&quot;)) (hurricane_hist &lt;- ggplot(damage, aes(Dam)) + geom_histogram(aes(y = ..density..), bins = 50, alpha = 0.75) + theme_bw() + scale_x_continuous(&quot;Damage (billions of USD)&quot;, labels = scales::dollar_format()) + ylab(&quot;Density&quot;)) Data such as this can help an insurance company with their financial planning. Knowing an upper quantile, such as the 0.8-quantile, would give a sense of the damage caused by the “worst” hurricanes. Consider the following sale prices of houses: house &lt;- read_csv(&quot;data/house.csv&quot;) ## Rows: 1460 Columns: 81 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (43): MSZoning, Street, Alley, LotShape, LandContour, Utilities, LotConf... ## dbl (38): Id, MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, Ye... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. house_hist &lt;- ggplot(house, aes(SalePrice)) + geom_histogram(aes(y = ..density..), alpha = 0.75) + theme_bw() + labs(x = &quot;Price&quot;, y = &quot;Density&quot;) + scale_x_continuous(labels = scales::dollar_format()) cowplot::plot_grid( house_hist, house_hist + scale_x_log10(labels = scales::dollar_format()) ) ## Scale for &#39;x&#39; is already present. Adding another scale for &#39;x&#39;, which will ## replace the existing scale. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.4.1.1 Step 1: Make a distributional assumption This step requires choosing a family to approximate the data distribution (you can find several families defined in the previous chapter). Here are two general guidelines that may help you choose a distribution family. Visually match the variable characteristics and shape of the data distribution to a distribution family. If possible, think about the process that “generated” the data, and match that process to the data-generating process defining a family. Always keep in mind that there is almost never a “correct” choice! Remember, we are making an approximation here, not seeking a “true” distribution family. Example 1. In the hurricane damages example, the data variable is a positive continuous variable, and has a histogram that appears to decay starting from a damage of zero. The selected distribution should accomodate this – Weibull, Gamma, or GPD can all accomodate this. But the family should allow for heavy-tailed distributions to accomodate the two large data values that we see in the histogram, leaving a GPD as a good candidate. As for the way the data are “generated”, the data are inherently recorded because they are extreme, and this matches the way that a GPD is derived. Example 2. In the house prices example, the family should also accomodate a positive continuous random variable, should be unimodal with two tails, and be skewed to the right. A Weibull or lognormal distribution so far seem like good candidates. Since the log of house prices looks like a Gaussian distribution, we choose a lognormal distribution, since this is how the lognormal family is defined. 6.4.1.2 Step 2: Estimate So far, we’ve selected a family of distributions. Now, in this step, we need to select the distribution from this family that best matches the data. This step is the namesake of MLE. The key is to select the distribution for which the observed data are most likely to have been drawn from. We can do this through a quantity called the likelihood, which can be calculated for any distribution that has a density/pmf, then finding the distribution that has the largest likelihood. Let’s break these two concepts down. Likelihood. The likelihood is a useful way to measure how well a distribution fits the data. To calculate the likelihood, denoted \\(\\mathcal{L}\\), from data \\(y_1, \\ldots, y_n\\) and a distribution with density/pmf \\(f\\), calculate the product of the densities/pmf’s evaluated at the data: \\[\\mathcal{L} = \\prod_{i=1}^n f(y_i).\\] When \\(f\\) is a pmf, you can interpret the likelihood as the probability of observing the data under the distribution \\(f\\). When \\(f\\) is a density, the interpretation of likelihood is less tangible, and is the probability density of observing the data under the distribution \\(f\\). These interpretations are exactly true if the data are independent, but are still approximately true if data are “almost” independent. Even with non-independent data, the likelihood is still a useful measurement. A similar quantity to the likelihood is the negative log likelihood (nllh), defined as \\[\\ell = -\\log\\mathcal{L} = -\\sum_{i=1}^n \\log f(y_i).\\] The nllh is numerically more convenient than the likelihood. For example, the likelihood (\\(\\mathcal{L}\\)) tends to be an extremely small number, whereas the nllh typically is not. For instance, 100 draws from a N(0,1) distribution results in a likelihood that’s typically around \\(3 \\times 10^{-62},\\) whereas the nllh is typically around 141.5. Note that minimizing the nllh is the same as maximizing the likelihood. Finding the distribution that has the largest likelihood Remember that each distribution in the distribution family that we selected can be represented by its parameters – for example, the Normal distribution by its mean and variance, or the Beta distribution by \\(\\alpha\\) and \\(\\beta\\). This means that we can view the likelihood as a function of the family’s parameters, and optimize this function! This can sometimes be done using calculus, but is most often done numerically. We end up with estimates of the distribution’s parameters, which is the same thing as having an estimate of the data’s distribution. Don’t stop here if you are looking to estimate something other than the distibution’s parameters – move on to Step 3. Example 1. For the hurricane example, first find the GPD shape and scale parameters that maximize the likelihood (or, minimize the nllh). gpd_nllh &lt;- function(parameters) { scale &lt;- parameters[1] shape &lt;- parameters[2] if (scale &lt;= 0) return(Inf) -sum(evd::dgpd(damage$Dam, scale = scale, shape = shape, log = TRUE)) } gpd_optim &lt;- optim(c(1, 1), gpd_nllh) gpd_scale &lt;- gpd_optim$par[1] gpd_shape &lt;- gpd_optim$par[2] Take a look at the GPD density corresponding to these parameters, with the data histogram in the background: gpd_density &lt;- function(x) evd::dgpd(x, scale = gpd_scale, shape = gpd_shape) hurricane_hist + stat_function(fun = gpd_density, colour = &quot;blue&quot;) + ylim(c(0, 0.13)) ## Warning: Removed 1 rows containing missing values (geom_bar). ## Warning: Removed 2 row(s) containing missing values (geom_path). Example 2. ln_nllh &lt;- function(parameters) { loc &lt;- parameters[1] scale &lt;- parameters[2] if (scale &lt;= 0) return(Inf) -sum(dlnorm(house$SalePrice, meanlog = loc, sdlog = scale, log = TRUE)) } ln_optim &lt;- optim(c(0, 1), ln_nllh) ln_loc &lt;- ln_optim$par[1] ln_scale &lt;- ln_optim$par[2] ln_density &lt;- function(x) dlnorm(x, meanlog = ln_loc, sdlog = ln_scale) house_hist + stat_function(fun = ln_density, colour = &quot;blue&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 6.4.1.3 Step 3: Extract Now that we have the data distribution estimated, we can extract any parameter we’d like, such as the mean or quantiles. This might involve looking up formulas based on the distribution’s parameters – for example, the mean of a Beta distribution with parameters \\(\\alpha\\) and \\(\\beta\\) is \\(\\alpha/(\\alpha + \\beta)\\). Example 1. Here is the MLE of the median hurricane damage: gpd_qf &lt;- function(p) evd::qgpd(p, scale = gpd_scale, shape = gpd_shape) gpd_qf(0.5) ## [1] 0.1884567 Here is the MLE of the 0.9-quantile: gpd_qf(0.9) ## [1] 6.659546 Here is the MLE of the IQR: gpd_qf(0.75) - gpd_qf(0.25) ## [1] 0.9199308 Example 2. Mean of the Lognormal distribution can be computed as \\(\\exp(\\mu + \\sigma^2/2)\\); so the MLE for the mean is: exp(ln_loc + ln_scale^2/2) ## [1] 180579.9 Variance of the Lognormal distribution can be computed as \\((\\exp(\\sigma^2)-1)(\\exp(2\\mu + \\sigma^2)\\); so the MLE for the variance is: (exp(ln_scale^2) - 1) * (exp(2*ln_loc + ln_scale^2)) ## [1] 5642912264 The MLE for the 0.9-quantile is: qlnorm(0.9, meanlog = ln_loc, sdlog = ln_scale) ## [1] 278205 6.4.1.4 Step 4: Check the Assumption In order to end up with “good” estimates from MLE, the fitted distribution from Step 2 should be a decent approximation to the data distribution. To see why, consider a poor distributional assumption – such as approximating the distribution of house prices as Uniform. We end up with the following density fitted by MLE: ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## [1] 214925 ## [1] 574975 The fitted density is flat, and does not match the data (histogram) at all. This means we can anticipate parameter estimates to be way off. For example, according to the model, the mean is somewhere around $400,000, and a half of the house prices roughly lie somewhere between $200,000 and $600,000. The above method of comparing the modelled density to the histogram is one effective way of checking the distributional assumption. Another way is to use a QQ-plot. These visual methods might seem informal, but they are very powerful, and should always be investigated if possible. After visualizing the fit, if you want to add more rigor to your assumption checking, you can consider a hypothesis test such as the Kolmogorov-Smirnov test, the Anderson-Darling test, or the Cram'{e}r-von Mises test for equality of distributions. Just remember that there is no such thing as a “correct” distributional assumption in practice, but rather just approximations that have different degrees of plausibility. The key is in avoiding bad assumptions, as opposed to finding the “right” assumption. 6.4.2 Usefulness in Practice The MLE is an overall superior way to estimate parameters, and has some theoretically desirable properties – as long as the distributional assumption from Step 1 is not a bad one. Overall, in the univariate setting, the improvement brought about by MLE is in general underwhelming, except in some situations. Where MLE really shines is in the regression setting, as is shown in a later chapter, but it’s important to understand the fundamentals in the univariate case before extending concepts to the regression setting. This chapter explores the following: Under what situations in the univariate setting does the MLE really shine? Just how “bad” does an assumption have to be in order for the MLE to be worse than the empirical estimate? Ultimately, the question is not whether the MLE is better than empirical estimates, but rather whether making a distributional assumption is better than not. 6.4.2.1 Where MLE really shines 6.4.2.2 Effect of Assumption Badness Take-aways: sampling distributions are narrower for MLE, but more and more biased as the distributional assumption gets worse and worse. Most of the time in the univariate setting, the MLE is often not much better than the empirical estimate, and is sometimes even identical. Aside from the cases described below, where MLE really shines is in the regression setting, Take a look at the sampling distributions of three estimators of the 0.8-quantile: the sample version, the MLE under a Gaussian assumption, and the MLE under a generalized Pareto distribution (GPD) assumption. The sampling distributions are obtained using bootstrapping. The Gaussian-based MLE is an example of a bad MLE that uses a poorly chosen assumption – its sampling distribution is much wider than that of the sample version. On the other hand, the GPD-based MLE is based on a much more realistic assumption, and shows a significant improvement to the sample version – its sampling distribution is much narrower. # dgpd &lt;- function(x, sig, xi) 1/sig*(1 + xi*(x/sig))^(-1/(xi+1)) # gpd_quantile &lt;- function(x, p = 0.5) { # fit &lt;- ismev::gpd.fit(x, 0, show = FALSE) # sigma &lt;- fit$mle[1] # xi &lt;- fit$mle[2] # sigma * (p^(-xi) - 1) / xi # } # wei_quantile &lt;- function(x, p = 0.5) { # nllh &lt;- function(par) -sum(dweibull(x, par[1], par[2], log = TRUE)) # par_hat &lt;- optim(c(1,1), nllh)$par # qweibull(p, par_hat[1], par_hat[2]) # } # sampling_dist &lt;- damage %&gt;% # bootstraps(times = 1000) %&gt;% # pull(splits) %&gt;% # map(as_tibble) %&gt;% # map_df(~ summarise( # .x, # bar = quantile(Dam, probs = 0.8), # mle_gpd = gpd_quantile(Dam, 0.8), # mle_gau = qnorm(0.8, mean = mean(Dam), sd = sd(Dam))) # ) # sampling_dist %&gt;% # gather(key = &quot;method&quot;, value = &quot;estimate&quot;) %&gt;% # ggplot(aes(estimate)) + # facet_wrap(~method, nrow = 1, scales = &quot;free_x&quot;) + # geom_histogram(bins = 30) + # theme_bw() Caution: A common misconception is that the two large observations in the dataset are outliers, and therefore should be removed. However, doing so would bias our understanding of hurricance damages, since these “outliers” are real occurences. In other cases, an MLE does not result in much of an improvement at all. Although there are far fewer cases in the univariate case compared to the regression setting where MLE gives a dramatic improvement to estimation, it’s still worth discussing when it’s most useful in the univariate setting and to ground concepts. High quantile example for a PI: # N &lt;- 10000 # rate &lt;- 1 # ordered &lt;- numeric(0) # mle &lt;- numeric(0) # for (i in 1:N) { # x &lt;- rexp(10, rate = rate) # ordered[i] &lt;- quantile(x, probs = 0.975, type = 1) # mle[i] &lt;- qexp(0.975, rate = 1/mean(x)) # } # tibble(ordered, mle) %&gt;% # gather(value = &quot;estimate&quot;) %&gt;% # ggplot(aes(estimate)) + # geom_density(aes(group = key, fill = key), alpha = 0.5) + # geom_vline(xintercept = qexp(0.975, rate = rate), # linetype = &quot;dashed&quot;) + # theme_bw() # sd(ordered) # sd(mle) In both cases, the sampling distribution of the MLE is better than that of the sample version – that is, more narrow and (sometimes) centered closer to the true value. Is there a better estimator than the MLE? It turns out that the MLE is realistically the best that we can do – as long as the distributional assumption is not too bad of an approximation. If you’re curious to learn more, the end of this chapter fleshes this out using precise terminology. If the improvement by using MLE does not seem very impressive to you, you’d be right – at least in the univariate world. To see much difference between the MLE and sample version estimators, you’d need to be estimating low-probability events with a small amount of data. In fact, estimating the mean using MLE most often results in the same estimator as the sample mean! Don’t write off the MLE just yet – it really shines in the regression setting, where it has even more benefits than just improved estimation. Tune in to Part II to learn more. As an example of (1), suppose you are measuring the ratio of torso height to body height. Since your sample falls between 0 and 1, and probably does not have a weirdly shaped density, a Beta distribution would be a good assumption, since the Beta family spans non-weird densities over (0, 1). However, not knowing the data-generating process, you would not be able to justify the distribution completely (and that’s OK). As an example of (2), perhaps you are operating the port of Vancouver, BC, and based on your experience, know that vessels arrive more-or-less independently at some average rate. This is how a Poisson distribution is defined. Not only that, but the data appear to be shaped like a Poisson distribution. Then it would be justifiable to assume the data follow a Poisson distribution. # n &lt;- 50 # N &lt;- 1000 # fit_mle &lt;- numeric(0) # fit_ls &lt;- numeric(0) # for (i in 1:N) { # x &lt;- rnorm(n) # mu &lt;- 1/(1+exp(-x)) # y &lt;- rbinom(n, size = 1, prob = mu) # fit_mle[i] &lt;- glm(y ~ x, family = &quot;binomial&quot;)$coefficients[2] # ls &lt;- function(par) sum((y - 1/(1+exp(-par[1]-par[2]*x)))^2) # fit_ls[i] &lt;- optim(c(0,1), ls)$par[2] # } # tibble(fit_mle, fit_ls) %&gt;% # gather(value = &quot;beta&quot;) %&gt;% # ggplot(aes(beta)) + # # scale_x_log10() + # geom_density(aes(group = key, fill = key), alpha = 0.5) # sd(fit_mle) # sd(fit_ls) # IQR(fit_mle) # IQR(fit_ls) # ## More extremes show up with LS (at least with n=50): # sort(fit_ls) %&gt;% tail(10) # sort(fit_mle) %&gt;% tail(10) # ## Gaussian assumption # ## - LS not even that good at n=100 -- bowed down. MLE is good. # ## - MLE qqplot with n=50 looks about the same as LS with n=100 # ## - LS at n=50 is heavy tailed (seemingly). # qqnorm(fit_mle) # qqnorm(fit_ls) For n=50, check out an example that results in an extreme beta: # n &lt;- 50 # beta &lt;- 0 # while (beta &lt; 300) { # x &lt;- rnorm(n) # mu &lt;- 1/(1+exp(-x)) # y &lt;- rbinom(n, size = 1, prob = mu) # ls &lt;- function(par) sum((y - 1/(1+exp(-par[1]-par[2]*x)))^2) # .optim &lt;- optim(c(0,1), ls) # beta &lt;- .optim$par[2] # alpha &lt;- .optim$par[1] # } # if (.optim$convergence == 0) stop(&quot;optim didn&#39;t successfully converge.&quot;) # mle &lt;- glm(y ~ x, family = &quot;binomial&quot;)$coefficients # qplot(x, y) + # stat_function(fun = function(x) 1/(1+exp(-alpha-beta*x)), mapping = aes(colour = &quot;LS&quot;)) + # stat_function(fun = function(x) 1/(1+exp(-mle[1]-mle[2]*x)), mapping = aes(colour = &quot;MLE&quot;)) # # MLE is still slightly narrower, even for a Beta(2,2) distribution (which is # # symmetric and bell-like) -- for n=5 and n=50. Both close to Gaussian, even at # # n=5 (as expected). # shape1 &lt;- 2 # shape2 &lt;- 2 # foo &lt;- function(x) dbeta(x, shape1, shape2) # curve(foo, 0, 1) # n &lt;- 5 # N &lt;- 1000 # xbar &lt;- numeric(0) # mle &lt;- numeric(0) # for (i in 1:N) { # x &lt;- rbeta(n, shape1, shape2) # xbar[i] &lt;- mean(x) # nllh &lt;- function(par) { # if (min(par) &lt;= 0) return(Inf) # -sum(dbeta(x, par[1], par[2], log = TRUE)) # } # .optim &lt;- optim(c(shape1, shape2), nllh) # par_hat &lt;- .optim$par # mle[i] &lt;- par_hat[1] / sum(par_hat) # } # plot(mle - xbar) # The estimates aren&#39;t the same. # tibble(mle, xbar) %&gt;% # gather() %&gt;% # ggplot(aes(value)) + # geom_density(aes(group = key, fill = key), alpha = 0.5) # sd(mle) # sd(xbar) # qqnorm(mle) # qqnorm(xbar) # # Univariate MLE *especially* important for heavy tailed distributions! # nu &lt;- 1.5 # n &lt;- 5 # N &lt;- 1000 # xbar &lt;- numeric(0) # mle &lt;- numeric(0) # for (i in 1:N) { # x &lt;- rt(n, df = nu) # xbar[i] &lt;- mean(x) # nllh &lt;- function(par) -sum(dt(x, df = par[1], ncp = par[2], log = TRUE)) # .optim &lt;- optim(c(nu, 0), nllh) # mle[i] &lt;- .optim$par[2] # } # plot(mle - xbar) # The estimates aren&#39;t the same. # tibble(mle, xbar) %&gt;% # gather() %&gt;% # ggplot(aes(value)) + # geom_density(aes(group = key, fill = key), alpha = 0.5) # sd(mle) # sd(xbar) # qqnorm(mle) # qqnorm(xbar) "],["prediction-harnessing-the-signal.html", "Prediction: harnessing the signal", " Prediction: harnessing the signal "],["reducing-uncertainty-of-the-outcome-conditional-distributions.html", "Chapter 7 Reducing uncertainty of the outcome: conditional distributions 7.1 Conditional Distributions 7.2 Joint Distributions 7.3 Multivariate Densities/pdf’s 7.4 Dependence concepts 7.5 Harvesting Dependence 7.6 Marginal Distributions", " Chapter 7 Reducing uncertainty of the outcome: conditional distributions 7.1 Conditional Distributions Probability distributions describe an uncertain outcome, but what if we have partial information? Consider the example of ships arriving at the port of Vancouver again. Each ship will stay at port for a random number of days, which we’ll call the length of stay (LOS) or \\(D\\), according to the following (made up) distribution: Length of Stay (LOS) Probability 1 0.25 2 0.35 3 0.20 4 0.10 5 0.10 Suppose a ship has been at port for 2 days now, and it’ll be staying longer. What’s the distribution of length-of-stay now? Using symbols, this is written as \\(P(D = d \\mid D &gt; 2)\\), where the bar “|” reads as “given” or “conditional on”, and this distribution is called a conditional distribution. We can calculate a conditional distribution in two ways: a “table approach” and a “formula approach”. Table approach: Subset the pmf table to only those outcomes that satisfy the condition (\\(D &gt; 2\\) in this case). You’ll end up with a “sub table”. Re-normalize the remaining probabilities so that they add up to 1. You’ll end up with the conditional distribution under that condition. Formula approach: In general for events \\(A\\) and \\(B\\), the conditional probability formula is \\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}.\\] For the ship example, the event \\(A\\) is \\(D = d\\) (for all possible \\(d\\)’s), and the event \\(B\\) is \\(D &gt; 2\\). Plugging this in, we get \\[P(D = d \\mid D &gt; 2) = \\frac{P(D = d \\cap D &gt; 2)}{P(D &gt; 2)} = \\frac{P(D = d)}{P(D &gt; 2)} \\text{ for } d = 3,4,5.\\] The only real “trick” is the numerator. How did we reduce the convoluted event \\(D = d \\cap D &gt; 2\\) to the simple event \\(D = d\\) for \\(d = 3,4,5\\)? The trick is to go through all outcomes and check which ones satisfy the requirement \\(D = d \\cap D &gt; 2\\). This reduces to \\(D = d\\), as long as \\(d = 3,4,5\\). 7.2 Joint Distributions So far we’ve only considered one random variable at a time. Its distribution is called univariate because there’s just one variable. But, we very often have more than one random variable. Let’s start by considering … We can visualize this as a joint distribution: Don’t be fooled, though! This is not really any different from what we’ve already seen. We can still write this a univariate distribution with four categories. This is useful to remember when we’re calculating probabilities. Outcome Probability HH 0.25 HT 0.25 TH 0.25 TT 0.25 Viewing the distribution as a (2-dimensional) matrix instead of a (1-dimensional) vector turns out to be more useful when determining properties of individual random variables. 7.2.1 Example: Length of Stay vs. Gang Demand Throughout today’s class, we’ll be working with the following joint distribution of length of stay of a ship, and its gang demand. Gangs = 1 Gangs = 2 Gangs = 3 Gangs = 4 LOS = 1 0.0017 0.0425 0.1247 0.0811 LOS = 2 0.0266 0.1698 0.1360 0.0176 LOS = 3 0.0511 0.1156 0.0320 0.0013 LOS = 4 0.0465 0.0474 0.0059 0.0001 LOS = 5 0.0740 0.0246 0.0014 0.0000 The joint distribution is stored in “tidy format” in an R variable named j: 7.2.2 Marginal Distributions We’ve just specified a joint distribution of length of stay and gang request. But, we’ve previously specified a distribution for these variables individually. These are not things that can be specified separately: If you have a joint distribution, then the distribution of each individual variable follows as a consequence. If you have the distribution of each individual variable, you still don’t have enough information to form the joint distribution between the variables. The distribution of an individual variable is called the marginal distribution (sometimes just “marginal” or “margin”). The word “marginal” is not really needed when we’re talking about a random variable – there’s no difference between the “marginal distribution of length of stay” and the “distribution of length of stay”, we just use the word “marginal” if we want to emphasize the distribution is being considered in isolation from other related variables. 7.2.3 Calculating Marginals from the Joint There’s no special way of calculating a marginal distribution from a joint distribution. As usual, it just involves adding up the probabilities corresponding to relevant outcomes. For example, to compute the marginal distribution of length of stay (LOS), we’ll first need to calculate \\(P(\\text{LOS} = 1)\\). Using the joint distribution of length of stay and gang request, the outcomes that satisfy this requirement are the entire first row of the probability table. It follows that the marginal distribution of LOS can be obtained by adding up each row. For the marginal of gang requests, just add up the columns. Here’s the marginal of LOS (don’t worry about the code, you’ll learn more about this code in DSCI 523 next block). Notice that the distribution of LOS is the same as before! Length of Stay Probability 1 0.25 2 0.35 3 0.20 4 0.10 5 0.10 Similarly, the distribution of gang request is the same as from last lecture: Gang request Probability 1 0.2 2 0.4 3 0.3 4 0.1 7.2.4 Conditioning on one Variable What’s usually more interesting than a joint distribution are conditional distributions, when other variables are fixed. This is a special type of conditional distribution and an extremely important type of distribution in data science. For example, a ship is arriving, and they’ve told you they’ll only be staying for 1 day. What’s the distribution of their gang demand under this information? That is, what is \\(P(\\text{gang} = g \\mid \\text{LOS} = 1)\\) for all possible \\(g\\)? Table approach: Isolating the outcomes satisfying the condition (\\(\\text{LOS} = 1\\)), we obtain the first row: Gangs: 1 Gangs: 2 Gangs: 3 Gangs: 4 0.0017 0.0425 0.1247 0.0811 Now, re-normalize the probabilities so that they add up to 1, by dividing them by their sum, which is 0.25: Gangs: 1 Gangs: 2 Gangs: 3 Gangs: 4 0.0068 0.1701 0.4988 0.3243 Formula Approach: Applying the formula for conditional probabilities, we get \\[P(\\text{gang} = g \\mid \\text{LOS} = 1) = \\frac{P(\\text{gang} = g, \\text{LOS} = 1)}{P(\\text{LOS} = 1)},\\] which is exactly row 1 divided by 0.25. Here’s a plot of this distribution. For comparison, we’ve also reproduced its marginal distribution. Interpretation: given information, about length of stay, we get an updated picture of the distribution of gang requests. Useful for decision making! 7.2.5 Law of Total Probability/Expectation Quite often, we know the conditional distributions, but don’t directly have the marginals. In fact, most of regression and machine learning is about seeking conditional means! For example, suppose you have the following conditional means of gang request given the length of stay of a ship. This curve is called a model function, and is useful if we want to predict a ship’s daily gang request if we know their length of stay. But what if we don’t know their length of stay, and we want to produce an expected gang request? We can use the marginal mean of gang request! In general, a marginal mean can be computed from the conditional means and the probabilities of the conditioning variable. The formula, known as the law of total expectation, is \\[E(Y) = \\sum_x E(Y \\mid X = x) P(X = x).\\] Here’s a table that outlines the relevant values: Length of Stay (LOS) E(Gang | LOS) P(LOS) 1 3.140580 0.25 2 2.412802 0.35 3 1.917192 0.20 4 1.596041 0.10 5 1.273317 0.10 Multiplying the last two columns together, and summing, gives us the marginal expectation: 2.3. Also, remember that probabilities are just means, so the result extends to probabilities: \\[P(Y = y) = \\sum_x P(Y = y \\mid X = x) P(X = x)\\] This is actually a generalization of the law of total probability we saw before: \\(P(Y=y)=\\sum_x P(Y = y, X = x)\\). 7.2.6 Exercises In pairs, come to a consensus with the following three questions. Given the conditional means of gang requests, and the marginal probabilities of LOS in the above table, what’s the expected gang requests, given that the ship captain says they won’t be at port any longer than 2 days? In symbols, \\[E(\\text{Gang} \\mid \\text{LOS} \\leq 2).\\] What’s the probability that a new ship’s total gang demand equals 4? In symbols, \\[P(\\text{Gang} \\times \\text{LOS} = 4).\\] What’s the probability that a new ship’s total gang demand equals 4, given that the ship won’t stay any longer than 2 days? In symbols, \\[P(\\text{Gang} \\times \\text{LOS} = 4 \\mid \\text{LOS} \\leq 2).\\] 7.3 Multivariate Densities/pdf’s Recall the joint pmf (discrete) from Lecture 4, between gang demand and length-of-stay: Gangs = 1 Gangs = 2 Gangs = 3 Gangs = 4 LOS = 1 0.0017 0.0425 0.1247 0.0811 LOS = 2 0.0266 0.1698 0.1360 0.0176 LOS = 3 0.0511 0.1156 0.0320 0.0013 LOS = 4 0.0465 0.0474 0.0059 0.0001 LOS = 5 0.0740 0.0246 0.0014 0.0000 Each entry in the table corresponds to the probability of that unique row (LOS value) and column (Gang value). These probabilities add to 1. For the continuous case, instead of rows and columns, we have an x- and y-axis for our two variables, defining a region of possible values. For example, if two marathon runners can only finish a marathon between 5.0 and 5.5 hours each, and their end times are totally random, then the possible values are indicated by a square in the following plot: Each point in the square is like an entry in the joint pmf table in the discrete case, except now instead of holding a probability, it holds a density. The density function, then, is a surface overtop of this square (or in general, the outcome space). That is, it’s a function that takes two variables (marathon time for Runner 1 and Runner 2), and calculates a single density value from those two points. This function is called a bivariate density function. Here’s an example of what a 2D pdf might look like: https://scipython.com/blog/visualizing-the-bivariate-gaussian-distribution/ Notation: For two random variables \\(X\\) and \\(Y\\), their joint density/pdf evaluated at the points \\(x\\) and \\(y\\) is usually denoted \\[f_{X,Y}(x,y),\\] or sometimes less rigorously, as just \\[f(x, y).\\] 7.3.1 Conditional Distributions, revisited Remember the formula for conditional probabilities: for events \\(A\\) and \\(B\\), \\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}.\\] But, this is only true if \\(P(B) \\neq 0\\), and it’s not useful if \\(P(A) = 0\\) – two situations we’re faced with in the continuous world! 7.3.1.1 When \\(P(A) = 0\\) To describe this situation, let’s use a univariate continuous example: the example of monthly expenses. Suppose the month is half-way over, and you find that you only have $2500 worth of expenses so far! What’s the distribution of this month’s total expenditures now, given this information? If we use the law of conditional probability, we would get a formula that’s not useful: letting \\(X = \\text{Expense}\\), \\[P(X = x \\mid X \\geq 2500) = \\frac{P(X = x)}{P(X \\geq 2500)} \\ \\ \\ \\text{(no!)}\\] This is no good, because the outcome \\(x\\) has a probability of 0. This equation just simplies to 0 = 0, which is not useful. Instead, in general, we replace probabilities with densities. In this case, what we actually have is: \\[f(x \\mid X \\geq 2500) = \\frac{f(x)}{P(X \\geq 2500)} \\ \\text{ for } x \\geq 2500,\\] and \\(f(x \\mid X \\geq 2500) = 0\\) for \\(x &lt; 2500\\). Notice from the formula that the resulting density is just the original density confined to \\(x \\geq 2500\\), and re-normalized to have area 1. This is what we did in the discrete case! The monthly expense example has expenditures \\(X \\sim\\) LN(8, 0.5). Here is its marginal distribution and the conditional distribution. Notice the conditional distribution is just a segment of the marginal, and then re-normalized to have area 1. 7.3.1.2 When \\(P(B) = 0\\) To describe this situation, let’s use the marathon runners’ example again. Runner 1 ended up finishing in 5.2 hours. What’s the distribution of Runner 2’s time? Letting \\(X\\) be the time for Runner 1, and \\(Y\\) for Runner 2, we’re asking for \\(f_{Y|X}(y \\mid X = 5.2)\\). But wait! Didn’t we say earlier that \\(P(X = 5.2) = 0\\)? This is the bizarre nature of continuous random variables. Although no outcome is possible, we must observe some outcome in the end. In this case, the stopwatch used to calculate run time has rounded the true run time to 5.2h, even though in reality, it would have been something like 5.2133843789373… hours. As before, plugging in the formula for conditional probabilities won’t work. But, as the case when \\(P(A) = 0\\), we can in general replace probabilities with densities. We end up with \\[f_{Y|X}(y \\mid 5.2) = \\frac{f_{Y,X}(y, 5.2)}{f_X(5.2)}.\\] This formula is true in general \\[f_{Y|X}(y \\mid x) = \\frac{f_{Y,X}(y, x)}{f_X(x)}.\\] In fact, this formula is even true for both pdf’s and pmf’s! 7.4 Dependence concepts A big part of data science is about harvesting the relationship between \\(X\\) and \\(Y\\), often called the dependence between \\(X\\) and \\(Y\\). 7.4.1 Independence Informally, \\(X\\) and \\(Y\\) are independent if knowing something about one tells us nothing about the other. Formally, the definition of \\(X\\) and \\(Y\\) being independent is: \\[P(X = x \\cap Y = y) = P(X = x) P(Y = y).\\] More usefully and intuitively, it’s better to think of independence such that conditioning on an independent variable tells us nothing: \\[P(Y = y \\mid X = x) = P(Y = y).\\] This is far less interesting than when there’s dependence, which implies that there are relationships between variables. 7.4.2 Measures of dependence When there is dependence, it’s often useful to measure the strength of dependence. Here are some measurements. 7.4.2.1 Covariance and Pearson’s Correlation Covariance is one common way of measuring dependence between two random variables. The idea is to take the average “signed area” of rectangles constructed between a sampled point and the mean, with the sign being determined by “concordance” relative to the mean: Concordant means \\(x &lt; \\mu_x\\) and \\(y &lt; \\mu_y\\), OR \\(x &gt; \\mu_x\\) and \\(y &gt; \\mu_y\\) – gets positive area. Discordant means \\(x &lt; \\mu_x\\) and \\(y &gt; \\mu_y\\), OR \\(x &gt; \\mu_x\\) and \\(y &lt; \\mu_y\\) – gets negative area. Here is a random sample of 10 points, with the 10 rectangles constructed with respect to the mean. Sign is indicated by colour. The covariance is the mean signed area. Formally, the definition is \\[\\mathrm{Cov(X, Y)} = E[(X-\\mu_X)(Y-\\mu_Y)],\\] where \\(\\mu_Y=E(Y)\\) and \\(\\mu_X=E(X)\\). This reduces to a more convenient form, \\[\\text{Cov}(X,Y)=E(XY)-E(X)E(Y)\\] In R, you can calculate the empirical covariance using the cov function: ## [1] -0.7111111 In the above example, the boxes are more often negative, so the covariance (and the “direction of dependence”) is negative. For the above example, the larger the LOS, the smaller the gang demand – this inverse relationship is indicative of negative covariance. Other interpretations of the sign: Positive covariance indicates that an increase in one variable is associated with an increase in the other variable. Zero covariance indicates that there is no linear trend – but this does not necessarily mean that \\(X\\) and \\(Y\\) are independent! It turns out covariance by itself isn’t very interpretable, because it depends on the scale (actually, spread) of \\(X\\) and \\(Y\\). For example, multiply \\(X\\) by 10, and suddenly the box sizes increase by a factor of 10, too, influencing the covariance. Pearson’s correlation fixes the scale problem by standardizing the distances according to standard deviations \\(\\sigma_X\\) and \\(\\sigma_Y\\), defined as \\[\\text{Corr}(X, Y) = E\\left[ \\left(\\frac{X-\\mu_X}{\\sigma_X}\\right) \\left(\\frac{Y-\\mu_Y}{\\sigma_Y}\\right) \\right] =\\frac{\\text{Cov}(X, Y)}{\\sqrt{\\text{Var}(X)\\text{Var}(Y)}}.\\] As a result, it turns out that \\[-1 \\leq \\text{Corr}(X, Y) \\leq 1.\\] The Pearson’s correlation measures the strength of linear dependence: -1 means perfect negative linear relationship between \\(X\\) and \\(Y\\). 0 means no linear relationship (Note: this does not mean independent!) 1 means perfect positive linear relationship. In R, you can calculate the empirical Pearson’s correlation using the cor function: ## [1] -0.6270894 Pearson’s correlation is ubiquitous, and is often what is meant when “correlation” is referred to. 7.4.2.2 Kendall’s tau Although Pearson’s correlation is ubiquitous, its forced adherance to measuring linear dependence is a big downfall, especially because many relationships between real world variables are not linear. An improvement is Kendall’s tau (\\(\\tau_K\\)): Instead of measuring concordance between each observation \\((x, y)\\) and the mean \\((\\mu_x, \\mu_y)\\), it measures concordance between each pair of observation \\((x_i, y_i)\\) and \\((x_j, y_j)\\). Instead of averaging the area of the boxes, it averages the amount of concordance and discordance by taking the difference between number of concordant and number of discordant pairs. Visually plotting the \\(10 \\choose 2\\) boxes for the above sample from the previous section: The formal definition is \\[\\frac{\\text{Number of concordant pairs} - \\text{Number of discordant pairs}}{{n \\choose 2}},\\] with the “true” Kendall’s tau value obtainined by sending \\(n \\rightarrow \\infty\\). Note that several ways have been proposed for dealing with ties, but this doesn’t matter when we’re dealing with continuous variables (Weeks 3 and 4). In R, the empirical version can be calculated using the cor() function with method = \"kendall\": ## [1] -0.579771 Like Pearson’s correlation, Kendall’s tau is also between -1 and 1, and also measures strength (and direction) of dependence. For example, consider the two correlation measures for the following data set. Note that the empirical Pearson’s correlation for the following data is not 1! Pearson Kendall 0.9013 1 But, Kendall’s tau still only measures the strength of monotonic dependence. This means that patterns like a parabola, which are not monotonically increasing or decreasing, will not be captured by Kendall’s tau either: Pearson Kendall 0 0 Even though both dependence measures are 0, there’s actually deterministic dependence here (\\(X\\) determines \\(Y\\)). But, luckily, there are many monotonic relationships in practice, making Kendall’s tau a very useful measure of dependence. 7.4.3 Dependence as separate from the marginals The amount of monotonic dependence in a joint distribution, as measured by kendall’s tau, has nothing to do with the marginal distributions. This can be a mind-boggling phenomenon, so don’t fret if you need to think this over several times. To demonstrate, here are joint distributions between LOS and gang demand having the same marginals, but different amounts of dependence. 7.4.4 Dependence as giving us more information Let’s return to the computation of the conditional distribution of gang requests given that a ship will only stay at port for one day. Let’s compare the marginal distribution (the case where we know nothing) to the conditional distributions for different levels of dependence (like we saw in the previous section). The means for each distribution are indicated as a vertical line: ## `summarise()` has grouped output by &#39;dep&#39;. You can override using the `.groups` argument. What’s of particular importance is comparing the uncertainty in these distributions. Let’s look at how the uncertainty measurements compare between marginal and conditional distributions (marginal measurements indicated as horizontal line): Moral of the story: more dependence (in either direction) gives us more certainty in the conditional distributions! This makes intuitive sense, because the more related \\(X\\) and \\(Y\\) are, the more that knowing what \\(X\\) is will inform what \\(Y\\) is. 7.5 Harvesting Dependence The opposite of independence is dependence: when knowing something about \\(X\\) tells us something about \\(Y\\) (or vice versa). Extracting this “signal” that \\(X\\) contains about \\(Y\\) is at the heart of supervised learning (regression and classification), covered in DSCI 571/561 and beyond. Usually, we reserve the letter \\(X\\) to be the variable that we know something about (usually an exact value), and \\(Y\\) to be the variable that we want to learn about. These variables go by many names – usually, \\(Y\\) is called the response variable, and \\(X\\) is sometimes called a feature, or explanatory variable, or predictor, etc. To extract the information that \\(X\\) holds about \\(Y\\), we simply use the conditional distribution of \\(Y\\) given what we know about \\(X\\). This is as opposed to just using the marginal distribution of \\(Y\\), which corresponds to the case where we don’t know anything about \\(X\\). Sometimes it’s enough to just communicate the resulting conditional distribution of \\(Y\\), but usually we reduce this down to some of the distributional properties that we saw earlier, like mean, median, or quantiles. We communicate uncertainty also using methods we saw earlier, like prediction intervals and standard deviation. Let’s look at an example. 7.5.1 Example: River Flow In the Rocky Mountains, snowmelt \\(X\\) is a major driver of river flow \\(Y\\). Suppose the joint density can be depicted as follows: Every day, a measurement of snowmelt is obtained. To predict the river flow, usually the conditional mean of river flow given snowmelt is used as a prediction, but median is also used. Here are the two quantities as a function of snow melt: These functions are called model functions, and there are a ton of methods out there to help us directly estimate these model functions without knowing the density. This is the topic of supervised learning – even advanced supervised learning methods like deep learning are just finding a model function like this (although, usually when there are more than one \\(X\\) variable). It’s also quite common to produce prediction intervals. Here is an example of an 80% prediction interval, using the 0.1- and 0.9-quantiles as the lower and upper limits: As a concrete example, consider the case where we know there’s been 1mm of snowmelt. To obtain the conditional distribution of flow (\\(Y\\)) given this information, we just “slice” the joint density at \\(x =\\) 1, and renormalize. Here is that density (which is now univariate!), compared with the marginal distribution of \\(Y\\) (representing the case where we know nothing about snowmelt, \\(X\\)): The following table presents some properties of these distributions: Quantity Marginal Conditional Mean 247.31 118.16 Median 150 74.03 80% PI [41.64, 540.33] [25.67, 236.33] Notice that we actually only need the conditional distribution of \\(Y\\) given \\(X=x\\) for each value of \\(x\\) to produce these plots! In practice, we usually just specify these conditional distributions. So, having the joint density is actually “overkill”. 7.5.2 Direction of Dependence Two variables can be dependent in a multitude of ways, but usually there’s an overall direction of dependence: Positively related random variables tend to increase together. That is, larger values of \\(X\\) are associated with larger values of \\(Y\\). Negatively related random variables have an inverse relationship. That is, larger values of \\(X\\) are associated with smaller values of \\(Y\\). We’ve already seen some measures of dependence in the discrete setting: covariance, correlation, and Kendall’s tau. These definitions carry over. It’s a little easier to visualize the definition of covariance as the signed sum of rectangular area: Correlation, remember, is also the signed sum of rectangles, but after converting \\(X\\) and \\(Y\\) to have variances of 1. Here are two positively correlated variables, because there is overall tendency of the contour lines to point up and to the right (or down and to the left): Here are two negatively correlated variables, because there is overall tendency for the contour lines to point down and to the right (or up and to the left): Another example of negative correlation. Although contour lines aren’t pointing in any one direction, there’s more density along a line that points down and to the right (or up and to the left) than there is any other direction. Here are two random variables that are dependent, yet have 0 correlation (both Pearson’s and Kendall’s) because the overall trend is flat (pointing left or right). You can think of this in terms of slicing: slicing at \\(x = -2\\) would result in a highly peaked distribution near \\(y = 0\\), whereas slicing at \\(x = 1\\) would result in a distribution with a much wider spread – these are not densities that are multiples of each other! Prediction intervals would get wider with larger \\(x\\). Note that the marginal distributions have nothing to do with the dependence between random variables. Here are some examples of joint distributions that all have the same marginals (\\(N(0,1)\\)), but different dependence structures and strengths of dependence: 7.6 Marginal Distributions In the river flow example, we used snowmelt to inform river flow by communicating the conditional distribution of river flow given snowmelt. But, this requires knowledge of snowmelt! What if one day we are missing an observation on snowmelt? Then, the best we can do is communicate the marginal distribution of river flow. But how can we get at that distribution? Usually, aside from the data, we only have information about the conditional distributions. But this is enough to calculate the marginal distribution! 7.6.1 Marginal Distribution from Conditional We can use the law of total probability to calculate a marginal density. Recall that for discrete random variables \\(X\\) and \\(Y\\), we have \\[P(Y = y) = \\sum_x P(X = x, Y = y) = \\sum_x P(Y = y \\mid X = x) P(X = x).\\] The same thing applies in the continuous case, except probabilities become densities and sums become integrals (as usual in the continuous world): for continuous \\(X\\) and \\(Y\\), \\[f_Y(y) = \\int_x f_{X,Y}(x,y)\\ \\text{d}x = \\int_x f_{Y\\mid X}(y \\mid x)\\ f_X(x)\\ \\text{d}x.\\] Notice that this is just an average of the conditional densities! If we have the conditional densities and a sample of \\(X\\) values \\(x_1, \\ldots, x_n\\), then using the empirical approximation of the mean, we have \\[f_Y(y) \\approx \\frac{1}{n} \\sum_{i = 1}^n f_{Y\\mid X}(y \\mid x_i).\\] A similar result holds for the cdf. We have \\[F_Y(y) = \\int_x F_{Y \\mid X}(y \\mid x)\\ f_X(x) \\ \\text{d}x,\\] and empirically, \\[F_Y(y) \\approx \\frac{1}{n}\\sum_{i = 1}^n F_{Y\\mid X}(y \\mid x_i).\\] 7.6.2 Marginal Mean from Conditional Perhaps more practical is finding the marginal mean, which we can obtain using the law of total expectation (similar to the discrete case we saw in a previous lecture): \\[E(Y) = \\int_x m(x) \\ f_{X}(x) \\ \\text{d}x = E(m(X)),\\] where \\(m(x) = E(Y \\mid X = x)\\) (i.e., the model function or regression curve). When you fit a model using supervised learning, you usually end up with an estimate of \\(m(x)\\). From the above, we can calculate the marginal mean as the mean of \\(m(X)\\), which we can do empirically using a sample of \\(X\\) values \\(x_1, \\ldots, x_n\\). Using the empirical mean, we have \\[E(Y) \\approx \\frac{1}{n} \\sum_{i=1}^n m(x_i).\\] 7.6.3 Marginal Quantiles from Conditional Unfortunately, if you have the \\(p\\)-quantile of \\(Y\\) given \\(X = x\\), then there’s no convenient way of calculating the \\(p\\)-quantile of \\(Y\\) as an average. To obtain this marginal quantity, you would need to calculate \\(F_Y(y)\\) (as above), and then find the value of \\(y\\) such that \\(F_Y(y) = p\\). 7.6.4 Activity You’ve observed the following data of snowmelt and river flow: Snowmelt (mm) Flow (m^3/s) 1 140 3 150 3 155 2 159 3 170 From this, you’ve deciphered that the mean flow given snowmelt is \\[E(\\text{Flow} \\mid \\text{Snowmelt} = x) = 100 + 20x.\\] You also decipher that the conditional standard deviation is constant, and is: \\[SD(\\text{Flow} \\mid \\text{Snowmelt} = x) = 15\\ m^3/s\\] It also looks like the conditional distribution of river flow given snowmelt follows a Lognormal distribution. Part 1: A new reading of snowmelt came in, and it’s 4mm. Make a prediction of river flow. What distribution describes your current understanding of what the river flow will be? Part 2: Your snowmelt-recording device is broken, so you don’t know how much snowmelt there’s been. Make a prediction of river flow. What distribution describes your current understanding of what the river flow will be? Someone tells you that a 90% prediction interval is [70, 170]. What do we know about the median? "],["estimating-parametric-model-functions.html", "Chapter 8 Estimating parametric model functions 8.1 Writing the sample mean as an optimization problem 8.2 Evaluating Model Goodness: Quantiles 8.3 Simple Linear Regression 8.4 Linear models in general 8.5 reference-treatment parameterization 8.6 Concepts", " Chapter 8 Estimating parametric model functions Caution: in a highly developmental stage! See Section 1.1. suppressPackageStartupMessages(library(tidyverse)) Wage &lt;- ISLR::Wage NCI60 &lt;- ISLR::NCI60 baseball &lt;- Lahman::Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) ## Warning: `tbl_df()` was deprecated in dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. cow &lt;- suppressMessages(read_csv(&quot;data/milk_fat.csv&quot;)) esoph &lt;- as_tibble(esoph) %&gt;% mutate(agegp = as.character(agegp)) titanic &lt;- na.omit(titanic::titanic_train) 8.1 Writing the sample mean as an optimization problem (DSCI 561 lab2, 2018-2019) It’s important to know that the sample mean can also be calculated by finding the value that minimizes the sum of squared errors with respect to that value. We’ll explore that here. Store some numbers in the vector y. Calculate the sample mean of the data, stored in mu_y. This is not worth any marks, but having it as its own question jibes better with the autograder. We’ve defined sse() below, a function that takes some number and returns the sum of squared “errors” of all values of y with respect to the inputted number. An “error” is defined as the difference between two values. We’ve also generated a quick plot of this function for you. sse &lt;- Vectorize(function(m) sum((y - m)^2)) curve(sse, mu_y - 2*sd(y), mu_y + 2*sd(y)) Your task: use the optimize() function to find the value that minimizes the sum of squared errors. Hint: for the interval argument, specify an interval that contains the sample mean. Important points: You should recognize that the sample mean minimizes this function! You’ll be seeing the sum of squared errors a lot through the program (mean squared error and R^2 are based on it). This is because the mean is a very popular quantity to model and use as a prediction. If you’re not convinced, play with different numbers to see for yourself. 8.2 Evaluating Model Goodness: Quantiles The question here is: if we have two or more models that predicts the \\(\\tau\\)-quantile, which model is best? We’ll need some way to score different models to do things such as: Choose which predictors to include in a model; Choose optimal hyperparameters; Estimate parameters in a quantile regression model. **NOTE**: Mean Squared Error is not appropriate here!! This is very important to remember. The reason is technical – the MSE is not a proper scoring rule for quantiles. In other words, the MSE does not elicit an honest prediction. If we’re predicting the median, then the mean absolute error works. This is like the MSE, but instead of squaring the errors, we take the absolute value. In general, a “correct” scoring rule for the \\(\\tau\\)-quantile is as follows: \\[ S = \\sum_{i=1}^{n} \\rho_{\\tau}(Y_i - \\hat{Q}_i(\\tau)), \\] where \\(Y_i\\) for \\(i=1,\\ldots,n\\) is the response data, \\(\\hat{Q}_i(\\tau)\\) are the \\(\\tau\\)-quantile estimates, and \\(\\rho_{\\tau}\\) is the check function (also known as the absolute asymmetric deviation function or tick function), given by \\[ \\rho_{\\tau}(s) = (\\tau - I(s&lt;0))s \\] for real \\(s\\). This scoring rule is negatively oriented, meaning the lower the score, the better. It cannot be below 0. Here is a plot of various check functions. Notice that, when \\(\\tau=0.5\\) (corresponding to the median), this is proportional to the absolute value: base &lt;- ggplot(data.frame(x=c(-2,2)), aes(x)) + theme_bw() + labs(y=expression(rho)) + theme(axis.title.y=element_text(angle=0, vjust=0.5)) + ylim(c(0, 1.5)) rho &lt;- function(tau) function(x) (tau - (x&lt;0))*x cowplot::plot_grid( base + stat_function(fun=rho(0.2)) + ggtitle(expression(paste(tau, &quot;=0.2&quot;))), base + stat_function(fun=rho(0.5)) + ggtitle(expression(paste(tau, &quot;=0.5&quot;))), base + stat_function(fun=rho(0.8)) + ggtitle(expression(paste(tau, &quot;=0.8&quot;))), ncol=3 ) ## Warning: Removed 4 row(s) containing missing values (geom_path). ## Warning: Removed 4 row(s) containing missing values (geom_path). For quantile regression estimation, we minimize the sum of scores instead of the sum of squared residuals, as in the usual (mean) linear regression. 8.3 Simple Linear Regression (From lab2, DSCI 561, 2018-2019) When a predictor is categorical, it’s easy to estimate the mean given a certain predictor value (i.e., given the category): just take the sample average of the data in that group. Now let’s consider a numeric predictor. Using the iris dataset again with sepal width as a response, use sepal length as the predictor. Here is a scatterplot of the data: (p_numeric_x &lt;- ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point(alpha=0.25) + theme_bw() + labs(x = &quot;Sepal Length&quot;, y = &quot;Sepal Width&quot;)) How can we estimate the mean sepal width (\\(Y\\)) for any given sepal length (\\(X\\))? Say we want the mean of \\(Y\\) at \\(X=x\\) (for some pre-decided \\(x\\)). Last week in DSCI 571 Lab 2 Exercise 5, you saw one way of estimating this: calculate the mean sepal width (\\(Y\\)) using only the \\(k\\) plants having sepal lengths (\\(X\\) values) closest to \\(x\\) (the sepal length you’re interested in). Methods like this are very powerful estimation methods, but there’s merit in assuming the mean is linear in \\(x\\): \\[E(Y \\mid X=x) = \\beta_0 + \\beta_1 x,\\] for some numbers \\(\\beta_0\\) and \\(\\beta_1\\) (to be estimated). How do we estimate \\(\\beta_0\\) and \\(\\beta_1\\)? In other words, how do we pick an acceptable line? Since we want the line to represent the mean, choose the line that minimizes the sum of squared errors – remember, this is another way of writing the sample average in the univariate case, and now we can generalize the univariate mean to the regression setting in this way. Is it possible to find a line that has a smaller sum of squared errors than what you found in Exercise 3.3? Why or why not? Is it possible to find a line that has a smaller sum of absolute errors (i.e., the absolute value of the errors)? Elaborate. 8.3.1 Model Specification You might see linear regression models specified in different ways. In this exercise, we’re still working with sepal length as the only predictor of sepal width. Denote \\(\\beta_0\\) as the true intercept of the regression line, and \\(\\beta_1\\) as the true slope. As we’ve said, we’re assuming that the mean of \\(Y\\) is linear in the predictor: \\[E(Y \\mid X=x) = \\beta_0 + \\beta_1 x.\\] There are other ways to write this model; i.e., different ways of saying the same thing (not to be confused with different parameterizations). We’ll explore this here. 4.1 rubric={reasoning:3} One way to write this model is to emphasize that this model holds for every single observation, instead of for a generic \\(Y\\). Denote \\(Y_i\\) as the random variable corresponding to the \\(i\\)’th observation of the response, and \\(x_i\\) the corresponding observed value of the predictor. Let \\(n\\) be the sample size. Your task: specify what goes in the \\(?\\) in the following equation: \\[E(Y_i | X_i = x_i) = \\text{ ?}, \\text{ for each } i=1,\\ldots,n.\\] YOUR ANSWER HERE 4.2 rubric={reasoning:3} We could also specify how \\(Y_i\\) itself was supposedly calculated. Your task: specify what goes in the \\(?\\) in the following equation. \\[Y_i = \\text{ ?}, \\text{ for each } i=1,\\ldots,n.\\] Hint: you’ll have to introduce a variable. Be sure to specify any assumptions about this variable so that your equation is equivalent to the one in Exercise 4.1 – this means not putting more assumptions than are necessary, too! YOUR ANSWER HERE 4.3 rubric={reasoning:3} Instead of having to say “for each \\(i=1,\\ldots,n\\)”, we could just write out each of the \\(n\\) equations. It’s actually convenient to do so, but expressed as one equation by using matrix algebra. We’ll use bold-face to denote vectors. Denote \\(\\boldsymbol{Y}\\) as the vector containing \\(Y_1,\\ldots,Y_n\\) (in that order), and similarly for \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{x}\\). Denote \\(\\boldsymbol{\\beta}\\) as the vector containing \\(\\beta_0\\) and \\(\\beta_1\\) (in that order). Then, the same equation becomes: \\[E(\\boldsymbol{Y} \\mid \\boldsymbol{X} = \\boldsymbol{x}) = \\text{? }\\boldsymbol{\\beta}, \\] where “?” is an \\(n \\times 2\\) matrix. Your task: specify the matrix indicated by “?” in the above equation. It’s probably most convenient to describe what each column contains. Each column is worth approx. 50% of your grade for this question. YOUR ANSWER HERE 8.4 Linear models in general Caution: in a highly developmental stage! See Section 1.1. (DSCI 561 lab 2, 2018-2019) In general, linear models estimate the mean using \\(p\\) predictors \\(X_1, \\ldots, X_p\\) (this time, the subscripts denote “predictor number” instead of “observation number”, and the vectors \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{x}\\) contain the predictors, not the observations), according to the following generic equation: \\[E(Y \\mid \\boldsymbol{X}=\\boldsymbol{x}) = \\beta_0 + \\beta_1 x_1 + \\cdots + \\beta_p x_p.\\] We saw that: a \\(K\\)-level categorical predictor enters the equation through \\(K-1\\) binary predictors (relative to a “baseline” category), and a numeric predictor enters the equation as itself. We will now consider using both sepal length (numeric) and species (categorical) as predictors of sepal width. 6.1 Fit a linear regression line to sepal length (\\(X\\)) vs. sepal width (\\(Y\\)) for each species independently. Plot the results by facetting by species. Note that all we’re looking for here is the plot. You can bypass the lm() calls by adding the layer geom_smooth(method=“lm”, se=FALSE), which runs the linear regression separately in each panel. Although these look like three separate models, it’s still just one model: one specification as to how to estimate the mean. We can write a single equation that describes this specification, using the following variables: \\[X_1 = \\text{sepal length}\\]\\[X_2 = 1 \\text{ if versicolor, } 0, \\text{ otherwise}\\]\\[X_3 = 1 \\text{ if virginica, } 0, \\text{ otherwise}\\]\\[X_4 = X_2 X_1\\]\\[X_5 = X_3 X_1\\] The model becomes: \\[E(Y \\mid \\boldsymbol{X} = \\boldsymbol{x}) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + \\beta_5 X_5 \\] Your task: specify the slope and intercept of the regression lines for each species, in terms of the \\(X\\)’s and \\(\\beta\\)’s above. We’ve given you the answer for Setosa already. Answer by copying and pasting the below table into the answer cell, and filling in the missing table cells. Hint: evaluate whatever \\(X\\)’s you can. Species Intercept Slope Setosa \\(\\beta_0\\) \\(\\beta_1\\) Versicolor Virginica YOUR ANSWER HERE 6.3 \\(X_4\\) and \\(X_5\\) are called interaction terms, and are not present by default in the lm() function. In their absence, what are the slopes and intercepts of the regression line for each species? Answer like you did above, in terms of the \\(X\\)’s and \\(\\beta\\)’s, by filling in the below table. We’ve given you the answer for Setosa already. Species Intercept Slope Setosa \\(\\beta_0\\) \\(\\beta_1\\) Versicolor Virginica YOUR ANSWER HERE 6.4 Make a similar plot as in Exercise 6.1, but for the model without interaction. 8.5 reference-treatment parameterization Caution: in a highly developmental stage! See Section 1.1. (From DSCI 561 lab1, 2018-2019) When data fall into groups, you already know how to estimate the population mean for each group: calculate the sample mean of the data in each group. For example, the mean Sepal.Width of the three species in the iris dataset are: iris %&gt;% group_by(Species) %&gt;% summarize(mean_sepal_width = mean(Sepal.Width)) In this exercise, you’ll start exploring a “trick” that linear regression models use so that we can obtain the same mean estimates by evaluating a line. Although it might seem silly to do, especially when we can just do a group_by() and summarize(), using this trick will be very important when we start incorporating more variables in our model, as we’ll soon see in this course. Next, using the “trick” that linear regression models use, you’ll write these estimates as a line, so that we can obtain the mean estimates by evaluating the line. Here’s the trick: convert the categories to a numeric variable, where one category takes the value 0, and the other takes the value 1. Let’s convert “first” to 0, and “other” to 1: x_map &lt;- c(first=0, other=1) preg &lt;- mutate(preg, num_x = x_map[birth_order]) head(preg) What’s the equation of the line that goes through the mean estimates of both groups? Specify this by storing the slope and (y-) intercept of the line in the variables preg_slope and preg_int, respectively. This line is called the regression line. In Inf-1 lab3, you made a plot of the data, including the two means with a confidence interval. Here’s the code (using asymptotics to form the CI), zooming in on the “center” of the data (uncomment preg_plot to view the plot): preg_plot &lt;- preg %&gt;% group_by(birth_order, num_x) %&gt;% summarize(mean = mean(prglngth), n = length(prglngth), se = sd(prglngth) / sqrt(n)) %&gt;% ggplot(aes(x = num_x)) + geom_violin(data = preg, mapping = aes(y = prglngth, group = birth_order), adjust = 5) + geom_point(aes(y = mean), colour = &quot;red&quot;) + geom_errorbar(aes(ymin = mean + qnorm(alpha/2)*se, ymax = mean - qnorm(alpha/2)*se), colour = &quot;red&quot;, width = 0.2) + theme_bw() + labs(x = &quot;Birth Order&quot;, y = &quot;Pregnancy Length (weeks)&quot;) + ylim(c(30, 50)) + scale_x_continuous(breaks = enframe(x_map)$value, labels = enframe(x_map)$name) Add the line to this plot. Can we always draw a straight line through the means of two groups? Why or why not? In this lab, we won’t be exploring the trick that linear regression models use when we have multiple groups. But, you’ll explore what we can’t do. For each species in the iris (three-group) data set, the code below: calculates the mean sepal width in the column mean_sepwid, along with the standard error of the mean in the se column, placed in the data frame iris_est. plots the raw data with a violin+jitter plot, stored in the variable iris_plot (uncomment it to view it). (iris_est &lt;- iris %&gt;% group_by(Species) %&gt;% summarize( mean_sepwid = mean(Sepal.Width), se = sd(Sepal.Width)/sqrt(length(Sepal.Width)) )) iris_plot &lt;- iris %&gt;% mutate(Species = fct_reorder(Species, Sepal.Width)) %&gt;% ggplot(aes(Species)) + geom_violin(aes(y = Sepal.Width)) + geom_jitter(aes(y = Sepal.Width), alpha=0.2, width=0.1, size=0.5) + theme_bw() + labs(x = &quot;Species&quot;, y = &quot;Sepal Width&quot;) iris_plot Your task is to add the group means and confidence intervals to the plot. You can do this by adding layers to iris_plot. You can use asymptotic theory to calculate the confidence intervals, calculated by: \\[\\bar{x} \\pm z_{\\alpha/2} \\text{SE}.\\] Can we fit a single straight line through the mean sepal widths across the three species groups? Why or why not? 8.5.1 More than one category (Lab 2) In class, we saw two “ways to store information” about groups means – in technical terms, two parameterizations. The first and most “direct” parameterization is cell-wise parameterization, a fancy way of saying that we’re just going to consider the raw means themselves: one mean for each group. For the three species in the iris dataset, here are estimates of these parameters: iris %&gt;% group_by(Species) %&gt;% summarize(mean_sepal_width = mean(Sepal.Width)) The mean of the response (conditional on species) can be written as a linear model, if we call the above means \\(\\mu_0,\\mu_1,\\mu_2\\) (respectively), and define the following three predictors: \\[X_0 = 1 \\text{ if setosa, } 0 \\text{ otherwise},\\] \\[X_1 = 1 \\text{ if versicolor, } 0 \\text{ otherwise},\\] \\[X_2 = 1 \\text{ if virginica, } 0 \\text{ otherwise}.\\] Then, the linear model is \\[E(\\text{Sepal Width} \\mid \\text{species}) = \\mu_0 X_0 + \\mu_1 X_1 + \\mu_2 X_2.\\] In this exercise, you’ll be exploring another parameterization that’s useful in linear regression: the reference-treatment parameterization. 1.1 To keep things different from lm(), let’s consider the virginica species as our “reference”. The reference-treatment parameterization is then: \\[\\theta=\\mu_{\\text{virginica}},\\] \\[\\tau_1=\\mu_{\\text{versicolor}}-\\theta,\\] \\[\\tau_2=\\mu_{\\text{setosa}}-\\theta,\\] where \\(\\mu\\) denotes the mean of that species’ sepal width. Your task: Calculate estimates of these parameters, and store the estimates in the (respective) variables theta, tau1, and tau2. Provide an interpretation for \\(\\theta\\), \\(\\tau_1\\), and \\(\\tau_2\\). One brief sentence for each is enough. Let’s now write this information as a single (linear) equation containing: two predictors \\(X_1\\) and \\(X_2\\), and the parameters \\(\\theta\\), \\(\\tau_1\\), and \\(\\tau_2\\). Let’s focus on the predictors first. Define: \\[X_1 = 1 \\text{ if versicolor, } 0 \\text{ otherwise},\\] \\[X_2 = 1 \\text{ if setosa, } 0 \\text{ otherwise}.\\] We’ve deliberately not defined a predictor for virginica, the reference group. Your task: What are the values of \\(X_1\\) and \\(X_2\\) for each species? Store the values in three length-2 vectors called x_setosa, x_versicolor, and x_virginica. Use the predictors \\(X_1\\) and \\(X_2\\), along with the parameters \\(\\theta\\), \\(\\tau_1\\), and \\(\\tau_2\\), to write a linear equation that returns the species mean \\(\\mu\\). The equation should look like: \\[E(Y \\mid \\text{species}) = (1) + (2)\\times(3) + (4)\\times(5)\\] To use the autograder for this question, specify the order that \\(X_1\\), \\(X_2\\), \\(\\theta\\), \\(\\tau_1\\), and \\(\\tau_2\\) are to appear in the equation, respectively, in a vector containing the numbers 1 through 5, named eq_order. For example, specifying eq_order &lt;- c(1,2,3,4,5) corresponds to the equation \\(E(Y \\mid \\text{species}) = X_1 + X_2 \\theta + \\tau_1 \\tau_2\\) (which is not the correct equation) (Based on your answers to 1.4 and 1.5, can you see why the parameter that goes in place of (1) is also called the “intercept”?) Now try using lm(): use the iris data with the same predictor and response (don’t include -1 in the formula, so that you end up with a reference-treatment parameterization). Your task: What’s the reference species? Put the name of the species as a character in the variable iris_lm_ref_species. 8.6 Concepts First, let’s talk about that table from last time, but in the univariate setting. How to estimate probabilistic quantities in the univariate setting (mean, quantiles, variance, etc) Distributional Assumption? Estimation Method No “sample versions”: ybar, s^2, quantile(), … Yes Use MLE to estimate distribution; extract desired quantity. Here’s a more accurate version of the regression version of the table. How to estimate a model function in the univariate setting (specifically mean and quantile model functions) Model function assumption? Distributional Assumption? Estimation Method No No Use “sample versions” with machine learning techniques (kNN, loess, random forests, …) Yes No Minimize “loss function version” of “sample versions”: least squares, least “rho” Yes Yes MLE (example: GLM, including linear regression) No Yes Use MLE with machine learning techniques (kNN, loess, random forests, …) List of concepts from today: If there are no distributional assumption, then: the model function that minimizes the sum of squared errors (least squares) is an estimate of the conditional mean; the model function that minimizes the sum of absolute errors (least absolute errors) is an estimate of the conditional median; the model function that minimizes the sum of the “rho function” is an estimate of a specific conditional quantile. If there is a distributional assumption, then we minimize the negative log likelihood to estimate the model function. To evaluate error associated with a model function, we (1) calculate the residuals (actual response minus estimate), (2) calculate a “score” or error for each observation, then (3) calculate the average error. The “score”/error should correspond to the loss function: squared error for mean model functions; absolute error for median model functions; rho function for a generic quantile. Using the entire conditional distribution of Y|X as a prediction carries the entire picture of uncertainty about the actual outcome, as opposed to a single number like the mean or a quantile. We can obtain a probabilistic forecast (a “predictive distribution”): from a GLM by plugging in the estimated distribution parameter(s) (just the mean in the case of Bernoulli or Poisson) to get the specific distribution, and plotting the distribution. using a local method by plotting an estimate of the univariate distribution that results from the relevant subsample of y occuring near a particular x value. A loss function is more robust than squared error (least squares) if the error function does not grow as fast as a quadratic curve. The Huber loss function is one such example, which is the squared error up until some point +/-c, after which the loss function grows linearly. "],["estimating-assumption-free-the-world-of-supervised-learning-techniques.html", "Chapter 9 Estimating assumption-free: the world of supervised learning techniques 9.1 What machine learning is 9.2 Types of Supervised Learning 9.3 Local Regression 9.4 Splines and Loess Regression", " Chapter 9 Estimating assumption-free: the world of supervised learning techniques Caution: in a highly developmental stage! See Section 1.1. suppressPackageStartupMessages(library(tidyverse)) Wage &lt;- ISLR::Wage NCI60 &lt;- ISLR::NCI60 baseball &lt;- Lahman::Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) ## Warning: `tbl_df()` was deprecated in dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. cow &lt;- suppressMessages(read_csv(&quot;data/milk_fat.csv&quot;)) esoph &lt;- as_tibble(esoph) %&gt;% mutate(agegp = as.character(agegp)) titanic &lt;- na.omit(titanic::titanic_train) 9.1 What machine learning is What is Machine Learning (ML) (or Statistical Learning)? As the ISLR book puts it, it’s a “vast set of tools for understanding data”. Before we explain more, we need to consider the two main types of ML: Supervised learning. (This is the focus of BAIT 509). Consider a “black box” that accepts some input(s), and returns some type of output. Feed it a variety of input, and write down the output each time (to obtain a data set). Supervised learning attempts to learn from these data to re-construct this black box. That is, it’s a way of building a forecaster/prediction tool. You’ve already seen examples throughout MBAN. For example, consider trying to predict someone’s wage (output) based on their age (input). Using the Wage data set from the ISLR R package, here are examples of inputs and outputs: ## age wage ## 1 18 75.04315 ## 2 24 70.47602 ## 3 45 130.98218 ## 4 43 154.68529 ## 5 50 75.04315 ## 6 54 127.11574 We try to model the relationship between age and wage so that we can predict the salary of a new individual, given their age. An example supervised learning technique is linear regression, which you’ve seen before in BABS 507/508. For an age x, let’s use linear regression to make a prediction that’s quadratic in x. Here’s the fit: The blue curve represents our attempt to “re-construct” the black box by learning from the existing data. So, for a new individual aged 70, we would predict a salary of about $100,000. A 50-year-old, about $125,000. Unsupervised learning. (BAIT 509 will not focus on this). Sometimes we can’t see the output of the black box. Unsupervised learning attempts to find structure in the data without any output. For example, consider the following two gene expression measurements (actually two principal components). Are there groups that we can identify here? You’ve seen methods for doing this in BABS 507/508, such as k-means. 9.2 Types of Supervised Learning There are two main types of supervised learning methods – determined entirely by the type of response variable. Regression is supervised learning when the response is numeric. Classification is supervised learning when the response is categorical. We’ll examine both equally in this course. Note: Don’t confuse classification with clustering! The latter is an unsupervised learning method. 9.3 Local Regression Caution: in a highly developmental stage! See Section 1.1. (BAIT 509 Class Meeting 03) Let’s turn our attention to the first “new” machine learning methods of the course: \\(k\\) Nearest Neighbours (aka kNN or \\(k\\)-NN) and loess (aka “LOcal regrESSion”). The fundamental idea behind these methods is to base your prediction on what happened in similar cases in the past. 9.3.1 kNN Pick a positive integer \\(k\\). To make a prediction of the response at a particular observation of the predictors (I’ll call this the query point) – that is, when \\(X_1=x_1\\), …, \\(X_p=x_p\\): Subset your data to \\(k\\) observations (rows) whose values of the predictors \\((X_1, \\ldots, X_p)\\) are closest to \\((x_1,\\ldots,x_p)\\). For kNN classificiation, use the “most popular vote” (i.e., the modal category) of the subsetted observations. For kNN regression, use the average \\(Y\\) of the remaining subsetted observations. Recall how to calculate distance between two vectors \\((a_1, \\ldots, a_p)\\) and \\((b_1, \\ldots, b_p)\\): \\[ \\text{distance} = \\sqrt{(a_1-b_1)^2 + \\cdots + (a_p-b_p)^2}. \\] It’s even easier when there’s one predictor: it’s just the absolute value of the difference. 9.3.2 loess (This is actually the simplest version of loess, sometimes called a moving window approach. We’ll get to the “full” loess). Pick a positive number \\(r\\) (not necessarily integer). To make a prediction of the response at a query point (that is, a particular observation of the predictors, \\(X_1=x_1\\), …, \\(X_p=x_p\\)): Subset your data to those observations (rows) having values of the predictors \\((X_1,\\ldots,X_p)\\) within \\(r\\) units of \\((x_1,\\ldots,x_p)\\). For kNN classificiation, use the “most popular vote” (i.e., the modal category) of the subsetted observations. For kNN regression, use the average \\(Y\\) of the remaining subsetted observations. Notice that Step 2 is the same as in kNN. \\(k\\) and \\(r\\) are called hyperparameters, because we don’t estimate them – we choose them outright. 9.3.3 In-Class Exercises Consider the following data set, given by dat. Here’s the top six rows of data: set.seed(87) dat &lt;- tibble(x = c(rnorm(100), rnorm(100)+5)-3, y = sin(x^2/5)/x + rnorm(200)/10 + exp(1)) Here’s a scatterplot of the data: ggplot(dat, aes(x,y)) + geom_point(colour=&quot;orange&quot;) + theme_bw() + rotate_y 9.3.3.1 Exercise 1: Mean at \\(X=0\\) Let’s check your understanding of loess and kNN. Consider estimating the mean of \\(Y\\) when \\(X=0\\) by using data whose \\(X\\) values are near 0. Eyeball the above scatterplot of the data. What would you say is a reasonable estimate of the mean of \\(Y\\) at \\(X=0\\)? Why? Estimate using loess and kNN (you choose the hyperparameters). Hints for kNN: First, add a new column in the data that stores the distance between \\(X=0\\) and each observation. If that column is named d, you can do this with the following partial code: dat$d &lt;- YOUR_CALCULATION_HERE. Recall that dat$x is a vector of the x column. Then, arrange the data from smallest distance to largest with arrange(dat) (you’ll need to load the tidyverse package first), and subset that to the first \\(k\\) rows. Hints for loess: Subset the data using the filter function. The condition to filter on: you want to keep rows whose distances (d) are … k &lt;- 10 r &lt;- 0.5 x0 &lt;- 0 dat$dist &lt;- FILL_THIS_IN dat &lt;- arrange(dat, dist) # sort by distance kNN_prediction &lt;- FILL_THIS_IN loess_prediction &lt;- FILL_THIS_IN What happens when you try to pick an \\(r\\) that is way too small? Say, \\(r=0.01\\)? Why? There’s a tradeoff between choosing large and small values of either hyperparameter. What’s good and what’s bad about choosing a large value? What about small values? 9.3.3.2 Exercise 2: Regression Curve Form the regression curve / model function by doing the estimation over a grid of x values, and connecting the dots: xgrid &lt;- seq(-5, 4, length.out=1000) k &lt;- 10 r &lt;- 0.5 kNN_estimates &lt;- map_dbl(xgrid, function(x_){ dat %&gt;% mutate(d = abs(x-x_)) %&gt;% arrange(d) %&gt;% summarize(yhat=mean(y[1:k])) %&gt;% `[[`(&quot;yhat&quot;) }) loess_estimates &lt;- map_dbl(xgrid, function(x_){ dat %&gt;% mutate(d = abs(x-x_)) %&gt;% filter(d&lt;r) %&gt;% summarize(yhat=mean(y)) %&gt;% `[[`(&quot;yhat&quot;) }) est &lt;- tibble(x=xgrid, kNN=kNN_estimates, loess=loess_estimates) %&gt;% gather(key=&quot;method&quot;, value=&quot;estimate&quot;, kNN, loess) ggplot() + geom_point(data=dat, mapping=aes(x,y)) + geom_line(data=est, mapping=aes(x,estimate, group=method, colour=method), size=1) + theme_bw() Exercises: Play with different values of \\(k\\) and \\(r\\), and regenerate the plot each time. What effect does increasing these values have on the regression curve? What about decreasing? What would you say is a “good” choice of \\(k\\) and \\(r\\), and why? What happens when you choose \\(k=n=200\\)? What happens if you choose \\(r=10\\) or bigger? The phenomenon you see when \\(k\\) and \\(r\\) are very small is called overfitting. This means that your model displays patterns that are not actually present. Underfitting, on the other hand, is when your model misses patterns in the data that are actually present. 9.3.4 Hyperparameters and the bias/variance tradeoff Let’s look at the bias and variance for different values of the hyperparameter in loess. set.seed(400) N &lt;- 100 xgrid &lt;- seq(-7, 6.5, length.out=300) true_mean &lt;- Vectorize(function(x){ if (x==0) return(exp(1)) else return(sin(x^2/5)/x + exp(1)) }) ## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop expand.grid(iter=1:N, r=c(0.5, 1, 2, 4)) %&gt;% group_by(iter, r) %&gt;% do({ this_r &lt;- unique(.$r) dat &lt;- tibble(x = c(rnorm(100), rnorm(100)+5)-3, y = sin(x^2/5)/x + rnorm(200)/10 + exp(1)) data.frame( ., x = xgrid, yhat = ksmooth(dat$x, dat$y, kernel=&quot;box&quot;, bandwidth=this_r, x.points=xgrid)$y ) }) %&gt;% ungroup() %&gt;% mutate(r=paste0(&quot;bandwidth=&quot;, r)) %&gt;% ggplot(aes(x=x, y=yhat)) + facet_wrap(~ r, ncol=1) + geom_line(aes(group=iter, colour=&quot;Estimates&quot;), alpha=0.1) + stat_function(fun=true_mean, mapping=aes(colour=&quot;True mean&quot;)) + theme_bw() + scale_colour_brewer(&quot;&quot;, palette=&quot;Dark2&quot;) + ylab(&quot;y&quot;) + rotate_y ## Warning: Removed 3240 row(s) containing missing values (geom_path). You can see the bias/variance tradeoff here: Notice that the estimates get narrower as the bandwidth increases – this means the variance reduces as the bandwidth increases. Notice that the estimates become biased as the bandwidth increases. A similar phenomenon exists with kNN regression. Notice some other things about these plots: There’s more variance whenever there’s less data – that’s at the tails, and (by design) at around \\(X=0\\). Estimates don’t exist sometimes, if no data fall in the “window”. You can see that the tails are cut short when the bandwidth is small. 9.3.5 Extensions to kNN and loess 9.3.5.1 Kernel weighting kNN and loess can be generalized by downweighing points that are further from the query point. In particular, we take a weighted average. Suppose \\(y_1, \\ldots, y_n\\) are \\(n\\) realizations of the response \\(Y\\). If we assign (respective) weights \\(w_1, \\ldots, w_n\\) to these realizations, then the weighted average is \\[ \\frac{\\sum_{i=1}^n w_i y_i}{\\sum_{i=1}^n w_i}. \\] If the response is categorical, then each subsetted observation gives a “weighted vote”. Sum up the weights corresponding to each category to obtain the total “number” of votes. We obtain these weights using a kernel function. A kernel function is any non-increasing function over the positive real numbers. Plug in the distance between the observed predictor(s) and the query point to obtain the weight. Some examples of kernel functions are plotted below. 9.3.5.2 Local polynomials Another extension of loess is to consider local polynomials. The idea here is, after subsetting the data lying within \\(r\\) units of the query point, add the following two steps: Fit a linear (or quadratic) regression model to the subsetted data only. This is the “local polynomial”. You can think of this as like a “mini linear regression”. Obtain your prediction by evaluating the regression curve at the query point. Throw away your fitted local polynomial. OK, the 3rd step isn’t really a true step, but I like to include it to emphasize that we only evaluate the local polynomial at the query point. Note: We could fit higher order polynomials, but that tends to overfit the data. We could fit any other curve locally besides a polynomial, but polynomials are justified by the Taylor approximation. Local polynomials with degree=0 is the same as “not doing” local polynomials. 9.3.5.3 Combination You can combine kernel weighting with local polynomials. When you fit the local polynomial to the subsetted data, you can run a weighted regression. Instead of minimizing the sum of squared errors, we minimize the weighted sum of squared errors. 9.3.5.4 Other distances We don’t have to use the “usual” notion of distance. The formula I gave you earlier (above) is called the Euclidean distance, or L2 norms. There’s also the L1 norm (also called the manhattan distance, which is distance by moving along the axes/rectangularly) and L0 norm (number of predictors having non-zero univariate distance). 9.3.5.5 Scaling When you’re using two or more predictors, your predictors might be on different scales. This means distances aren’t weighed equally, depending on the direction. Instead of measuring distance on the original scale of the predictors, consider re-scaling the predictors by subtracting the mean and dividing by the standard deviation for each predictor. 9.3.5.6 Demonstration Let’s look at the same example, but with kernel downweighting and local polynomials. Warning! The “bandwidth” hyperparameter in this plot is parameterized differently than in the previous plot, but carries the same interpretation. set.seed(400) N &lt;- 100 xgrid &lt;- seq(-7, 6.5, length.out=300) true_mean &lt;- Vectorize(function(x){ if (x==0) return(exp(1)) else return(sin(x^2/5)/x + exp(1)) }) ## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop expand.grid(iter=1:N, r=c(0.1, 0.5, 0.7), d=0:2) %&gt;% group_by(iter, r, d) %&gt;% do({ this_r &lt;- unique(.$r) this_d &lt;- unique(.$d) dat &lt;- tibble(x = c(rnorm(100), rnorm(100)+5)-3, y = sin(x^2/5)/x + rnorm(200)/10 + exp(1)) data.frame( ., x = xgrid, yhat = predict(loess(y~x, data=dat, span=this_r, degree=this_d), newdata=data.frame(x=xgrid)) ) }) %&gt;% ungroup() %&gt;% mutate(r=paste0(&quot;bandwidth=&quot;, r), d=paste0(&quot;degree=&quot;, d)) %&gt;% ggplot(aes(x=x, y=yhat)) + facet_grid(r ~ d) + geom_line(aes(group=iter, colour=&quot;Estimates&quot;), alpha=0.1) + stat_function(fun=true_mean, mapping=aes(colour=&quot;True mean&quot;)) + theme_bw() + scale_colour_brewer(&quot;&quot;, palette=&quot;Dark2&quot;) + ylab(&quot;y&quot;) + rotate_y ## Warning: Removed 8034 row(s) containing missing values (geom_path). Notice: For small bandwidth, increasing the degree of the poynomial just results in more variance – degree=0 looks best for this bandwidth. But by increasing the degree (inc. variance, dec. bias) and increasing the bandwidth (dec. variance, inc. bias), we end up getting an overall better fit: low bias, low variance. Bandwidth=0.5 and Degree=2 here seem to work best. 9.3.6 Model assumptions and the bias/variance tradeoff Recall that we saw an incorrect model assumption leads to bias, such as fitting linear regression when the true mean is non-linear. When you make model assumptions that are close to the truth, then this has the effect of decreasing variance. Adding good model assumptions is like adding more data – after all data is information, and a model assumption is also information. Here’s a demonstration: Consider \\[ Y = X + \\varepsilon, \\] where \\(X\\) (predictor) is N(0,1), and \\(\\varepsilon\\) (error term) is also N(0,1) (both are independent). I’ll generate a sample of size 100, 100 times. For each sample, I’ll fit a linear regression model and a loess model. Here are the resulting 100 regression curves for each (the dashed line is the true mean): set.seed(474) n &lt;- 100 N &lt;- 100 xgrid &lt;- data.frame(x=seq(-4,4, length.out=100)) ## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop tibble(iter=1:N) %&gt;% group_by(iter) %&gt;% do({ dat &lt;- tibble(x=rnorm(n), y=x+rnorm(n)) data.frame( ., xgrid, Local = predict(loess(y~x, data=dat), newdata=xgrid), Linear = predict(lm(y~x, data=dat), newdata=xgrid) ) }) %&gt;% gather(key=&quot;method&quot;, value=&quot;Prediction&quot;, Local, Linear) %&gt;% ggplot(aes(x=x, y=Prediction)) + facet_wrap(~ method) + geom_line(aes(group=iter), colour=&quot;orange&quot;, alpha=0.1) + geom_abline(intercept=0, slope=1, linetype=&quot;dashed&quot;) + theme_bw() ## Warning: Removed 1869 row(s) containing missing values (geom_path). Notice that the local method has higher variance than the linear regression method. 9.4 Splines and Loess Regression Caution: in a highly developmental stage! See Section 1.1. This tutorial describes spline and loess regression in R. You can think of splines as regression between knots. We’ll use the splines package to do this. Here’s some generated data: x &lt;- rnorm(1000) y &lt;- x^2 + rnorm(1000) qplot(x, y, alpha=I(0.5)) + theme_bw() First, we need to “set up” the regression by placing knots. x2 &lt;- splines::ns(x, knots=c(-2, 0, 2)) Now we can do regression between these knots, with the natural spline shape (as opposed to linear): fit &lt;- lm(y ~ x2) qplot(x, y, alpha=I(0.5)) + geom_line(data=data.frame(x=x, y=predict(fit)), mapping=aes(x, y), colour=&quot;blue&quot;) + theme_bw() 9.4.1 Loess Loess is “local regression”, and is based on the idea of estimating the mean response based on similar observed data in the predictor space. Kernel smoothing is a method that estimates the mean using a weighted sample average, where observations that are further away in the predictor space get downweighted more, according to some kernel function. Local polynomials is a method that takes kernel smoothing one step further: instead of a weighted sample average, we use nearby data (in the predictor space) to fit a polynomial regression, and then fit a polynomial regression model. There is a more basic version, too: a “moving window”, described next, before seeing how loess is done in R. 9.4.1.1 The “Moving Window” The “moving window” approach is a special type of kernel smoother. For a given value of the predictor \\(X=x\\), the mean response is estimated as the sample average of all response values whose predictor values are “near” \\(x\\) – within some distance \\(h\\). For example, if you want to estimate the mean number of “runs” of a baseball team when walks=100 and hits=1000, only look at cases where “walks” is approximately 100, “hits” is approximately 1000, and then average the response. It’s a special case of kernel regression, with a kernel function of \\[ k\\left(t\\right) = I\\left(|t-x| &lt; h \\right), \\] sometimes called the “boxcar” function. Note the similarity to kNN! kNN regression uses the nearest \\(k\\) points, resulting in a variable distance \\(h\\), whereas the moving window regression uses a fixed distance \\(h\\), resulting in a variable number of points \\(k\\) used. 9.4.1.2 ggplot2 ggplot2 comes with a fairly powerful tool for plotting a smoother, with geom_smooth. qplot(x, y) + geom_smooth(method=&quot;loess&quot;) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; You can choose the bandwidth through the span argument: qplot(x, y) + geom_smooth(method=&quot;loess&quot;, span=0.1) + theme_bw() ## `geom_smooth()` using formula &#39;y ~ x&#39; Too wiggly. The default looks fine. Note that geom_smooth can fit lm and glm fits too: qplot(x, y) + geom_smooth(method=&quot;lm&quot;, formula=y~x+I(x^2)) + theme_bw() 9.4.1.3 Manual method You can use the loess or ksmooth function in R to do kernel smoothing yourself. You get more flexibility here, since you can get things such as predictions. It works just like the other regression functions: (fit &lt;- loess(y ~ x)) ## Call: ## loess(formula = y ~ x) ## ## Number of Observations: 1000 ## Equivalent Number of Parameters: 5.18 ## Residual Standard Error: 1.037 We can make predictions as usual: yhat &lt;- predict(fit) qplot(x, y) + geom_line(data=data.frame(x=x, y=yhat), mapping=aes(x, y), colour=&quot;blue&quot;) + theme_bw() If you want the standard errors of the predictions, you can indicate this with se=TRUE in the predict function. Some key things that you might want to change in your kernel smoothing regression, through arguments in the loess function: Bandwidth/smoothing parameter. Indicate through the span argument. Degree of the local polynomial fitted. Indicate through the degree argument. Kernel function. The kernel function is not readily specified in loess. But you can use the ksmooth, where you’re allowed to specify a “box” or “normal” kernel. "],["special-cases.html", "Special cases", " Special cases "],["regression-when-data-are-censored-survival-analysis.html", "Chapter 10 Regression when data are censored: survival analysis 10.1 Data 10.2 Univariate Estimation 10.3 Regression with Survival Data 10.4 Concept list", " Chapter 10 Regression when data are censored: survival analysis suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(survival)) suppressPackageStartupMessages(library(ggfortify)) suppressPackageStartupMessages(library(broom)) In this case study, we will use the ovarian data from the survival R package to investigate survival times (futime) for ovarian cancer patients. Specifically, we will focus on both regression tasks: prediction, given treatment (rx) interpreting/inferring the effect of treatment (rx) To get started, ensure you have the requisite R packages installed. You probably already have broom and tidyverse installed, but not these: install.packages(&quot;survival&quot;) install.packages(&quot;ggfortify&quot;) 10.1 Data Here is a snippet of the relevant data. Notice a defining feature of this dataset: not all survival times are complete! Censored observation: a number for which we know the actual outcome is larger. An “incomplete” observation. Also called survival data. (Naturally, we can extend the notion of censoring to “smaller”, and to other notions, but we won’t consider this here) ovarian &lt;- survival::ovarian %&gt;% as_tibble() %&gt;% select(futime, fustat, rx) %&gt;% mutate(rx = factor(rx), fustat = fustat) head(ovarian) ## # A tibble: 6 × 3 ## futime fustat rx ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 59 1 1 ## 2 115 1 1 ## 3 156 1 1 ## 4 421 0 2 ## 5 431 1 1 ## 6 448 0 1 With R, we can indicate a response is censored using the survival::Surv() function. This forms the foundation to survival analysis in R. Surv() takes the survival times and the censoring indicator Surv(ovarian$futime, event = (ovarian$fustat)) ## [1] 59 115 156 421+ 431 448+ 464 475 477+ 563 638 744+ ## [13] 769+ 770+ 803+ 855+ 1040+ 1106+ 1129+ 1206+ 1227+ 268 329 353 ## [25] 365 377+ Here is a plot of the data: ovarian %&gt;% mutate(censor = if_else(fustat == 1, &quot;Died&quot;, &quot;Ongoing&quot;)) %&gt;% ggplot(aes(rx, futime)) + geom_jitter(aes(colour = censor), width = 0.2) + theme_bw() + labs(x = &quot;Treatment&quot;, y = &quot;Survival time&quot;) + scale_colour_discrete(&quot;&quot;) Other data examples: Churn. You want to keep subscribers to your YouTube channel. Google has given you the subscription date and drop-out date (if applicable) for all subscribers you’ve ever had. Light Bulbs. You’ve designed a new light bulb, and wish to tell buyers how long they will last. 10.2 Univariate Estimation No matter whether we’re interested in prediction or interpretation, we first need to be able to estimate things from univariate data. But how can we even estimate something as simple as the mean? What are the consequences for the following two approaches? Ignore the censoring: mean(ovarian$futime) ## [1] 599.5385 Remove censored data: ovarian %&gt;% filter(fustat == 0) %&gt;% .[[&quot;futime&quot;]] %&gt;% mean() ## [1] 812.2857 10.2.1 Non-parametric Estimates with Kaplan-Meier Survival Function Question: In what ways can we define a distribution? Obtain the Kaplan-Meier estimate of the survival function using survival::survfit(). It’s expecting a formula, but we’re still working with the null model. (fit_km &lt;- survfit( Surv(futime, fustat) ~ 1, data = ovarian )) ## Call: survfit(formula = Surv(futime, fustat) ~ 1, data = ovarian) ## ## n events median 0.95LCL 0.95UCL ## 26 12 638 464 NA To obtain the survival function, we have two options. Obtain the heights of each “step” of the survival function, and their corresponding time-values (y). Use summary(), or better, broom::tidy(): tidy(fit_km) ## # A tibble: 26 × 8 ## time n.risk n.event n.censor estimate std.error conf.high conf.low ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 59 26 1 0 0.962 0.0392 1 0.890 ## 2 115 25 1 0 0.923 0.0566 1 0.826 ## 3 156 24 1 0 0.885 0.0708 1 0.770 ## 4 268 23 1 0 0.846 0.0836 0.997 0.718 ## 5 329 22 1 0 0.808 0.0957 0.974 0.670 ## 6 353 21 1 0 0.769 0.107 0.949 0.623 ## 7 365 20 1 0 0.731 0.119 0.923 0.579 ## 8 377 19 0 1 0.731 0.119 0.923 0.579 ## 9 421 18 0 1 0.731 0.119 0.923 0.579 ## 10 431 17 1 0 0.688 0.134 0.894 0.529 ## # … with 16 more rows Just want the plot? ggfortify::autoplot() to the rescue! You can even add other layers to the resulting ggplot2 object. Notice the “notches” wherever there’s a censored observation. autoplot(fit_km) + theme_bw() + ylab(&quot;Survival Function&quot;) + ylim(0, 1) ## Scale for &#39;y&#39; is already present. Adding another scale for &#39;y&#39;, which will ## replace the existing scale. Quantiles You can find the median on the printout of survfit above. But in general, we can use the survival::quantile() S3 generic function. quantile(fit_km, probs = c(0.25, 0.5), conf.int = FALSE) ## 25 50 ## 365 638 What quantiles do not exist? Mean Does the mean exist? Obtain the restricted mean using print.survfit(), or better, broom::glance(). Where is the restriction? glance(fit_km) ## # A tibble: 1 × 10 ## records n.max n.start events rmean rmean.std.error median conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26 26 26 12 794. 91.5 638 464 NA ## # … with 1 more variable: nobs &lt;int&gt; 10.2.2 Parametric Estimation The Weibull family of distributions is a flexible and popular distributional assumption to make. Fit using survival::survreg(), just like survival::survfit(), but with dist=\"weibull\". (fit_wei &lt;- survreg( Surv(futime, fustat) ~ 1, data = ovarian, dist = &quot;weibull&quot; )) ## Call: ## survreg(formula = Surv(futime, fustat) ~ 1, data = ovarian, dist = &quot;weibull&quot;) ## ## Coefficients: ## (Intercept) ## 7.111038 ## ## Scale= 0.9024784 ## ## Loglik(model)= -98 Loglik(intercept only)= -98 ## n= 26 Get at the model parameters using summary(), or better, using broom::tidy(): tidy(fit_wei) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 7.11 0.293 24.3 2.36e-130 ## 2 Log(scale) -0.103 0.254 -0.405 6.86e- 1 Lab assignment: calculate Weibull parameters using these estimates; then calculate estimates of probabilistic quantities. Hint: see the bottom of the ?survreg documentation. 10.3 Regression with Survival Data The tricky part here? When there’s a parametric assumption on the “model function” – or perhaps it’s more accurate here to say across the predictor space. Question: How can we set up a regression model with a linear predictor? Feel free to use a link function to solve the restricted range problem. 10.3.1 Proportional Hazards Model We can add “hazard function” to our list of things that can define a distribution. Why is this useful to model? Fit a proportional hazards model with survival::coxph(). Our predictor is treatment (rx). (fit_ph &lt;- coxph( Surv(futime, fustat) ~ rx, data = ovarian )) ## Call: ## coxph(formula = Surv(futime, fustat) ~ rx, data = ovarian) ## ## coef exp(coef) se(coef) z p ## rx2 -0.5964 0.5508 0.5870 -1.016 0.31 ## ## Likelihood ratio test=1.05 on 1 df, p=0.3052 ## n= 26, number of events= 12 Question: Linear regression over two categories is the same as estimating the mean for both categories. Is the Proportional Hazards regression model also estimating the hazard function separately for each category? We can already see Treatment 2 is not significant (under a 0.05 level). If it was significant, we’d interpet the hazard function under Treatment 2 to be exp(-0.5964) = 0.55 as much as the hazard function under Treatment 1. 10.3.2 Prediction Your turn: Under the Proportional Hazards model, plot the survival function and obtain a mean estimate for Treatment 1 by filling in the following steps. Convert the fitted model to a survfit object (which you can think of as a specific distribution). Be sure to specify the newdata argument like you would when using predict(). (fit_ph_survfit &lt;- survfit( fit_ph, newdata = data.frame(rx = 1)#ovarian[1, ] )) ## Warning in model.frame.default(data = structure(list(rx = 1), class = ## &quot;data.frame&quot;, row.names = c(NA, : variable &#39;rx&#39; is not a factor ## Call: survfit(formula = fit_ph, newdata = data.frame(rx = 1)) ## ## n events median 0.95LCL 0.95UCL ## 26 12 NA 475 NA Plot the survival function that’s stored in this new variable. Hint: there’s a handy function for that. fit_ph_survfit %&gt;% autoplot() Obtain a mean estimate using a function from the broom package. fit_ph_survfit %&gt;% glance() ## # A tibble: 1 × 10 ## records n.max n.start events rmean rmean.std.error median conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 26 26 26 12 892. 108. NA 475 NA ## # … with 1 more variable: nobs &lt;int&gt; Obtain a median estimate using the quantile function. fit_ph_survfit %&gt;% quantile(probs = 0.5) ## $quantile ## 50 ## NA ## ## $lower ## 50 ## 475 ## ## $upper ## 50 ## NA 10.4 Concept list A measurement is censored if we only know that its true value lies above some point. For ease of discussion, we call the random variable of interest “time until event”. There are other types of censoring, but they are a simple extension of this definition. Removing censored data will result in uncertainty in our estimates to be larger than it could be, if we were to include the censored data. Removing censored data could also result in biased estimates, if data have only been collected for a short duration. Ignoring the censorship status (i.e., taking a censored observations to be the actual observations) will likely result in a biased (overly small) estimate. There are many ways a distribution can be depicted. Aside from the density/pmf and cdf, there’s also: The survival function (= 1 - cdf) at t evaluates to the probability that the outcome exceeds t. The hazard function (= density / survival function) at t can be interpreted as the instantaneous “chance” of the event occuring, given that the event has not occured up until time t. Options for estimating quantities by incorporating the partial information contained in censored observations: Survival function: if no distributional assumption is made, the Kaplan-Meier method can be used to estimate the survival function. Mean: can be estimated as the area under an estimate of the survival function. Quantiles: can be estimated by inverting an estimate of the survival function. If a distributional assumption is made, the partial likelihood can be used to fit the distribution, and any quantity can be extracted from that distribution (not necessarily through the survival function). The Kaplan-Meier estimate of the survival function does not always drop to zero (when the largest observation is censored), in which case estimates of some high quantiles and the mean would not exist. A common “fix” is to force the survival function to drop to zero at the largest observation. The mean estimate that results is called the restricted mean. The Cox proportional hazards model is a commonly used model that allows us to interpret how predictors influence a censored response. It models an individual’s hazard function as some baseline hazard, multiplied by exp(eta), where eta is a linear combination of predictors. The coefficient beta on a predictor X (contained in eta) has the following interpretation: an increase in X by one unit is associated with an increase in hazard (at any time) by exp(beta) times (i.e., the effect is multiplicative). This assumes that any two hazard functions on the predictor space are the same, up to some multiplicative constant. The hazard is useful to model due to its flexibility and interpretability. We can obtain a prediction at X=x from a proportional hazards model by converting the estimated hazard function evaluated at x to a survival function, and obtaining a univariate estimate as described above. "],["regression-when-data-are-ordinal.html", "Chapter 11 Regression when data are ordinal 11.1 Concept list", " Chapter 11 Regression when data are ordinal 11.1 Concept list A variable is ordinal if its values have a natural ordering. For example, months have an inherent order. A proportional odds model is a commonly used model that allows us to interpret how predictors influence an ordinal response. Let’s consider lower levels as being “worse”. It models an individual’s odds of having an outcome “worse than” (less than or equal to) level k for all k as being some baseline odds, multiplied by exp(eta), where eta is a linear combination of the predictors. Sometimes (like in R’s MASS::polr()) eta is a negative linear combination of predictors, so that the multiplicative factor is exp(-eta). The coefficient beta on a predictor X (contained in eta) has the following interpretation (if eta is defined as a linear combination of predictors without a negative sign in front): an increase in X by one unit is associated with exp(beta) times the odds of being worse off. If eta is defined with a negative sign, the same interpretation follows with exp(-beta) instead of exp(beta). "],["regression-when-data-are-missing-multiple-imputation.html", "Chapter 12 Regression when data are missing: multiple imputation 12.1 Mean Imputation 12.2 Multiple Imputation 12.3 Step 0: What data are missing? 12.4 Step 1: Handling Missing Data 12.5 Step 3: Pool results 12.6 Concepts", " Chapter 12 Regression when data are missing: multiple imputation Caution: in a highly developmental stage! See Section 1.1. (DSCI 562 Tutorial) suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(mice)) Let’s take a closer look at mean imputation vs. multiple imputation. 12.1 Mean Imputation Let’s consider a simple linear regression example, with one explanatory variable. We’ll generate 100 data points, and make 10 of the response values missing. set.seed(13) x &lt;- rnorm(100) y &lt;- -1 + 2 * x + rnorm(100) y[1:10] &lt;- NA Here are the data: x ## [1] 0.55432694 -0.28027194 1.77516337 0.18732012 1.14252615 0.41552613 ## [7] 1.22950656 0.23667967 -0.36538277 1.10514427 -1.09359397 0.46187091 ## [13] -1.36098453 -1.85602715 -0.43985541 -0.19394690 1.39643151 0.10066325 ## [19] -0.11443881 0.70222523 0.26254267 1.83616330 0.35740242 -1.04541013 ## [25] 0.62018413 0.14935453 -1.45931685 -2.02704380 -1.05695776 -0.72814372 ## [31] -0.00821067 0.84779738 -0.38349150 -0.52651151 -0.27322596 -0.60574161 ## [37] -0.33286731 -0.24153755 -0.86277540 -0.84697075 0.10034035 1.59003353 ## [43] 0.56649488 1.61447949 -0.46865016 -0.72610140 -1.02333900 -1.93781553 ## [49] 0.27714729 1.40835367 0.27312919 0.75552507 -0.34901841 -0.54619076 ## [55] 0.23436199 -0.29782822 -0.84047613 0.82651036 1.48369123 0.69967564 ## [61] -1.26157415 0.29827174 -0.14780711 -0.88892233 1.01306586 -0.92052508 ## [67] -0.57389450 1.15036548 1.14382456 -0.23944276 -1.08680215 -0.06144699 ## [73] -0.51669734 -1.90767369 0.10715648 -1.17737519 1.74542691 -0.39869853 ## [79] 0.44243942 0.45027946 -0.07606216 0.29751322 -1.19435471 -1.99687548 ## [85] 1.38851305 -0.08248357 0.39251449 -1.08276971 1.60212039 1.00406897 ## [91] 0.37989570 -0.56550536 -1.21377810 -1.36430159 -1.41613295 -0.25557803 ## [97] -1.22542595 0.21383426 0.06722356 0.85663511 y ## [1] NA NA NA NA NA ## [6] NA NA NA NA NA ## [11] -4.053704654 1.559887706 -4.743071625 -5.523818780 -1.930473677 ## [16] -1.500485206 2.645513317 -0.051326812 -0.839409352 0.218430884 ## [21] 0.850566209 3.126514212 -0.368543711 -3.162228622 -1.105128040 ## [26] -0.902167648 -4.154972172 -5.111932100 -4.145855361 -3.322836591 ## [31] -0.168810384 0.819016643 -1.365461091 -1.130697319 0.240141538 ## [36] -2.390779077 -2.058218000 -0.761012345 -1.767026541 -3.989408584 ## [41] -0.459957593 1.032304370 -0.957928133 0.235330176 -1.507042163 ## [46] -0.745023600 -4.628929629 -5.232634575 -2.144358973 0.664736992 ## [51] -3.298505466 2.225627728 -2.926853644 -2.406454479 -0.601188334 ## [56] -2.433257875 -2.434091710 -1.922107514 1.971684397 2.745313865 ## [61] -3.459211491 0.705772270 -1.719688562 -2.123423177 1.619808115 ## [66] -3.826059870 -1.864032144 1.330204157 2.087342480 -2.782158129 ## [71] -0.403227138 -1.468562228 -1.026899366 -2.000743246 0.042009453 ## [76] -0.473418979 2.841051621 -2.703355211 0.700489075 -0.592450346 ## [81] -1.683073694 0.229914942 -5.462264420 -5.187776409 1.969480413 ## [86] -2.450285233 -0.005289845 -3.465216049 1.160366954 1.827202816 ## [91] 0.761346358 -1.696197511 -2.962489498 -4.827255075 -3.221340010 ## [96] -1.494866659 -2.714534509 0.673879626 -1.598181064 0.641959448 Here’s the scatterplot with the missing data removed, and the corresponding linear regression fit: p &lt;- qplot(x, y) + geom_smooth(method=&quot;lm&quot;, se=FALSE) p ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 10 rows containing non-finite values (stat_smooth). ## Warning: Removed 10 rows containing missing values (geom_point). The mean imputation method replaces the NA’s with an estimate for the mean of \\(Y\\). The simplest case is to use the sample average of the response. The imputed observations are shown in red, and the resulting lm fit is also in red. ybar &lt;- mean(y, na.rm=TRUE) datrm &lt;- na.omit(data.frame(x=x, y=y)) datimp &lt;- data.frame(x=x[1:10], y=ybar) p + geom_point(data=datimp, colour=&quot;red&quot;) + geom_smooth(data=rbind(datrm, datimp), method=&quot;lm&quot;, se=FALSE, colour=&quot;red&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 10 rows containing non-finite values (stat_smooth). ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 10 rows containing missing values (geom_point). Notice that the new regression line is flatter. Another mean-imputation method is to replace the NA’s with an alternative mean estimate: the regression predictions. fit2 &lt;- lm(y ~ x, na.action=na.omit) yhat &lt;- predict(fit2, newdata=data.frame(x=x[1:10])) datimp2 &lt;- data.frame(x=x[1:10], y=yhat) p + geom_point(data=datimp2, colour=&quot;red&quot;) + geom_smooth(data=rbind(datrm, datimp2), method=&quot;lm&quot;, se=FALSE, colour=&quot;red&quot;, size=0.5) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 10 rows containing non-finite values (stat_smooth). ## `geom_smooth()` using formula &#39;y ~ x&#39; ## Warning: Removed 10 rows containing missing values (geom_point). The regression line has not changed. This method seems smarter, but it still has consequences, since the imputed data suggests that the dataset is bound closer to the regression line than reality. So the residual variance is biased to be smaller. These are both mean imputation methods. So, in your Lab 2 assignment, you can use any mean imputation method – your explanation of the comparison will just depend on what you choose. 12.2 Multiple Imputation Recall that multiple imputation is a technique for handling missing data. It replaces the missing data with many plausible values, to obtain mutliple data sets. An analysis is done on each data set, and the results are combined. A very powerful R package to assist with multiple imputation is the mice package. Some key things that it does: Displays patterns in missing data. Imputes data to obtain multiple data sets. Pools multiple analyses into one. We’ll look at the airquality dataset in R. head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 12.2.1 Patterns Where are the NAs? md.pattern(airquality) ## Wind Temp Month Day Solar.R Ozone ## 111 1 1 1 1 1 1 0 ## 35 1 1 1 1 1 0 1 ## 5 1 1 1 1 0 1 1 ## 2 1 1 1 1 0 0 2 ## 0 0 0 0 7 37 44 A “1” indicates that an observation is present, and a “0” indicates absense. The periphery of the matrix are counts: to the right, are the number of NAs in the row; at the bottom, are the number of NAs in each column; to the left, are the number of observations having a missing data pattern indicated in the matrix. So we can see that there are 7 missing Solar Radiation observations, and 37 missing Ozone observations. We could check that in another way as follows: sum(is.na(airquality$Solar.R)) ## [1] 7 sum(is.na(airquality$Ozone)) ## [1] 37 12.2.2 Multiple Imputation There are many methods of doing an imputation. But generally, they use other columns in the data set to do prediction on the missing data. The function to do this is mice. Let’s impute 50 data sets using the “Predictive Mean Matching” method. (dats &lt;- mice(airquality, m=50, method=&quot;pmm&quot;, seed=123, printFlag=FALSE)) ## Class: mids ## Number of multiple imputations: 50 ## Imputation methods: ## Ozone Solar.R Wind Temp Month Day ## &quot;pmm&quot; &quot;pmm&quot; &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## Ozone Solar.R Wind Temp Month Day ## Ozone 0 1 1 1 1 1 ## Solar.R 1 0 1 1 1 1 ## Wind 1 1 0 1 1 1 ## Temp 1 1 1 0 1 1 ## Month 1 1 1 1 0 1 ## Day 1 1 1 1 1 0 The m argument is the number of imputed datasets. method is the method (you can check out the other methods in the “Details” part of the documentation of mice). Because there’s a random component to the imputation, seed indicates the seed to initiate the random number generator – useful for reproducibility! Finally, I didn’t want mice to be verbose with its output, so I silenced it with printFlag=FALSE. dats isn’t just a list of 50 datasets. It has more information bundled in it. The info is bundled in an object of type “mids”: class(dats) ## [1] &quot;mids&quot; But we can extract the data sets. Want to see the fourth imputed data set? Here it is: head(mice::complete(dats, 4)) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 6 320 14.3 56 5 5 ## 6 28 276 14.9 66 5 6 12.2.3 Pooling The mice package allows you to pool many types of regression analyses. Let’s try a simple linear regression to predict Ozone from Solar.R, Wind, and Temp. You’ll need to use base R’s with function. fits &lt;- with(dats, lm(Ozone ~ Solar.R + Wind + Temp)) If you were to print fits to the screen, it would look like a list of 50 regression fits – one for each of the imputed data sets. But it’s not. Take a look: names(fits) ## [1] &quot;call&quot; &quot;call1&quot; &quot;nmis&quot; &quot;analyses&quot; Like dats, fits has more info in it. But it does have the 50 regression fits. And they can be pooled using the pool function: (fit &lt;- pool(fits)) ## Class: mipo m = 50 ## term m estimate ubar b t dfcom ## 1 (Intercept) 50 -61.74901609 3.860658e+02 1.525150e+02 5.416310e+02 149 ## 2 Solar.R 50 0.05819172 4.158538e-04 1.053628e-04 5.233239e-04 149 ## 3 Wind 50 -3.13670807 3.187991e-01 1.513848e-01 4.732116e-01 149 ## 4 Temp 50 1.59560738 4.740713e-02 1.286813e-02 6.053263e-02 149 ## df riv lambda fmi ## 1 89.08804 0.4029502 0.2872163 0.3026968 ## 2 106.16681 0.2584324 0.2053606 0.2199188 ## 3 81.51324 0.4843567 0.3263075 0.3422504 ## 4 103.69822 0.2768675 0.2168334 0.2315134 summary(fit) ## term estimate std.error statistic df p.value ## 1 (Intercept) -61.74901609 23.27296769 -2.653251 89.08804 9.439329e-03 ## 2 Solar.R 0.05819172 0.02287627 2.543759 106.16681 1.240661e-02 ## 3 Wind -3.13670807 0.68790380 -4.559806 81.51324 1.782885e-05 ## 4 Temp 1.59560738 0.24603380 6.485318 103.69822 3.068203e-09 And there are the results of the pooled fit. This pooling works for more than just lm! suppressPackageStartupMessages(library(tidyverse)) suppressPackageStartupMessages(library(broom)) suppressPackageStartupMessages(library(mice)) Consider predicting the air quality (ozone levels) in New York mid-year. We’ll use the airquality dataset, recorded for mid 1973: airquality ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 ## 9 8 19 20.1 61 5 9 ## 10 NA 194 8.6 69 5 10 ## 11 7 NA 6.9 74 5 11 ## 12 16 256 9.7 69 5 12 ## 13 11 290 9.2 66 5 13 ## 14 14 274 10.9 68 5 14 ## 15 18 65 13.2 58 5 15 ## 16 14 334 11.5 64 5 16 ## 17 34 307 12.0 66 5 17 ## 18 6 78 18.4 57 5 18 ## 19 30 322 11.5 68 5 19 ## 20 11 44 9.7 62 5 20 ## 21 1 8 9.7 59 5 21 ## 22 11 320 16.6 73 5 22 ## 23 4 25 9.7 61 5 23 ## 24 32 92 12.0 61 5 24 ## 25 NA 66 16.6 57 5 25 ## 26 NA 266 14.9 58 5 26 ## 27 NA NA 8.0 57 5 27 ## 28 23 13 12.0 67 5 28 ## 29 45 252 14.9 81 5 29 ## 30 115 223 5.7 79 5 30 ## 31 37 279 7.4 76 5 31 ## 32 NA 286 8.6 78 6 1 ## 33 NA 287 9.7 74 6 2 ## 34 NA 242 16.1 67 6 3 ## 35 NA 186 9.2 84 6 4 ## 36 NA 220 8.6 85 6 5 ## 37 NA 264 14.3 79 6 6 ## 38 29 127 9.7 82 6 7 ## 39 NA 273 6.9 87 6 8 ## 40 71 291 13.8 90 6 9 ## 41 39 323 11.5 87 6 10 ## 42 NA 259 10.9 93 6 11 ## 43 NA 250 9.2 92 6 12 ## 44 23 148 8.0 82 6 13 ## 45 NA 332 13.8 80 6 14 ## 46 NA 322 11.5 79 6 15 ## 47 21 191 14.9 77 6 16 ## 48 37 284 20.7 72 6 17 ## 49 20 37 9.2 65 6 18 ## 50 12 120 11.5 73 6 19 ## 51 13 137 10.3 76 6 20 ## 52 NA 150 6.3 77 6 21 ## 53 NA 59 1.7 76 6 22 ## 54 NA 91 4.6 76 6 23 ## 55 NA 250 6.3 76 6 24 ## 56 NA 135 8.0 75 6 25 ## 57 NA 127 8.0 78 6 26 ## 58 NA 47 10.3 73 6 27 ## 59 NA 98 11.5 80 6 28 ## 60 NA 31 14.9 77 6 29 ## 61 NA 138 8.0 83 6 30 ## 62 135 269 4.1 84 7 1 ## 63 49 248 9.2 85 7 2 ## 64 32 236 9.2 81 7 3 ## 65 NA 101 10.9 84 7 4 ## 66 64 175 4.6 83 7 5 ## 67 40 314 10.9 83 7 6 ## 68 77 276 5.1 88 7 7 ## 69 97 267 6.3 92 7 8 ## 70 97 272 5.7 92 7 9 ## 71 85 175 7.4 89 7 10 ## 72 NA 139 8.6 82 7 11 ## 73 10 264 14.3 73 7 12 ## 74 27 175 14.9 81 7 13 ## 75 NA 291 14.9 91 7 14 ## 76 7 48 14.3 80 7 15 ## 77 48 260 6.9 81 7 16 ## 78 35 274 10.3 82 7 17 ## 79 61 285 6.3 84 7 18 ## 80 79 187 5.1 87 7 19 ## 81 63 220 11.5 85 7 20 ## 82 16 7 6.9 74 7 21 ## 83 NA 258 9.7 81 7 22 ## 84 NA 295 11.5 82 7 23 ## 85 80 294 8.6 86 7 24 ## 86 108 223 8.0 85 7 25 ## 87 20 81 8.6 82 7 26 ## 88 52 82 12.0 86 7 27 ## 89 82 213 7.4 88 7 28 ## 90 50 275 7.4 86 7 29 ## 91 64 253 7.4 83 7 30 ## 92 59 254 9.2 81 7 31 ## 93 39 83 6.9 81 8 1 ## 94 9 24 13.8 81 8 2 ## 95 16 77 7.4 82 8 3 ## 96 78 NA 6.9 86 8 4 ## 97 35 NA 7.4 85 8 5 ## 98 66 NA 4.6 87 8 6 ## 99 122 255 4.0 89 8 7 ## 100 89 229 10.3 90 8 8 ## 101 110 207 8.0 90 8 9 ## 102 NA 222 8.6 92 8 10 ## 103 NA 137 11.5 86 8 11 ## 104 44 192 11.5 86 8 12 ## 105 28 273 11.5 82 8 13 ## 106 65 157 9.7 80 8 14 ## 107 NA 64 11.5 79 8 15 ## 108 22 71 10.3 77 8 16 ## 109 59 51 6.3 79 8 17 ## 110 23 115 7.4 76 8 18 ## 111 31 244 10.9 78 8 19 ## 112 44 190 10.3 78 8 20 ## 113 21 259 15.5 77 8 21 ## 114 9 36 14.3 72 8 22 ## 115 NA 255 12.6 75 8 23 ## 116 45 212 9.7 79 8 24 ## 117 168 238 3.4 81 8 25 ## 118 73 215 8.0 86 8 26 ## 119 NA 153 5.7 88 8 27 ## 120 76 203 9.7 97 8 28 ## 121 118 225 2.3 94 8 29 ## 122 84 237 6.3 96 8 30 ## 123 85 188 6.3 94 8 31 ## 124 96 167 6.9 91 9 1 ## 125 78 197 5.1 92 9 2 ## 126 73 183 2.8 93 9 3 ## 127 91 189 4.6 93 9 4 ## 128 47 95 7.4 87 9 5 ## 129 32 92 15.5 84 9 6 ## 130 20 252 10.9 80 9 7 ## 131 23 220 10.3 78 9 8 ## 132 21 230 10.9 75 9 9 ## 133 24 259 9.7 73 9 10 ## 134 44 236 14.9 81 9 11 ## 135 21 259 15.5 76 9 12 ## 136 28 238 6.3 77 9 13 ## 137 9 24 10.9 71 9 14 ## 138 13 112 11.5 71 9 15 ## 139 46 237 6.9 78 9 16 ## 140 18 224 13.8 67 9 17 ## 141 13 27 10.3 76 9 18 ## 142 24 238 10.3 68 9 19 ## 143 16 201 8.0 82 9 20 ## 144 13 238 12.6 64 9 21 ## 145 23 14 9.2 71 9 22 ## 146 36 139 10.3 81 9 23 ## 147 7 49 10.3 69 9 24 ## 148 14 20 16.6 63 9 25 ## 149 30 193 6.9 70 9 26 ## 150 NA 145 13.2 77 9 27 ## 151 14 191 14.3 75 9 28 ## 152 18 131 8.0 76 9 29 ## 153 20 223 11.5 68 9 30 12.3 Step 0: What data are missing? There are some missing data. Use md.pattern to see patterns in missingness: md.pattern(airquality) ## Wind Temp Month Day Solar.R Ozone ## 111 1 1 1 1 1 1 0 ## 35 1 1 1 1 1 0 1 ## 5 1 1 1 1 0 1 1 ## 2 1 1 1 1 0 0 2 ## 0 0 0 0 7 37 44 Fill in the following: There are 111 rows of complete data. There are 35 rows where only ozone is missing. There are 2 rows where both ozone and Solar.R are missing. There are 37 rows missing an ozone measurement. There are 44 NA’s in the dataset. 12.4 Step 1: Handling Missing Data 12.4.1 Any Ideas? Here is a scatterplot of Solar.R and Ozone, with missing values “pushed” to the intercepts: airquality %&gt;% mutate(missing = if_else(is.na(Solar.R) | is.na(Ozone), TRUE, FALSE), Solar.R = ifelse(is.na(Solar.R), 0, Solar.R), Ozone = ifelse(is.na(Ozone), 0, Ozone)) %&gt;% ggplot(aes(Solar.R, Ozone)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;) + geom_point(aes(colour = missing)) + theme_bw() + scale_colour_discrete(guide = FALSE) ## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please ## use `guide = &quot;none&quot;` instead. Discussion: What are some ways of handling the missing data? What are the consequences of these ways? Remove data. Remove rows with missing data (called the complete case). Consequence: We’re throwing away information that could be used to reduce the final model function’s SE. Remove rows where only the response is missing, and don’t use Solar.R in your regression (because it has some missing values). Consequence: If Solar.R is predictive of Ozone, then we’d be losing that predictive power by not including it. Impute: Mean imputation: replace an NA with a prediction of its mean using other variables as predictors. Consequence: imputed data would fall artificially close to the center of the “data cloud”. This means a fitted model function would have an artificially small SE. Multiple imputation: impute with multiple draws from a predictive distribution. A great choice! No real “consequences” here, aside from the inherent risk of biasing the model function that comes with imputing values. 12.4.2 mice First, remove the day and month columns (we won’t be using them): airquality &lt;- airquality %&gt;% select(-Month, -Day) Make m random imputations using mice::mice(): m &lt;- 10 (init &lt;- mice(airquality, m = m, printFlag = FALSE)) ## Class: mids ## Number of multiple imputations: 10 ## Imputation methods: ## Ozone Solar.R Wind Temp ## &quot;pmm&quot; &quot;pmm&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## Ozone Solar.R Wind Temp ## Ozone 0 1 1 1 ## Solar.R 1 0 1 1 ## Wind 1 1 0 1 ## Temp 1 1 1 0 Check out the first imputed data set using mice::complete(). WARNING: there’s also a tidyr::complete()! Rows 5 and 6, for example, originally contained missing data. mice::complete(init, 1) ## Ozone Solar.R Wind Temp ## 1 41 190 7.4 67 ## 2 36 118 8.0 72 ## 3 12 149 12.6 74 ## 4 18 313 11.5 62 ## 5 4 25 14.3 56 ## 6 28 49 14.9 66 ## 7 23 299 8.6 65 ## 8 19 99 13.8 59 ## 9 8 19 20.1 61 ## 10 13 194 8.6 69 ## 11 7 194 6.9 74 ## 12 16 256 9.7 69 ## 13 11 290 9.2 66 ## 14 14 274 10.9 68 ## 15 18 65 13.2 58 ## 16 14 334 11.5 64 ## 17 34 307 12.0 66 ## 18 6 78 18.4 57 ## 19 30 322 11.5 68 ## 20 11 44 9.7 62 ## 21 1 8 9.7 59 ## 22 11 320 16.6 73 ## 23 4 25 9.7 61 ## 24 32 92 12.0 61 ## 25 6 66 16.6 57 ## 26 1 266 14.9 58 ## 27 37 290 8.0 57 ## 28 23 13 12.0 67 ## 29 45 252 14.9 81 ## 30 115 223 5.7 79 ## 31 37 279 7.4 76 ## 32 16 286 8.6 78 ## 33 65 287 9.7 74 ## 34 11 242 16.1 67 ## 35 46 186 9.2 84 ## 36 47 220 8.6 85 ## 37 16 264 14.3 79 ## 38 29 127 9.7 82 ## 39 168 273 6.9 87 ## 40 71 291 13.8 90 ## 41 39 323 11.5 87 ## 42 85 259 10.9 93 ## 43 61 250 9.2 92 ## 44 23 148 8.0 82 ## 45 31 332 13.8 80 ## 46 20 322 11.5 79 ## 47 21 191 14.9 77 ## 48 37 284 20.7 72 ## 49 20 37 9.2 65 ## 50 12 120 11.5 73 ## 51 13 137 10.3 76 ## 52 37 150 6.3 77 ## 53 49 59 1.7 76 ## 54 23 91 4.6 76 ## 55 115 250 6.3 76 ## 56 44 135 8.0 75 ## 57 29 127 8.0 78 ## 58 14 47 10.3 73 ## 59 16 98 11.5 80 ## 60 18 31 14.9 77 ## 61 59 138 8.0 83 ## 62 135 269 4.1 84 ## 63 49 248 9.2 85 ## 64 32 236 9.2 81 ## 65 44 101 10.9 84 ## 66 64 175 4.6 83 ## 67 40 314 10.9 83 ## 68 77 276 5.1 88 ## 69 97 267 6.3 92 ## 70 97 272 5.7 92 ## 71 85 175 7.4 89 ## 72 16 139 8.6 82 ## 73 10 264 14.3 73 ## 74 27 175 14.9 81 ## 75 39 291 14.9 91 ## 76 7 48 14.3 80 ## 77 48 260 6.9 81 ## 78 35 274 10.3 82 ## 79 61 285 6.3 84 ## 80 79 187 5.1 87 ## 81 63 220 11.5 85 ## 82 16 7 6.9 74 ## 83 37 258 9.7 81 ## 84 44 295 11.5 82 ## 85 80 294 8.6 86 ## 86 108 223 8.0 85 ## 87 20 81 8.6 82 ## 88 52 82 12.0 86 ## 89 82 213 7.4 88 ## 90 50 275 7.4 86 ## 91 64 253 7.4 83 ## 92 59 254 9.2 81 ## 93 39 83 6.9 81 ## 94 9 24 13.8 81 ## 95 16 77 7.4 82 ## 96 78 157 6.9 86 ## 97 35 95 7.4 85 ## 98 66 183 4.6 87 ## 99 122 255 4.0 89 ## 100 89 229 10.3 90 ## 101 110 207 8.0 90 ## 102 50 222 8.6 92 ## 103 29 137 11.5 86 ## 104 44 192 11.5 86 ## 105 28 273 11.5 82 ## 106 65 157 9.7 80 ## 107 32 64 11.5 79 ## 108 22 71 10.3 77 ## 109 59 51 6.3 79 ## 110 23 115 7.4 76 ## 111 31 244 10.9 78 ## 112 44 190 10.3 78 ## 113 21 259 15.5 77 ## 114 9 36 14.3 72 ## 115 16 255 12.6 75 ## 116 45 212 9.7 79 ## 117 168 238 3.4 81 ## 118 73 215 8.0 86 ## 119 50 153 5.7 88 ## 120 76 203 9.7 97 ## 121 118 225 2.3 94 ## 122 84 237 6.3 96 ## 123 85 188 6.3 94 ## 124 96 167 6.9 91 ## 125 78 197 5.1 92 ## 126 73 183 2.8 93 ## 127 91 189 4.6 93 ## 128 47 95 7.4 87 ## 129 32 92 15.5 84 ## 130 20 252 10.9 80 ## 131 23 220 10.3 78 ## 132 21 230 10.9 75 ## 133 24 259 9.7 73 ## 134 44 236 14.9 81 ## 135 21 259 15.5 76 ## 136 28 238 6.3 77 ## 137 9 24 10.9 71 ## 138 13 112 11.5 71 ## 139 46 237 6.9 78 ## 140 18 224 13.8 67 ## 141 13 27 10.3 76 ## 142 24 238 10.3 68 ## 143 16 201 8.0 82 ## 144 13 238 12.6 64 ## 145 23 14 9.2 71 ## 146 36 139 10.3 81 ## 147 7 49 10.3 69 ## 148 14 20 16.6 63 ## 149 30 193 6.9 70 ## 150 21 145 13.2 77 ## 151 14 191 14.3 75 ## 152 18 131 8.0 76 ## 153 20 223 11.5 68 Plot one of them: mice::complete(init, 1) %&gt;% mutate(missing = if_else(is.na(airquality$Solar.R) | is.na(airquality$Ozone), TRUE, FALSE)) %&gt;% ggplot(aes(Solar.R, Ozone)) + geom_point(aes(colour = missing)) + theme_bw() Now, fit a linear model on each data set using the with() generic function (method with.mids(): (fits &lt;- with(init, lm(Ozone ~ Solar.R + Wind + Temp))) ## call : ## with.mids(data = init, expr = lm(Ozone ~ Solar.R + Wind + Temp)) ## ## call1 : ## mice(data = airquality, m = m, printFlag = FALSE) ## ## nmis : ## Ozone Solar.R Wind Temp ## 37 7 0 0 ## ## analyses : ## [[1]] ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp) ## ## Coefficients: ## (Intercept) Solar.R Wind Temp ## -53.11243 0.06629 -3.34743 1.48175 ## ## ## [[2]] ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp) ## ## Coefficients: ## (Intercept) Solar.R Wind Temp ## -50.80543 0.05793 -3.85354 1.54888 ## ## ## [[3]] ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp) ## ## Coefficients: ## (Intercept) Solar.R Wind Temp ## -50.62717 0.07319 -3.58363 1.47478 ## ## ## [[4]] ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp) ## ## Coefficients: ## (Intercept) Solar.R Wind Temp ## -59.62411 0.06313 -3.02095 1.53625 ## ## ## [[5]] ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp) ## ## Coefficients: ## (Intercept) Solar.R Wind Temp ## -62.09685 0.06194 -2.98683 1.55723 ## ## ## [[6]] ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp) ## ## Coefficients: ## (Intercept) Solar.R Wind Temp ## -52.78210 0.06118 -3.00750 1.43609 ## ## ## [[7]] ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp) ## ## Coefficients: ## (Intercept) Solar.R Wind Temp ## -88.05491 0.04588 -2.47311 1.88132 ## ## ## [[8]] ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp) ## ## Coefficients: ## (Intercept) Solar.R Wind Temp ## -58.39802 0.06504 -3.12379 1.53293 ## ## ## [[9]] ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp) ## ## Coefficients: ## (Intercept) Solar.R Wind Temp ## -52.16945 0.07695 -3.38366 1.45056 ## ## ## [[10]] ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp) ## ## Coefficients: ## (Intercept) Solar.R Wind Temp ## -68.46852 0.05727 -2.86923 1.64138 Looks can be deceiving. This is not actually a list of length m! Unveil its true nature: unclass(fits) %&gt;% str(max.level = 1) ## List of 4 ## $ call : language with.mids(data = init, expr = lm(Ozone ~ Solar.R + Wind + Temp)) ## $ call1 : language mice(data = airquality, m = m, printFlag = FALSE) ## $ nmis : Named int [1:4] 37 7 0 0 ## ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Ozone&quot; &quot;Solar.R&quot; &quot;Wind&quot; &quot;Temp&quot; ## $ analyses:List of 10 It’s now easier to find the lm fit on the first dataset: fits$analyses[[1]] ## ## Call: ## lm(formula = Ozone ~ Solar.R + Wind + Temp) ## ## Coefficients: ## (Intercept) Solar.R Wind Temp ## -53.11243 0.06629 -3.34743 1.48175 Or, we can obtain a summary of each fitted model: summary(fits) ## # A tibble: 40 × 6 ## term estimate std.error statistic p.value nobs ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 (Intercept) -53.1 20.2 -2.63 9.34e- 3 153 ## 2 Solar.R 0.0663 0.0209 3.18 1.80e- 3 153 ## 3 Wind -3.35 0.579 -5.78 4.16e- 8 153 ## 4 Temp 1.48 0.223 6.64 5.46e-10 153 ## 5 (Intercept) -50.8 21.1 -2.41 1.71e- 2 153 ## 6 Solar.R 0.0579 0.0220 2.63 9.37e- 3 153 ## 7 Wind -3.85 0.606 -6.35 2.40e- 9 153 ## 8 Temp 1.55 0.232 6.68 4.50e-10 153 ## 9 (Intercept) -50.6 20.0 -2.53 1.24e- 2 153 ## 10 Solar.R 0.0732 0.0208 3.52 5.81e- 4 153 ## # … with 30 more rows As an aside, let’s demonstrate that we can also use mice to fit GLM’s: with(init, glm( Ozone ~ Solar.R + Wind + Temp, family = Gamma(link=&quot;log&quot;) )) %&gt;% summary() ## # A tibble: 40 × 6 ## term estimate std.error statistic p.value nobs ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 (Intercept) 0.654 0.461 1.42 1.58e- 1 153 ## 2 Solar.R 0.00202 0.000476 4.25 3.74e- 5 153 ## 3 Wind -0.0745 0.0132 -5.63 8.62e- 8 153 ## 4 Temp 0.0416 0.00510 8.15 1.33e-13 153 ## 5 (Intercept) 0.977 0.448 2.18 3.09e- 2 153 ## 6 Solar.R 0.00204 0.000468 4.35 2.51e- 5 153 ## 7 Wind -0.0848 0.0129 -6.57 7.87e-10 153 ## 8 Temp 0.0389 0.00494 7.88 6.19e-13 153 ## 9 (Intercept) 0.786 0.451 1.74 8.35e- 2 153 ## 10 Solar.R 0.00234 0.000469 4.99 1.64e- 6 153 ## # … with 30 more rows 12.5 Step 3: Pool results The last step is to pool the results together: pool(fits) ## Class: mipo m = 10 ## term m estimate ubar b t dfcom ## 1 (Intercept) 10 -59.61389905 3.835088e+02 1.330724e+02 5.298884e+02 149 ## 2 Solar.R 10 0.06287982 4.093623e-04 7.439883e-05 4.912010e-04 149 ## 3 Wind 10 -3.16496740 3.157597e-01 1.528463e-01 4.838906e-01 149 ## 4 Temp 10 1.55411797 4.750070e-02 1.685197e-02 6.603786e-02 149 ## df riv lambda fmi ## 1 55.94156 0.3816852 0.2762461 0.3008045 ## 2 88.92990 0.1999175 0.1666094 0.1847404 ## 3 41.95312 0.5324647 0.3474564 0.3764886 ## 4 54.91493 0.3902503 0.2807051 0.3055448 The estimate column you see are just the averages of all m models. Column names make more sense in light of the book “Multiple Imputation for Nonresponse in Surveys” by Rubin (1987), page 76-77: estimate: the average of the regression coefficients across m models. ubar: the average variance (i.e., average SE^2) across m models. b: the sample variance of the m regression coefficients. t: a final estimate of the SE^2 of each regression coefficient. = ubar + (1 + 1/m) b df: the degrees of freedom associated with the final regression coefficient estimates. An alpha-level CI: estimate +/- qt(alpha/2, df) * sqrt(t). riv: the relative increase in variance due to randomness. = t/ubar - 1 12.6 Concepts There are three common missing data mechanisms: Missing Completely At Random (MCAR): when the chance of missingness does not depend on any variable; missingness is totally random. Missing At Random (MAR): when the chance of missingness depends on other observed variables. Missing Not At Random (MNAR): when the chance of missingness depends on unobserved variables. Proceeding with an analysis by removing missing data can result in a model with standard errors of the estimates that are larger than they could be by including partially complete records. Proceeding with an analysis by imputing missing data by an estimate of the mean can result in a model with standard errors of the estimates that are smaller than they ought to be. An approach that uses the information contained in partially complete records, yet does not assume any more information, is to use multiple imputations. The approach contains three steps: Form multiple datasets containing imputed values. Each dataset should be formed by imputing the missing records in each unit/row with a random draw from a predictive distribution for those records. Fit the model of interest on each imputed dataset. Combine the models to obtain one final model. "],["regression-on-an-entire-distribution-probabilistic-forecasting.html", "Chapter 13 Regression on an entire distribution: Probabilistic Forecasting 13.1 Probabilistic Forecasting: What it is 13.2 Review: Univariate distribution estimates 13.3 Probabilistic Forecasts: subset-based learning methods 13.4 Discussion Points 13.5 When are they not useful?", " Chapter 13 Regression on an entire distribution: Probabilistic Forecasting Caution: in a highly developmental stage! See Section 1.1. suppressPackageStartupMessages(library(tidyverse)) baseball &lt;- Lahman::Teams %&gt;% tbl_df %&gt;% select(runs=R, hits=H) ## Warning: `tbl_df()` was deprecated in dplyr 1.0.0. ## Please use `tibble::as_tibble()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. Up until now, we’ve only seen different ways of using a predictor to give us more information the mean and mode of the response. The world holds a huge emphasis on the mean and mode, but these are not always what’s important. Two alternatives are: Probabilistic forecasting Quantile Regression (numeric response only) 13.1 Probabilistic Forecasting: What it is The idea here is to put forth an entire probability distribution as a prediction. Let’s look at an example. Suppose there are two baseball teams, one that gets 1000 total hits in a year, and another that gets 1500. Using “total hits in a year” as a predictor, we set out to predict the total number of runs of both teams. Here’s the top snippet of the data: baseball ## # A tibble: 2,955 × 2 ## runs hits ## &lt;int&gt; &lt;int&gt; ## 1 401 426 ## 2 302 323 ## 3 249 328 ## 4 137 178 ## 5 302 403 ## 6 376 410 ## 7 231 274 ## 8 351 384 ## 9 310 375 ## 10 617 753 ## # … with 2,945 more rows Let’s not concern ourselves with the methods yet. Using a standard regression technique, here are our predictions: r &lt;- 20 datsub &lt;- filter(baseball, (hits&gt;=1000-r &amp; hits&lt;=1000+r) | (hits&gt;=1500-r &amp; hits&lt;=1500+r)) %&gt;% mutate(approx_hits = if_else(hits&gt;=1000-r &amp; hits&lt;=1000+r, 1000, 1500)) datsub %&gt;% group_by(approx_hits) %&gt;% summarize(expected_runs=round(mean(runs))) %&gt;% rename(hits=approx_hits) %&gt;% select(hits, expected_runs) ## # A tibble: 2 × 2 ## hits expected_runs ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1000 549 ## 2 1500 770 Using a probabilistic forecast, here are our predictions: Don’t you think this is far more informative than the mean estimates in the above table? The probabilistic forecast/prediction contains the most amount of information about the response as possible (based on a set of predictors), because it communicates the entire belief of what \\(Y\\) values are most plausible, given values of the predictor. Predictions/forecasts here are called predictive distributions. From @gneiting_raftery: Indeed, over the past two decades, probabilistic forecasting has become routine in such applications as weather and climate prediction (Palmer 2002; Gneiting and Raftery 2005), computational finance (Duffle and Pan 1997), and macroeconomic forecasting (Garratt, Lee, Pesaran, and Shin 2003; Granger 2006). 13.2 Review: Univariate distribution estimates Let’s review how to estimate a univariate probability density function or probability mass function. 13.2.1 Continuous response Here’s a random sample of 10 continuous variables, ordered from smallest to largest, stored in the variable x: Recall that we can use histograms to estimate the density of the data. The idea is: Cut the range of the data into “bins” of a certain width. For these data, the range is 40. Let’s set up four bins of width 10: -19.8 to -9.8, -9.8 to 0.2, etc. Count the number of observations that fall into each bin. For our setup, the number of observations falling into the four bins, in order, are: 3,2,2,3. Make a bar plot (with no space between the bars), where the bar width corresponds to the bins, and the bar height corresponds to the number of observations in that bin. For our setup, we have: ggplot(data.frame(x=x), aes(x)) + geom_histogram(binwidth=10, center=min(x)+5, fill=&quot;orange&quot;, colour=&quot;black&quot;) + theme_bw() (Note: this is not a true density, since the area under the curve is not 1, but the shape is what matters) You’d have to play with the binwidth to get a histogram that looks about right (not too jagged, not too coarse). For the above example, there are too few data to make a good estimate. Let’s now generate 1000 observations, and make a histogram using qplot from R’s ggplot2 package, with a variety of binwidths – too small, too large, and just right. x &lt;- rnorm(1000, sd=10) qplot(x, binwidth=1) # too small qplot(x, binwidth=10) # too big qplot(x, binwidth=3.5) # just right Advanced method: There’s a technique called the kernel density estimate that works as an alernative to the histogram. The idea is to put a “little mound” (a kernel) on top of each observation, and add them up. Instead of playing with the binwidth, you can play with the “bandwidth” of the kernels. Use geom=\"density\" in qplot, and use bw to play with the bandwidth: qplot(x, geom=&quot;density&quot;, bw=2.5) 13.2.2 Discrete Response When the response is discrete (this includes categorical), the approach is simpler: Calculate the proportion of observations that fall into each category. Make a bar chart, placing a bar over each category, and using the proportions as the bar heights. Here are ten observations, stored in x: x ## [1] 1 0 0 0 2 0 1 2 3 0 The proportions are as follows: props &lt;- tibble(Value=x) %&gt;% group_by(Value) %&gt;% summarize(Proportion=length(Value)/length(x)) You can plot these proportions with qplot, specifying geom=\"col\": qplot(x=Value, y=Proportion, data=props, geom=&quot;col&quot;) You can use ggplot2 to calculate the proportions, but it’s more complex. It’s easier to plot the raw counts, instead of proportions – and that’s fine, you’ll still get the same shape. Using qplot again, let’s make a plot for 1000 observations (note that I indicate that my data are discrete by using the factor function): set.seed(2) x &lt;- rpois(1000, lambda=1) qplot(factor(x)) Here’s the code to get proportions instead of counts: qplot(factor(x), mapping=aes(y=..prop..), group=1) 13.3 Probabilistic Forecasts: subset-based learning methods 13.3.1 The techniques The local methods and classification/regression trees that we’ve seen so far can be used to produce probabilistic forecasts. For local methods, let’s ignore the complications of kernel weighting and local polynomials. These methods result in a subset of the data, for which we’re used to taking the mean or mode. Instead, use the subsetted data to plot a distribution. For kNN, form a histogram/density plot/bar plot using the \\(k\\) nearest neighbours. For the moving window (loess), form a histogram/density plot/bar plot using the observations that fall in the window. For tree-based methods, use the observations within a leaf to form a histogram/density plot/bar plot for that leaf. The above baseball example used a moving window with a radius of 20 hits. Visually, you can see the data that I subsetted within these two narrow windows, for hits of 1000 and 1500: ggplot(baseball, aes(hits, runs)) + geom_point(colour=&quot;orange&quot;, alpha=0.1) + geom_vline(xintercept=c(1000+c(-r,r), 1500+c(-r,r)), linetype=&quot;dashed&quot;) + theme_bw() + labs(x=&quot;Number of Hits (X)&quot;, y=&quot;Number of Runs (Y)&quot;) 13.3.2 Exercise Install the Lahman package, which contains the Teams dataset. Build a null model probabilistic forecast of “number of runs” (R column). Build a probabilistic forecast, using kNN, of “number of runs” for a team that has 1500 hits (H column) and 70 wins (W column). Don’t forget to scale the predictors! Do the same thing, but using linear regression. What additional assumption(s) is/are needed here? 13.3.3 Bias-variance tradeoff Let’s examine the bias-variance / overfitting-underfitting tradeoff with kNN-based probabilistic forecasts. I’ll run a simulation like so: Generate data from a bivariate Normal distribution, so that \\(X \\sim N(0, 100)\\), and \\(Y = X + N(0, 100)\\). Training data will contain 500 observations, for which a kNN probabilistic forecast will be built when \\(X=25\\). Try both a small (k=15) and large (k=100) value of \\(k\\). For each value of \\(k\\), we’ll generate 20 training data sets. Here are the 20 estimates for the values of \\(k\\). The overall mean of the distributions are indicated by a vertical dashed line. Notice that: When \\(k\\) is large, our estimates are biased, because the distributions are not centered correctly. But, the estimates are more consistent. When \\(k\\) is small, our estimates are less biased, because the distributions overall have a mean that is close to the true mean. But the variance is high – we get all sorts of distribution shapes here. A similar thing happens with a moving window, with the window width parameter. For tree-based methods, the amount that you partition the predictor space controls the bias-variance tradeoff. 13.3.4 Evaluating Model Goodness To choose a balance between bias and variance, we need a measure of prediction goodness. When predicting the mean, the MSE works. When predicting the mode, the classification error works. But what works for probabilistic forecasts? This is an active area of research. The idea is to use a proper scoring rule – a way of assigning a score based on the forecast distribution and the outcome only, that also encourages honesty. We won’t go into details – see [@gneiting_raftery] for details. At the very least, one should check that the forecast distributions are “calibrated” – that is, the actual outcomes are spread evenly amongst the forecasts. You can check this by applying the forecast cdf to the corresponding outcome – the resulting sample should be Uniform(0,1). Note that this is built-in to at least some proper scoring rules. For this course, we won’t be picky about how you choose your tuning parameters. Just look for a subset that you think has “enough” observations in it so that the distribution starts to take some shape, but not so much that it starts to shift. 13.4 Discussion Points For (1) and (2) below, you’re choosing between two candidates to hire. Discuss the pros and cons of choosing one candidate over the other in the following situations. Both are predicted to have the same productivity score of 75, but have the following probabilistic forecasts. It’s hard to make a decision here. On the one hand, we can be fairly certain that the actual productivity of candidate A will be about 75, but there’s more of a gamble with candidate B. There’s a very real chance that B’s productivity is actually quite a bit higher than A – for example, a productivity of 80 is plausible for B, but not for A. On the other hand, there’s also a very real chance that B’s productivity is actually quite a bit lower than A, for the same reason. Your decision would depend on whether you would want to take a risk or not. On the other hand, in reality, this is only one tool out of many other aspects of the candidate that you would consider. It might be a good idea to chat with B to get a better sense of what their productivity might actually be. Two “non-overlapping” forecasts: In this case, B is very very likely to have higher productivity than A, because all “plausible” productivity values for B are higher than all “plausible” productivity values of A. Again, this is just one tool you might use to make a decision. You’ve formed a probabilistic forecast for a particular value of the predictors, displayed below as a density. You then collect test data for that same value of the predictor, indicated as the points below the density. What is the problem with the probabilistic forecast? The forecast is biased, because the actual values are occuring near the upper tail of the distribution – they should be scattered about the middle, with a higher density of points occuring near 0. If using local methods, we’d have to reduce \\(k\\) or the window width to decrease bias (to remove “further” data that are less relevant); if using a tree-based method, you could grow the tree deeper to lower the bias. 13.5 When are they not useful? Probabilistic forecasts are useful if you’re making a small amount of decisions at a time. For example: Predicting which hockey team will win the Stanley Cup Looking at the 2-day-ahead prediction of river flow every day to decide whether to take flood mitigation measures. But they are not appropriate when making decisions en-masse. For example: A bus company wants to know how long it takes a bus to travel between stops, for all stops and all busses. You want to predict future behaviour of customers. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
