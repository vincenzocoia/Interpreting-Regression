<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 12 Overfitting: The problem with adding too many parameters | Interpreting Regression</title>
  <meta name="description" content="A book about the why of regression to help you make decisions about your analysis.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 12 Overfitting: The problem with adding too many parameters | Interpreting Regression" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book about the why of regression to help you make decisions about your analysis." />
  <meta name="github-repo" content="vincenzocoia/Interpreting-Regression" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Overfitting: The problem with adding too many parameters | Interpreting Regression" />
  
  <meta name="twitter:description" content="A book about the why of regression to help you make decisions about your analysis." />
  

<meta name="author" content="Vincenzo Coia">


<meta name="date" content="2019-08-09">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html">
<link rel="next" href="describing-relationships.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Interpreting Regression</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preamble</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#caution"><i class="fa fa-check"></i><b>1.1</b> Caution</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#purpose-of-the-book"><i class="fa fa-check"></i><b>1.2</b> Purpose of the book</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#tasks-that-motivate-regression"><i class="fa fa-check"></i><b>1.3</b> Tasks that motivate Regression</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#examples"><i class="fa fa-check"></i><b>1.4</b> Examples</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html"><i class="fa fa-check"></i><b>2</b> Regression in the context of problem solving</a><ul>
<li class="chapter" data-level="2.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#communicating-distillation-3-4"><i class="fa fa-check"></i><b>2.1</b> Communicating (Distillation 3-4)</a></li>
<li class="chapter" data-level="2.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#modelling-distillation-2-3"><i class="fa fa-check"></i><b>2.2</b> Modelling (Distillation 2-3)</a></li>
<li class="chapter" data-level="2.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#asking-useful-statistical-questions-distillation-1-2"><i class="fa fa-check"></i><b>2.3</b> Asking useful statistical questions (Distillation 1-2)</a><ul>
<li class="chapter" data-level="2.3.1" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#business-objectives-examples"><i class="fa fa-check"></i><b>2.3.1</b> Business objectives: examples</a></li>
<li class="chapter" data-level="2.3.2" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-refining"><i class="fa fa-check"></i><b>2.3.2</b> Statistical objectives: refining</a></li>
<li class="chapter" data-level="2.3.3" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-examples"><i class="fa fa-check"></i><b>2.3.3</b> Statistical objectives: examples</a></li>
<li class="chapter" data-level="2.3.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-questions-are-not-the-full-picture"><i class="fa fa-check"></i><b>2.3.4</b> Statistical questions are not the full picture</a></li>
<li class="chapter" data-level="2.3.5" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#statistical-objectives-unrelated-to-supervised-learning"><i class="fa fa-check"></i><b>2.3.5</b> Statistical objectives unrelated to supervised learning</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="regression-in-the-context-of-problem-solving.html"><a href="regression-in-the-context-of-problem-solving.html#prerequisites-to-an-analysis"><i class="fa fa-check"></i><b>2.4</b> Prerequisites to an analysis</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="an-outcome-on-its-own.html"><a href="an-outcome-on-its-own.html"><i class="fa fa-check"></i>An outcome on its own</a></li>
<li class="chapter" data-level="3" data-path="distributions-uncertainty-is-worth-explaining.html"><a href="distributions-uncertainty-is-worth-explaining.html"><i class="fa fa-check"></i><b>3</b> Distributions: Uncertainty is worth explaining</a></li>
<li class="chapter" data-level="4" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html"><i class="fa fa-check"></i><b>4</b> Explaining an uncertain outcome: interpretable quantities</a><ul>
<li class="chapter" data-level="4.1" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#probabilistic-quantities"><i class="fa fa-check"></i><b>4.1</b> Probabilistic Quantities</a></li>
<li class="chapter" data-level="4.2" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#what-is-the-mean-anyway"><i class="fa fa-check"></i><b>4.2</b> What is the mean, anyway?</a></li>
<li class="chapter" data-level="4.3" data-path="explaining-an-uncertain-outcome-interpretable-quantities.html"><a href="explaining-an-uncertain-outcome-interpretable-quantities.html#quantiles"><i class="fa fa-check"></i><b>4.3</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>5</b> Estimation</a><ul>
<li class="chapter" data-level="5.1" data-path="estimation.html"><a href="estimation.html#estimation-of-probabilistic-quantities"><i class="fa fa-check"></i><b>5.1</b> Estimation of Probabilistic Quantities</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html"><i class="fa fa-check"></i><b>6</b> Parametric Families of Distributions</a><ul>
<li class="chapter" data-level="6.1" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#parametric-families-of-distributions-1"><i class="fa fa-check"></i><b>6.1</b> Parametric Families of Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#analyses-under-a-distributional-assumption"><i class="fa fa-check"></i><b>6.2</b> Analyses under a Distributional Assumption</a><ul>
<li class="chapter" data-level="6.2.1" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>6.2.1</b> Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="6.2.2" data-path="parametric-families-of-distributions.html"><a href="parametric-families-of-distributions.html#usefulness-in-practice"><i class="fa fa-check"></i><b>6.2.2</b> Usefulness in Practice</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="prediction-harnessing-the-signal.html"><a href="prediction-harnessing-the-signal.html"><i class="fa fa-check"></i>Prediction: harnessing the signal</a></li>
<li class="chapter" data-level="7" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html"><i class="fa fa-check"></i><b>7</b> Reducing uncertainty of the outcome: including predictors</a><ul>
<li class="chapter" data-level="7.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-terminology"><i class="fa fa-check"></i><b>7.1</b> Variable terminology</a><ul>
<li class="chapter" data-level="7.1.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#variable-types"><i class="fa fa-check"></i><b>7.1.1</b> Variable types</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#irreducible-error"><i class="fa fa-check"></i><b>7.2</b> Irreducible Error</a></li>
<li class="chapter" data-level="7.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#in-class-exercises-irreducible-error"><i class="fa fa-check"></i><b>7.3</b> In-class Exercises: Irreducible Error</a><ul>
<li class="chapter" data-level="7.3.1" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-regression"><i class="fa fa-check"></i><b>7.3.1</b> Oracle regression</a></li>
<li class="chapter" data-level="7.3.2" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#oracle-classification"><i class="fa fa-check"></i><b>7.3.2</b> Oracle classification</a></li>
<li class="chapter" data-level="7.3.3" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-random-prediction"><i class="fa fa-check"></i><b>7.3.3</b> (BONUS) Random prediction</a></li>
<li class="chapter" data-level="7.3.4" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-a-more-non-standard-regression"><i class="fa fa-check"></i><b>7.3.4</b> (BONUS) A more non-standard regression</a></li>
<li class="chapter" data-level="7.3.5" data-path="reducing-uncertainty-of-the-outcome-including-predictors.html"><a href="reducing-uncertainty-of-the-outcome-including-predictors.html#bonus-oracle-mse"><i class="fa fa-check"></i><b>7.3.5</b> (BONUS) Oracle MSE</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html"><i class="fa fa-check"></i><b>8</b> The signal: model functions</a><ul>
<li class="chapter" data-level="8.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#linear-quantile-regression"><i class="fa fa-check"></i><b>8.1</b> Linear Quantile Regression</a><ul>
<li class="chapter" data-level="8.1.1" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#exercise"><i class="fa fa-check"></i><b>8.1.1</b> Exercise</a></li>
<li class="chapter" data-level="8.1.2" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-crossing-quantiles"><i class="fa fa-check"></i><b>8.1.2</b> Problem: Crossing quantiles</a></li>
<li class="chapter" data-level="8.1.3" data-path="the-signal-model-functions.html"><a href="the-signal-model-functions.html#problem-upper-quantiles"><i class="fa fa-check"></i><b>8.1.3</b> Problem: Upper quantiles</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html"><i class="fa fa-check"></i><b>9</b> The Model-Fitting Paradigm in R</a><ul>
<li class="chapter" data-level="9.1" data-path="the-model-fitting-paradigm-in-r.html"><a href="the-model-fitting-paradigm-in-r.html#broom-package"><i class="fa fa-check"></i><b>9.1</b> <code>broom</code> package</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html"><i class="fa fa-check"></i><b>10</b> Estimating parametric model functions</a><ul>
<li class="chapter" data-level="10.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#writing-the-sample-mean-as-an-optimization-problem"><i class="fa fa-check"></i><b>10.1</b> Writing the sample mean as an optimization problem</a></li>
<li class="chapter" data-level="10.2" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#evaluating-model-goodness-quantiles"><i class="fa fa-check"></i><b>10.2</b> Evaluating Model Goodness: Quantiles</a></li>
<li class="chapter" data-level="10.3" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#simple-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Simple Linear Regression</a><ul>
<li class="chapter" data-level="10.3.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#model-specification"><i class="fa fa-check"></i><b>10.3.1</b> Model Specification</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#linear-models-in-general"><i class="fa fa-check"></i><b>10.4</b> Linear models in general</a></li>
<li class="chapter" data-level="10.5" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#reference-treatment-parameterization"><i class="fa fa-check"></i><b>10.5</b> reference-treatment parameterization</a><ul>
<li class="chapter" data-level="10.5.1" data-path="estimating-parametric-model-functions.html"><a href="estimating-parametric-model-functions.html#more-than-one-category-lab-2"><i class="fa fa-check"></i><b>10.5.1</b> More than one category (Lab 2)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><i class="fa fa-check"></i><b>11</b> Estimating assumption-free: the world of supervised learning techniques</a><ul>
<li class="chapter" data-level="11.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#what-machine-learning-is"><i class="fa fa-check"></i><b>11.1</b> What machine learning is</a></li>
<li class="chapter" data-level="11.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#types-of-supervised-learning"><i class="fa fa-check"></i><b>11.2</b> Types of Supervised Learning</a></li>
<li class="chapter" data-level="11.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#local-regression"><i class="fa fa-check"></i><b>11.3</b> Local Regression</a><ul>
<li class="chapter" data-level="11.3.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#knn"><i class="fa fa-check"></i><b>11.3.1</b> kNN</a></li>
<li class="chapter" data-level="11.3.2" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess"><i class="fa fa-check"></i><b>11.3.2</b> loess</a></li>
<li class="chapter" data-level="11.3.3" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#in-class-exercises"><i class="fa fa-check"></i><b>11.3.3</b> In-Class Exercises</a></li>
<li class="chapter" data-level="11.3.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#hyperparameters-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>11.3.4</b> Hyperparameters and the bias/variance tradeoff</a></li>
<li class="chapter" data-level="11.3.5" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#extensions-to-knn-and-loess"><i class="fa fa-check"></i><b>11.3.5</b> Extensions to kNN and loess</a></li>
<li class="chapter" data-level="11.3.6" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#model-assumptions-and-the-biasvariance-tradeoff"><i class="fa fa-check"></i><b>11.3.6</b> Model assumptions and the bias/variance tradeoff</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#splines-and-loess-regression"><i class="fa fa-check"></i><b>11.4</b> Splines and Loess Regression</a><ul>
<li class="chapter" data-level="11.4.1" data-path="estimating-assumption-free-the-world-of-supervised-learning-techniques.html"><a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html#loess-1"><i class="fa fa-check"></i><b>11.4.1</b> Loess</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html"><i class="fa fa-check"></i><b>12</b> Overfitting: The problem with adding too many parameters</a><ul>
<li class="chapter" data-level="12.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#classification-exercise-do-together"><i class="fa fa-check"></i><b>12.1</b> Classification Exercise: Do Together</a></li>
<li class="chapter" data-level="12.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#training-error-vs.generalization-error"><i class="fa fa-check"></i><b>12.2</b> Training Error vs. Generalization Error</a></li>
<li class="chapter" data-level="12.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-complexity"><i class="fa fa-check"></i><b>12.3</b> Model complexity</a><ul>
<li class="chapter" data-level="12.3.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#activity"><i class="fa fa-check"></i><b>12.3.1</b> Activity</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducible-error"><i class="fa fa-check"></i><b>12.4</b> Reducible Error</a><ul>
<li class="chapter" data-level="12.4.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#what-is-it"><i class="fa fa-check"></i><b>12.4.1</b> What is it?</a></li>
<li class="chapter" data-level="12.4.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#example"><i class="fa fa-check"></i><b>12.4.2</b> Example</a></li>
<li class="chapter" data-level="12.4.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#bias-and-variance"><i class="fa fa-check"></i><b>12.4.3</b> Bias and Variance</a></li>
<li class="chapter" data-level="12.4.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#reducing-reducible-error"><i class="fa fa-check"></i><b>12.4.4</b> Reducing reducible error</a></li>
<li class="chapter" data-level="12.4.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#error-decomposition"><i class="fa fa-check"></i><b>12.4.5</b> Error decomposition</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection"><i class="fa fa-check"></i><b>12.5</b> Model Selection</a><ul>
<li class="chapter" data-level="12.5.1" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#exercise-cv"><i class="fa fa-check"></i><b>12.5.1</b> Exercise: CV</a></li>
<li class="chapter" data-level="12.5.2" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#out-of-sample-error"><i class="fa fa-check"></i><b>12.5.2</b> Out-of-sample Error</a></li>
<li class="chapter" data-level="12.5.3" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#alternative-measures-of-model-goodness"><i class="fa fa-check"></i><b>12.5.3</b> Alternative measures of model goodness</a></li>
<li class="chapter" data-level="12.5.4" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-and-model-selection-setup"><i class="fa fa-check"></i><b>12.5.4</b> Feature and model selection: setup</a></li>
<li class="chapter" data-level="12.5.5" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#model-selection-1"><i class="fa fa-check"></i><b>12.5.5</b> Model selection</a></li>
<li class="chapter" data-level="12.5.6" data-path="overfitting-the-problem-with-adding-too-many-parameters.html"><a href="overfitting-the-problem-with-adding-too-many-parameters.html#feature-predictor-selection"><i class="fa fa-check"></i><b>12.5.6</b> Feature (predictor) selection</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="describing-relationships.html"><a href="describing-relationships.html"><i class="fa fa-check"></i>Describing Relationships</a></li>
<li class="chapter" data-level="13" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html"><i class="fa fa-check"></i><b>13</b> There’s meaning in parameters</a><ul>
<li class="chapter" data-level="13.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#the-types-of-parametric-assumptions"><i class="fa fa-check"></i><b>13.1</b> The types of parametric assumptions</a><ul>
<li class="chapter" data-level="13.1.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#when-defining-a-model-function."><i class="fa fa-check"></i><b>13.1.1</b> 1. When defining a <strong>model function</strong>.</a></li>
<li class="chapter" data-level="13.1.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#when-defining-probability-distributions."><i class="fa fa-check"></i><b>13.1.2</b> 2. When defining <strong>probability distributions</strong>.</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#the-value-of-making-parametric-assumptions"><i class="fa fa-check"></i><b>13.2</b> The value of making parametric assumptions</a><ul>
<li class="chapter" data-level="13.2.1" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#value-1-reduced-error"><i class="fa fa-check"></i><b>13.2.1</b> Value #1: Reduced Error</a></li>
<li class="chapter" data-level="13.2.2" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#value-2-interpretation"><i class="fa fa-check"></i><b>13.2.2</b> Value #2: Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="theres-meaning-in-parameters.html"><a href="theres-meaning-in-parameters.html#anova"><i class="fa fa-check"></i><b>13.3</b> ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="the-meaning-of-interaction.html"><a href="the-meaning-of-interaction.html"><i class="fa fa-check"></i><b>14</b> The meaning of interaction</a></li>
<li class="chapter" data-level="15" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html"><i class="fa fa-check"></i><b>15</b> Scales and the restricted range problem</a><ul>
<li class="chapter" data-level="15.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#problems"><i class="fa fa-check"></i><b>15.1</b> Problems</a></li>
<li class="chapter" data-level="15.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solutions"><i class="fa fa-check"></i><b>15.2</b> Solutions</a><ul>
<li class="chapter" data-level="15.2.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-1-transformations"><i class="fa fa-check"></i><b>15.2.1</b> Solution 1: Transformations</a></li>
<li class="chapter" data-level="15.2.2" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-2-link-functions"><i class="fa fa-check"></i><b>15.2.2</b> Solution 2: Link Functions</a></li>
<li class="chapter" data-level="15.2.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#solution-3-scientifically-backed-functions"><i class="fa fa-check"></i><b>15.2.3</b> Solution 3: Scientifically-backed functions</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#glms-in-r"><i class="fa fa-check"></i><b>15.3</b> GLM’s in R</a><ul>
<li class="chapter" data-level="15.3.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#broomaugment"><i class="fa fa-check"></i><b>15.3.1</b> <code>broom::augment()</code></a></li>
</ul></li>
<li class="chapter" data-level="15.4" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#options-for-logistic-regression"><i class="fa fa-check"></i><b>15.4</b> Options for Logistic Regression</a><ul>
<li class="chapter" data-level="15.4.1" data-path="scales-and-the-restricted-range-problem.html"><a href="scales-and-the-restricted-range-problem.html#models"><i class="fa fa-check"></i><b>15.4.1</b> Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="improving-estimation-through-distributional-assumptions.html"><a href="improving-estimation-through-distributional-assumptions.html"><i class="fa fa-check"></i><b>16</b> Improving estimation through distributional assumptions</a></li>
<li class="chapter" data-level="17" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html"><i class="fa fa-check"></i><b>17</b> When we only want interpretation on some predictors</a><ul>
<li class="chapter" data-level="17.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability-in-gams"><i class="fa fa-check"></i><b>17.1</b> Non-identifiability in GAMS</a><ul>
<li class="chapter" data-level="17.1.1" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#non-identifiability"><i class="fa fa-check"></i><b>17.1.1</b> Non-identifiability</a></li>
<li class="chapter" data-level="17.1.2" data-path="when-we-only-want-interpretation-on-some-predictors.html"><a href="when-we-only-want-interpretation-on-some-predictors.html#question-1b"><i class="fa fa-check"></i><b>17.1.2</b> Question 1b</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="special-cases.html"><a href="special-cases.html"><i class="fa fa-check"></i>Special cases</a></li>
<li class="chapter" data-level="18" data-path="regression-when-data-are-censored-survival-analysis.html"><a href="regression-when-data-are-censored-survival-analysis.html"><i class="fa fa-check"></i><b>18</b> Regression when data are censored: survival analysis</a></li>
<li class="chapter" data-level="19" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html"><i class="fa fa-check"></i><b>19</b> Regression in the presence of outliers: robust regression</a><ul>
<li class="chapter" data-level="19.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#robust-regression-in-r"><i class="fa fa-check"></i><b>19.1</b> Robust Regression in R</a><ul>
<li class="chapter" data-level="19.1.1" data-path="regression-in-the-presence-of-outliers-robust-regression.html"><a href="regression-in-the-presence-of-outliers-robust-regression.html#heavy-tailed-regression"><i class="fa fa-check"></i><b>19.1.1</b> Heavy Tailed Regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="20" data-path="regression-in-the-presence-of-extremes-extreme-value-regression.html"><a href="regression-in-the-presence-of-extremes-extreme-value-regression.html"><i class="fa fa-check"></i><b>20</b> Regression in the presence of extremes: extreme value regression</a></li>
<li class="chapter" data-level="21" data-path="regression-when-data-are-ordinal.html"><a href="regression-when-data-are-ordinal.html"><i class="fa fa-check"></i><b>21</b> Regression when data are ordinal</a></li>
<li class="chapter" data-level="22" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html"><i class="fa fa-check"></i><b>22</b> Regression when data are missing: multiple imputation</a><ul>
<li class="chapter" data-level="22.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#mean-imputation"><i class="fa fa-check"></i><b>22.1</b> Mean Imputation</a></li>
<li class="chapter" data-level="22.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation"><i class="fa fa-check"></i><b>22.2</b> Multiple Imputation</a><ul>
<li class="chapter" data-level="22.2.1" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#patterns"><i class="fa fa-check"></i><b>22.2.1</b> Patterns</a></li>
<li class="chapter" data-level="22.2.2" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#multiple-imputation-1"><i class="fa fa-check"></i><b>22.2.2</b> Multiple Imputation</a></li>
<li class="chapter" data-level="22.2.3" data-path="regression-when-data-are-missing-multiple-imputation.html"><a href="regression-when-data-are-missing-multiple-imputation.html#pooling"><i class="fa fa-check"></i><b>22.2.3</b> Pooling</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="23" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html"><i class="fa fa-check"></i><b>23</b> Regression under many groups: mixed effects models</a><ul>
<li class="chapter" data-level="23.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#motivation-for-lme"><i class="fa fa-check"></i><b>23.1</b> Motivation for LME</a><ul>
<li class="chapter" data-level="23.1.1" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#definition"><i class="fa fa-check"></i><b>23.1.1</b> Definition</a></li>
<li class="chapter" data-level="23.1.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#r-tools-for-fitting"><i class="fa fa-check"></i><b>23.1.2</b> R Tools for Fitting</a></li>
</ul></li>
<li class="chapter" data-level="23.2" data-path="regression-under-many-groups-mixed-effects-models.html"><a href="regression-under-many-groups-mixed-effects-models.html#mixed-effects-models-in-r-tutorial"><i class="fa fa-check"></i><b>23.2</b> Mixed Effects Models in R: tutorial</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html"><i class="fa fa-check"></i><b>24</b> Regression on an entire distribution: Probabilistic Forecasting</a><ul>
<li class="chapter" data-level="24.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasting-what-it-is"><i class="fa fa-check"></i><b>24.1</b> Probabilistic Forecasting: What it is</a></li>
<li class="chapter" data-level="24.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#review-univariate-distribution-estimates"><i class="fa fa-check"></i><b>24.2</b> Review: Univariate distribution estimates</a><ul>
<li class="chapter" data-level="24.2.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#continuous-response"><i class="fa fa-check"></i><b>24.2.1</b> Continuous response</a></li>
<li class="chapter" data-level="24.2.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discrete-response"><i class="fa fa-check"></i><b>24.2.2</b> Discrete Response</a></li>
</ul></li>
<li class="chapter" data-level="24.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#probabilistic-forecasts-subset-based-learning-methods"><i class="fa fa-check"></i><b>24.3</b> Probabilistic Forecasts: subset-based learning methods</a><ul>
<li class="chapter" data-level="24.3.1" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#the-techniques"><i class="fa fa-check"></i><b>24.3.1</b> The techniques</a></li>
<li class="chapter" data-level="24.3.2" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#exercise-1"><i class="fa fa-check"></i><b>24.3.2</b> Exercise</a></li>
<li class="chapter" data-level="24.3.3" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>24.3.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="24.3.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#evaluating-model-goodness"><i class="fa fa-check"></i><b>24.3.4</b> Evaluating Model Goodness</a></li>
</ul></li>
<li class="chapter" data-level="24.4" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#discussion-points"><i class="fa fa-check"></i><b>24.4</b> Discussion Points</a></li>
<li class="chapter" data-level="24.5" data-path="regression-on-an-entire-distribution-probabilistic-forecasting.html"><a href="regression-on-an-entire-distribution-probabilistic-forecasting.html#when-are-they-not-useful"><i class="fa fa-check"></i><b>24.5</b> When are they not useful?</a></li>
</ul></li>
<li class="chapter" data-level="25" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html"><i class="fa fa-check"></i><b>25</b> Regression when order matters: time series and spatial analysis</a><ul>
<li class="chapter" data-level="25.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#timeseries-in-base-r"><i class="fa fa-check"></i><b>25.1</b> Timeseries in (base) R</a></li>
<li class="chapter" data-level="25.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#spatial-example"><i class="fa fa-check"></i><b>25.2</b> Spatial Example</a></li>
<li class="chapter" data-level="25.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#a-model-for-river-rock-size"><i class="fa fa-check"></i><b>25.3</b> A Model for River Rock Size</a><ul>
<li class="chapter" data-level="25.3.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#average-rock-size"><i class="fa fa-check"></i><b>25.3.1</b> 1. Average rock size:</a></li>
<li class="chapter" data-level="25.3.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-rock-size"><i class="fa fa-check"></i><b>25.3.2</b> 2. Mean rock size:</a></li>
<li class="chapter" data-level="25.3.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#downstream-fining-curve"><i class="fa fa-check"></i><b>25.3.3</b> 3. Downstream fining curve:</a></li>
</ul></li>
<li class="chapter" data-level="25.4" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objectives"><i class="fa fa-check"></i><b>25.4</b> Statistical Objectives</a><ul>
<li class="chapter" data-level="25.4.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#preliminaries-variance-and-correlation"><i class="fa fa-check"></i><b>25.4.1</b> Preliminaries: Variance and Correlation</a></li>
</ul></li>
<li class="chapter" data-level="25.5" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#three-concepts"><i class="fa fa-check"></i><b>25.5</b> Three Concepts</a><ul>
<li class="chapter" data-level="25.5.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#error-variance-sigma_e2leftxright"><i class="fa fa-check"></i><b>25.5.1</b> Error Variance <span class="math inline">\(\sigma_{E}^{2}\left(x\right)\)</span></a></li>
<li class="chapter" data-level="25.5.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-variance-sigma_m2"><i class="fa fa-check"></i><b>25.5.2</b> Mean Variance <span class="math inline">\(\sigma_{M}^{2}\)</span></a></li>
<li class="chapter" data-level="25.5.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#mean-correlation-rholeftdright"><i class="fa fa-check"></i><b>25.5.3</b> Mean Correlation <span class="math inline">\(\rho\left(d\right)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="25.6" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#estimation-1"><i class="fa fa-check"></i><b>25.6</b> Estimation</a><ul>
<li class="chapter" data-level="25.6.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#constant-error-variance"><i class="fa fa-check"></i><b>25.6.1</b> Constant Error Variance</a></li>
<li class="chapter" data-level="25.6.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#non-constant-error-variance"><i class="fa fa-check"></i><b>25.6.2</b> Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="25.7" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-1-downstream-fining-curve"><i class="fa fa-check"></i><b>25.7</b> Statistical Objective 1: Downstream Fining Curve</a><ul>
<li class="chapter" data-level="25.7.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#regression-form"><i class="fa fa-check"></i><b>25.7.1</b> Regression Form</a></li>
</ul></li>
<li class="chapter" data-level="25.8" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#statistical-objective-2-river-profile"><i class="fa fa-check"></i><b>25.8</b> Statistical Objective 2: River Profile</a><ul>
<li class="chapter" data-level="25.8.1" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#simple-kriging"><i class="fa fa-check"></i><b>25.8.1</b> Simple Kriging</a></li>
<li class="chapter" data-level="25.8.2" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#universal-kriging"><i class="fa fa-check"></i><b>25.8.2</b> Universal Kriging</a></li>
<li class="chapter" data-level="25.8.3" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#kriging-under-non-constant-error-variance"><i class="fa fa-check"></i><b>25.8.3</b> Kriging under Non-Constant Error Variance</a></li>
</ul></li>
<li class="chapter" data-level="25.9" data-path="regression-when-order-matters-time-series-and-spatial-analysis.html"><a href="regression-when-order-matters-time-series-and-spatial-analysis.html#confidence-intervals-of-the-river-profile"><i class="fa fa-check"></i><b>25.9</b> Confidence Intervals of the River Profile</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/vincenzocoia/Interpreting-Regression" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Interpreting Regression</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="overfitting-the-problem-with-adding-too-many-parameters" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Overfitting: The problem with adding too many parameters</h1>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<p>(Reducible Error)</p>
<p>(BAIT 509 Class Meeting 02)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">suppressPackageStartupMessages</span>(<span class="kw">library</span>(tidyverse))</code></pre></div>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;tibble&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;purrr&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;dplyr&#39; was built under R version 3.5.2</code></pre>
<pre><code>## Warning: package &#39;stringr&#39; was built under R version 3.5.2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Wage &lt;-<span class="st"> </span>ISLR::Wage
NCI60 &lt;-<span class="st"> </span>ISLR::NCI60
baseball &lt;-<span class="st"> </span>Lahman::Teams %&gt;%<span class="st"> </span>tbl_df %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">select</span>(<span class="dt">runs=</span>R, <span class="dt">hits=</span>H)
cow &lt;-<span class="st"> </span><span class="kw">suppressMessages</span>(<span class="kw">read_csv</span>(<span class="st">&quot;data/milk_fat.csv&quot;</span>))
esoph &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(esoph) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">agegp =</span> <span class="kw">as.character</span>(agegp))
titanic &lt;-<span class="st"> </span><span class="kw">na.omit</span>(titanic::titanic_train)</code></pre></div>
<div id="classification-exercise-do-together" class="section level2">
<h2><span class="header-section-number">12.1</span> Classification Exercise: Do Together</h2>
<p>Let’s use the <code>Default</code> data in the <code>ISLR</code> R package for classification: predict whether or not a new individual will default on their credit card debt.</p>
<p>Our task: make the best prediction model we can.</p>
<ol style="list-style-type: decimal">
<li>Code the null model.
<ul>
<li>What would you predict for a new person?</li>
<li>What’s the error using the original data (aka training data)?</li>
</ul></li>
<li>Plot all the data, and come up with a classifier by eye.
<ul>
<li>What would you predict for a non-student with income $40,000 and balance $1,011?</li>
<li>Classify the original (training) data. What’s the overall error?</li>
</ul></li>
<li>How might we optimize the classifier? Let’s code it.</li>
</ol>
<p>Discussion: does it make sense to classify a new iris plant species based on the <code>iris</code> dataset?</p>
</div>
<div id="training-error-vs.generalization-error" class="section level2">
<h2><span class="header-section-number">12.2</span> Training Error vs. Generalization Error</h2>
<p>Discussion:</p>
<ol style="list-style-type: decimal">
<li>What’s wrong with calculating error on the training data?</li>
<li>What’s an alternative way to compute error?</li>
</ol>
<p>Activity: Get the generalization error for the above classifier.</p>
</div>
<div id="model-complexity" class="section level2">
<h2><span class="header-section-number">12.3</span> Model complexity</h2>
<p>A big part of machine learning is choosing how complex to make a model. Examples:</p>
<ul>
<li>More partitions in the above credit card default classifier</li>
<li>Adding higher order polynomials in linear regression</li>
</ul>
<p>And lots more that we’ll see when we explore more machine learning models.</p>
<p><strong>The difficulty</strong>: adding more complexity will decrease training error. What’s a way of dealing with this?</p>
<div id="activity" class="section level3">
<h3><span class="header-section-number">12.3.1</span> Activity</h3>
<p>Let’s return to the <code>iris</code> example from lecture 1, predicting Sepal Width, this time using Petal Width only, using polynomial linear regression.</p>
<p><strong>Task</strong>: Choose an optimal polynomial order.</p>
<p>Here’s code for a graph to visualize the model fit:</p>
<pre><code>p &lt;- 4
ggplot(iris, aes(Petal.Width, Sepal.Width)) +
    geom_point(alpha = 0.2) +
    geom_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, p)) +
    theme_bw()</code></pre>
</div>
</div>
<div id="reducible-error" class="section level2">
<h2><span class="header-section-number">12.4</span> Reducible Error</h2>
<div id="what-is-it" class="section level3">
<h3><span class="header-section-number">12.4.1</span> What is it?</h3>
<p>Last time, we saw what irreducible error is, and how to “beat” it.</p>
<p>The other type of error is <strong>reducible error</strong>, which arises from not knowing the true distribution of the data (or some aspect of it, such as the mean or mode). We therefore have to <em>estimate</em> this. Error in this estimation is known as the reducible error.</p>
</div>
<div id="example" class="section level3">
<h3><span class="header-section-number">12.4.2</span> Example</h3>
<ul>
<li>one numeric predictor</li>
<li>one numeric response</li>
<li>true (unknown) distribution of the data is <span class="math inline">\(Y|X=x \sim N(5/x, 1)\)</span> (and take <span class="math inline">\(X \sim 1+Exp(1)\)</span>).</li>
<li>you only see the following 100 observations stored in <code>dat</code>, plotted below, and choose to use linear regression as a model:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">400</span>)
n &lt;-<span class="st"> </span><span class="dv">100</span>
dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(
    <span class="dt">x =</span> <span class="kw">rexp</span>(n) +<span class="st"> </span><span class="dv">1</span>,
    <span class="dt">y =</span> <span class="kw">rnorm</span>(n, <span class="dt">mean =</span> <span class="dv">5</span>/x)
)
<span class="kw">ggplot</span>(dat, <span class="kw">aes</span>(x, y)) +
<span class="st">    </span><span class="kw">stat_smooth</span>(<span class="dt">method=</span><span class="st">&quot;lm&quot;</span>, <span class="dt">se=</span><span class="ot">FALSE</span>, <span class="dt">size=</span><span class="fl">0.5</span>,
                <span class="dt">mapping=</span><span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;Estimate&quot;</span>)) +
<span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun=</span>function(x) <span class="dv">5</span>/x,
                  <span class="dt">mapping=</span><span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;True mean&quot;</span>)) +
<span class="st">    </span><span class="kw">scale_colour_brewer</span>(<span class="st">&quot;&quot;</span>, <span class="dt">palette=</span><span class="st">&quot;Dark2&quot;</span>) +
<span class="st">    </span><span class="kw">theme_bw</span>() +
<span class="st">    </span>rotate_y</code></pre></div>
<p><img src="100-The_problem_with_adding_too_many_parameters__files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The difference between the true curve and the estimated curve is due to reducible error.</p>
<p>In the classification setting, a misidentification of the mode is due to reducible error.</p>
<p>(<strong>Why the toy data set instead of real ones?</strong> Because I can embed characteristics into the data for pedagogical reasons. You’ll see real data at least in the assignments and final project.)</p>
</div>
<div id="bias-and-variance" class="section level3">
<h3><span class="header-section-number">12.4.3</span> Bias and Variance</h3>
<p>There are two key aspects to reducible error: <strong>bias</strong> and <strong>variance</strong>. They only make sense in light of the hypothetical situation of building a model/forecaster over and over again as we generate a new data set over and over again.</p>
<ul>
<li><strong>Bias</strong> occurs when your estimates are systematically different from the truth. For regression, this means that the estimated mean is either usually bigger or usually smaller than the true mean. For a classifier, it’s the systematic tendency to choosing an incorrect mode.</li>
<li><strong>Variance</strong> refers to the variability of your estimates.</li>
</ul>
<p>There is usually (always?) a tradeoff between bias and variance. It’s referred to as the <strong>bias/variance tradeoff</strong>, and we’ll see examples of this later.</p>
<p>Let’s look at the above linear regression example again. I’ll generate 100 data sets, and fit a linear regression for each:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">400</span>)
n &lt;-<span class="st"> </span><span class="dv">100</span>
N &lt;-<span class="st"> </span><span class="dv">100</span>
xgrid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">6</span>, <span class="dt">length.out=</span><span class="dv">100</span>)) +<span class="st"> </span><span class="dv">1</span>
## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop
bias_plot &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">iter=</span><span class="dv">1</span>:N) %&gt;%<span class="st"> </span><span class="kw">group_by</span>(iter) %&gt;%<span class="st"> </span><span class="kw">do</span>({
    dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x=</span><span class="kw">rexp</span>(n)+<span class="dv">1</span>, 
                  <span class="dt">y=</span><span class="dv">5</span>/x+<span class="kw">rnorm</span>(n))
    <span class="kw">data.frame</span>(
        .,
        xgrid,
        <span class="dt">Linear =</span> <span class="kw">predict</span>(<span class="kw">lm</span>(y~x, <span class="dt">data=</span>dat),
                         <span class="dt">newdata=</span>xgrid)
    )
}) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>Linear)) +
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group=</span>iter, <span class="dt">colour=</span><span class="st">&quot;Estimates&quot;</span>), <span class="dt">alpha=</span><span class="fl">0.1</span>) +
<span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun=</span>function(x) <span class="dv">5</span>/x,
                  <span class="dt">mapping=</span><span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;True mean&quot;</span>)) +
<span class="st">    </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">scale_colour_brewer</span>(<span class="st">&quot;&quot;</span>, <span class="dt">palette=</span><span class="st">&quot;Dark2&quot;</span>) +
<span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;y&quot;</span>) +<span class="st"> </span>rotate_y
bias_plot</code></pre></div>
<p><img src="100-The_problem_with_adding_too_many_parameters__files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The <em>spread</em> of the linear regression estimates is the variance; the difference between the <em>center of the regression lines</em> and the true mean curve is the bias.</p>
</div>
<div id="reducing-reducible-error" class="section level3">
<h3><span class="header-section-number">12.4.4</span> Reducing reducible error</h3>
<p>As the name suggests, we can reduce reducible error. Exactly how depends on the machine learning method, but in general:</p>
<ul>
<li>We can reduce variance by increasing the sample size, and adding more model assumptions.</li>
<li>We can reduce bias by being less strict with model assumptions, OR by specifying them to be closer to the truth (which we never know).</li>
</ul>
<p>Consider the above regression example again. Notice how my estimates tighten up when they’re based on a larger sample size (1000 here, instead of 100):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">400</span>)
n &lt;-<span class="st"> </span><span class="dv">1000</span>
N &lt;-<span class="st"> </span><span class="dv">100</span>
xgrid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">6</span>, <span class="dt">length.out=</span><span class="dv">100</span>)) +<span class="st"> </span><span class="dv">1</span>
## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop
<span class="kw">tibble</span>(<span class="dt">iter=</span><span class="dv">1</span>:N) %&gt;%<span class="st"> </span><span class="kw">group_by</span>(iter) %&gt;%<span class="st"> </span><span class="kw">do</span>({
    dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x=</span><span class="kw">rexp</span>(n)+<span class="dv">1</span>, 
                  <span class="dt">y=</span><span class="dv">5</span>/x+<span class="kw">rnorm</span>(n))
    <span class="kw">data.frame</span>(
        .,
        xgrid,
        <span class="dt">Linear =</span> <span class="kw">predict</span>(<span class="kw">lm</span>(y~x, <span class="dt">data=</span>dat),
                         <span class="dt">newdata=</span>xgrid)
    )
}) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span>x, <span class="dt">y=</span>Linear)) +
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group=</span>iter, <span class="dt">colour=</span><span class="st">&quot;Estimates&quot;</span>), <span class="dt">alpha=</span><span class="fl">0.1</span>) +
<span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun=</span>function(x) <span class="dv">5</span>/x,
                  <span class="dt">mapping=</span><span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;True mean&quot;</span>)) +
<span class="st">    </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">scale_colour_brewer</span>(<span class="st">&quot;&quot;</span>, <span class="dt">palette=</span><span class="st">&quot;Dark2&quot;</span>) +
<span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;y&quot;</span>) +<span class="st"> </span>rotate_y</code></pre></div>
<p><img src="100-The_problem_with_adding_too_many_parameters__files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Notice how, after fitting the linear regression <span class="math inline">\(E(Y|X=x)=\beta_0 + \beta_1 (1/x)\)</span> (which is a <em>correct</em> model assumption), the regression estimates are centered around the truth – that is, they are unbiased:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">400</span>)
n &lt;-<span class="st"> </span><span class="dv">100</span>
N &lt;-<span class="st"> </span><span class="dv">100</span>
xgrid &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">xinv=</span>(<span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">6</span>, <span class="dt">length.out=</span><span class="dv">100</span>))) +<span class="st"> </span><span class="dv">1</span>
## Use &quot;tibble %&gt;% group_by %&gt;% do&quot; in place of `for` loop
<span class="kw">tibble</span>(<span class="dt">iter=</span><span class="dv">1</span>:N) %&gt;%<span class="st"> </span><span class="kw">group_by</span>(iter) %&gt;%<span class="st"> </span><span class="kw">do</span>({
    dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x=</span><span class="kw">rexp</span>(n)+<span class="dv">1</span>, 
                  <span class="dt">xinv=</span><span class="dv">1</span>/x,
                  <span class="dt">y=</span><span class="dv">5</span>/x+<span class="kw">rnorm</span>(n))
    <span class="kw">data.frame</span>(
        .,
        xgrid,
        <span class="dt">Linear =</span> <span class="kw">predict</span>(<span class="kw">lm</span>(y~xinv, <span class="dt">data=</span>dat),
                         <span class="dt">newdata=</span>xgrid)
    )
}) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x=</span><span class="dv">1</span>/xinv, <span class="dt">y=</span>Linear)) +
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group=</span>iter, <span class="dt">colour=</span><span class="st">&quot;Estimates&quot;</span>), <span class="dt">alpha=</span><span class="fl">0.1</span>) +
<span class="st">    </span><span class="kw">stat_function</span>(<span class="dt">fun=</span>function(x) <span class="dv">5</span>/x,
                  <span class="dt">mapping=</span><span class="kw">aes</span>(<span class="dt">colour=</span><span class="st">&quot;True mean&quot;</span>)) +
<span class="st">    </span><span class="kw">theme_bw</span>() +
<span class="st">    </span><span class="kw">scale_colour_brewer</span>(<span class="st">&quot;&quot;</span>, <span class="dt">palette=</span><span class="st">&quot;Dark2&quot;</span>) +
<span class="st">    </span><span class="kw">ylab</span>(<span class="st">&quot;y&quot;</span>) +<span class="st"> </span>rotate_y +
<span class="st">    </span><span class="kw">xlab</span>(<span class="st">&quot;x&quot;</span>)</code></pre></div>
<p><img src="100-The_problem_with_adding_too_many_parameters__files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="error-decomposition" class="section level3">
<h3><span class="header-section-number">12.4.5</span> Error decomposition</h3>
<p>We saw that we measure error using mean squared error (MSE) in the case of regression, and the error rate in the case of a classifier. These both contain all errors: irreducible error, bias, and variance:</p>
<p>MSE = bias^2 + variance + irreducible variance</p>
<p>A similar decomposition for error rate exists.</p>
<p><strong>Note</strong>: If you look online, the MSE is often defined as the expected squared difference between a parameter and its estimate, in which case the “irreducible error” is not present. We’re taking MSE to be the expected squared distance between a true “new” observation and our prediction (mean estimate).</p>
</div>
</div>
<div id="model-selection" class="section level2">
<h2><span class="header-section-number">12.5</span> Model Selection</h2>
<p><strong>Caution: in a highly developmental stage! See Section <a href="index.html#caution">1.1</a>.</strong></p>
<p>(BAIT 509 Class Meeting 04)</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="st">&quot;orange&quot;</span> &lt;-<span class="st"> &quot;#d95f02&quot;</span>
rotate_y &lt;-<span class="st"> </span><span class="kw">theme</span>(<span class="dt">axis.title.y=</span><span class="kw">element_text</span>(<span class="dt">angle=</span><span class="dv">0</span>, <span class="dt">vjust=</span><span class="fl">0.5</span>))</code></pre></div>
<div id="exercise-cv" class="section level3">
<h3><span class="header-section-number">12.5.1</span> Exercise: CV</h3>
<p>k-fold cross validation with <code>caret::train()</code> in R. See <a href="https://machinelearningmastery.com/how-to-estimate-model-accuracy-in-r-using-the-caret-package">this resource</a></p>
<p>In python, can use <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score"><code>sklearn.model_selection.cross_val_score</code></a>.</p>
</div>
<div id="out-of-sample-error" class="section level3">
<h3><span class="header-section-number">12.5.2</span> Out-of-sample Error</h3>
<div id="the-fundamental-problem" class="section level4">
<h4><span class="header-section-number">12.5.2.1</span> The fundamental problem</h4>
<p>First, some terminology: The data that we use to fit a model is called <strong>training data</strong>, and the fitting procedure is called <strong>training</strong>. New data (or at least, data <em>not</em> used in the training process) is called <strong>test data</strong>.</p>
<p>The goal of supervised learning is to build a model that has low error on <em>new</em> (test) data.</p>
<p>*** A fundamental fact of supervised learning is that the error on the training data will (on average) be <strong>better</strong> (lower) than the error on new data!</p>
<p>More terminology: <strong>training error</strong> and <strong>test error</strong> are errors computed on the respective data sets. Often, the test error is called <strong>generalization error</strong>.</p>
<p>Let’s check using loess on an artificial data set (from last time). Here’s the training error (MSE):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">87</span>)
n &lt;-<span class="st"> </span><span class="dv">200</span>
dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>(n/<span class="dv">2</span>), <span class="kw">rnorm</span>(n/<span class="dv">2</span>)+<span class="dv">5</span>)-<span class="dv">3</span>,
              <span class="dt">y =</span> <span class="kw">sin</span>(x^<span class="dv">2</span>/<span class="dv">5</span>)/x +<span class="st"> </span><span class="kw">rnorm</span>(n)/<span class="dv">10</span> +<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
fit &lt;-<span class="st"> </span><span class="kw">loess</span>(y ~<span class="st"> </span>x, <span class="dt">data=</span>dat, <span class="dt">span=</span><span class="fl">0.3</span>)
yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit)
<span class="kw">mean</span>((yhat -<span class="st"> </span>dat$y)^<span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.009599779</code></pre>
<p>Here’s the test error:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">1000</span>
newdat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>(n/<span class="dv">2</span>), <span class="kw">rnorm</span>(n/<span class="dv">2</span>)+<span class="dv">5</span>)-<span class="dv">3</span>,
                 <span class="dt">y =</span> <span class="kw">sin</span>(x^<span class="dv">2</span>/<span class="dv">5</span>)/x +<span class="st"> </span><span class="kw">rnorm</span>(n)/<span class="dv">10</span> +<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, <span class="dt">newdata =</span> newdat)
<span class="kw">mean</span>((yhat -<span class="st"> </span>newdat$y)^<span class="dv">2</span>, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.0112968</code></pre>
<p>If you think this was due to luck, go ahead and try changing the seed – more often than not, you’ll see the test error &gt; training error.</p>
<p>This fundamental problem exists because, by definition, we build the model to be optimal based on the training data! For example, kNN and loess make a prediction that’s <em>as close as possible</em> to the training data.</p>
<p>The more we try to make the model fit the training data – i.e., the more we overfit the data – the worse the problem gets. Let’s reduce the loess bandwidth to emulate this effect. Here’s the training error:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">87</span>)
n &lt;-<span class="st"> </span><span class="dv">200</span>
dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>(n/<span class="dv">2</span>), <span class="kw">rnorm</span>(n/<span class="dv">2</span>)+<span class="dv">5</span>)-<span class="dv">3</span>,
              <span class="dt">y =</span> <span class="kw">sin</span>(x^<span class="dv">2</span>/<span class="dv">5</span>)/x +<span class="st"> </span><span class="kw">rnorm</span>(n)/<span class="dv">10</span> +<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
fit &lt;-<span class="st"> </span><span class="kw">loess</span>(y ~<span class="st"> </span>x, <span class="dt">data=</span>dat, <span class="dt">span=</span><span class="fl">0.1</span>)
yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit)
<span class="kw">mean</span>((yhat -<span class="st"> </span>dat$y)^<span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 0.008518578</code></pre>
<p>Test error:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">n &lt;-<span class="st"> </span><span class="dv">1000</span>
newdat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>(n/<span class="dv">2</span>), <span class="kw">rnorm</span>(n/<span class="dv">2</span>)+<span class="dv">5</span>)-<span class="dv">3</span>,
                 <span class="dt">y =</span> <span class="kw">sin</span>(x^<span class="dv">2</span>/<span class="dv">5</span>)/x +<span class="st"> </span><span class="kw">rnorm</span>(n)/<span class="dv">10</span> +<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
yhat &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, <span class="dt">newdata =</span> newdat)
<span class="kw">mean</span>((yhat -<span class="st"> </span>newdat$y)^<span class="dv">2</span>, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] 0.01233726</code></pre>
<p>The effect gets even worse if we have less training data.</p>
<p>For kNN and loess, we can play with the hyperparameter, weight function, and degree of local polynomial (in the case of regression) to try and avoid overfitting. Playing with these things is often called <strong>tuning</strong>.</p>
</div>
<div id="solution-1-use-a-hold-out-set." class="section level4">
<h4><span class="header-section-number">12.5.2.2</span> Solution 1: Use a hold-out set.</h4>
<p>One solution is to split the data into two parts: <strong>training</strong> and <strong>validation</strong> data. The validation set is called a <em>hold-out set</em>, because we’re holding it out in the model training.</p>
<p>Then, we can tune the model (such as choosing the <span class="math inline">\(k\)</span> in kNN or <span class="math inline">\(r\)</span> in loess) to minimize error <em>on the validation set</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">87</span>)
n &lt;-<span class="st"> </span><span class="dv">200</span>
dat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>(n/<span class="dv">2</span>), <span class="kw">rnorm</span>(n/<span class="dv">2</span>)+<span class="dv">5</span>)-<span class="dv">3</span>,
              <span class="dt">y =</span> <span class="kw">sin</span>(x^<span class="dv">2</span>/<span class="dv">5</span>)/x +<span class="st"> </span><span class="kw">rnorm</span>(n)/<span class="dv">10</span> +<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
n &lt;-<span class="st"> </span><span class="dv">1000</span>
newdat &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">x =</span> <span class="kw">c</span>(<span class="kw">rnorm</span>(n/<span class="dv">2</span>), <span class="kw">rnorm</span>(n/<span class="dv">2</span>)+<span class="dv">5</span>)-<span class="dv">3</span>,
                 <span class="dt">y =</span> <span class="kw">sin</span>(x^<span class="dv">2</span>/<span class="dv">5</span>)/x +<span class="st"> </span><span class="kw">rnorm</span>(n)/<span class="dv">10</span> +<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>))
<span class="kw">tibble</span>(<span class="dt">r =</span> <span class="kw">seq</span>(<span class="fl">0.05</span>, <span class="fl">0.7</span>, <span class="dt">length.out=</span><span class="dv">100</span>)) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">group_by</span>(r) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">do</span>({
        this_r &lt;-<span class="st"> </span>.$r
        fit &lt;-<span class="st"> </span><span class="kw">loess</span>(y ~<span class="st"> </span>x, <span class="dt">data=</span>dat, <span class="dt">span=</span>this_r)
        yhat_tr  &lt;-<span class="st"> </span><span class="kw">predict</span>(fit)
        yhat_val &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, <span class="dt">newdata =</span> newdat)
        <span class="kw">data.frame</span>(
            <span class="dt">r =</span> this_r,
            <span class="dt">training =</span> <span class="kw">mean</span>((yhat_tr -<span class="st"> </span>dat$y)^<span class="dv">2</span>),
            <span class="dt">validation =</span> <span class="kw">mean</span>((yhat_val -<span class="st"> </span>newdat$y)^<span class="dv">2</span>, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)
        )
    }) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">gather</span>(<span class="dt">key=</span><span class="st">&quot;set&quot;</span>, <span class="dt">value=</span><span class="st">&quot;mse&quot;</span>, training, validation) %&gt;%<span class="st"> </span>
<span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(r, mse)) +
<span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">group=</span>set, <span class="dt">colour=</span>set)) +
<span class="st">    </span><span class="kw">theme_bw</span>()</code></pre></div>
<pre><code>## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
## parametric, : k-d tree limited by memory. ncmax= 200

## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
## parametric, : k-d tree limited by memory. ncmax= 200

## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
## parametric, : k-d tree limited by memory. ncmax= 200

## Warning in simpleLoess(y, x, w, span, degree = degree, parametric =
## parametric, : k-d tree limited by memory. ncmax= 200</code></pre>
<p><img src="100-The_problem_with_adding_too_many_parameters__files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>We would choose a bandwidth (<span class="math inline">\(r\)</span>) of approximately 0.35, because the error on the validation set is smallest.</p>
<p>Notice from this plot:</p>
<ul>
<li>The training error is lower than the out-of-sample error.</li>
<li>We can make the training error arbitrarily small by decreasing <span class="math inline">\(r\)</span>.</li>
<li>The out-of-sample error decreases, and then starts to increase again.
<ul>
<li>NOTE: This doesn’t <em>always</em> happen, as you’ll see in Assignment 1. But it usually does.</li>
</ul></li>
</ul>
<p>After choosing the model that gives the smallest error on the validation set, then the <em>validation error</em> is also going to be on average lower than in a test set – that is, if we get even more data! The more tuning parameters we optimize using a validation set, the more pronounced this effect will be. Two things to note from this:</p>
<ol style="list-style-type: decimal">
<li>This is not as bad as the original problem (where the training error is less than the test error), because the tuning parameters are still chosen on an out-of-sample set.</li>
<li>If we want to use the validation error as an estimate of the out-of-sample error, we just have to be mindful of the fact that this is an optimistic estimate of the generalization error.</li>
</ol>
<p>If you wanted an unbiased estimate of generalization error, you can start your procedure by splitting your data into three sets: training and validation as before, but also a test set that is <strong>never touched until you’ve claimed a final model</strong>! You only use the test set to get an unbiased estimate of generalization error.</p>
<p>There’s not really a standard choice for deciding <em>how much</em> data to put aside for each set, but something like 60% training, 20% validation, and 20% test is generally acceptable.</p>
</div>
<div id="solution-2-cross-validation" class="section level4">
<h4><span class="header-section-number">12.5.2.3</span> Solution 2: Cross-validation</h4>
<p>The problem with the training-validation-test set approach is that you’re wasting a lot of data – lots of data are not being used in training! Another problem is that it’s not easy to choose how much data to put aside for each set.</p>
<p>A solution is to use (<span class="math inline">\(c\)</span>-fold) <strong>cross validation</strong> (CV), which can be used to estimate out-of-sample error, and to choose tuning parameters. (Note that usually people refer to this as <span class="math inline">\(k\)</span>-fold cross validation, but I don’t want to overload <span class="math inline">\(k\)</span> from kNN!) <span class="math inline">\(c=10\)</span> is generally accepted as the defacto standard. Taking <span class="math inline">\(c\)</span> equal to the sample size is a special case called leave-one-out cross validation.</p>
<p>The general procedure is as follows:</p>
<ol style="list-style-type: decimal">
<li>Partition the data into <span class="math inline">\(c\)</span> (approximately equal) chunks.</li>
<li>Hold out chunk 1; train the model on the other <span class="math inline">\(c-1\)</span> chunks; calculate error on the held-out chunk.</li>
<li>Hold out chunk 2; train the model on the other <span class="math inline">\(c-1\)</span> chunks; calculate error on the held-out chunk.</li>
<li>Hold out chunk 3; train the model on the other <span class="math inline">\(c-1\)</span> chunks; calculate error on the held-out chunk.</li>
<li>etc., until you’ve held out each chunk exactly once.</li>
<li>Average the <span class="math inline">\(c\)</span> errors to get an estimate of the generalization error.</li>
</ol>
<p>You can then repeat this procedure for different values of the tuning parameters, choosing values that give the lowest error. Once you choose this tuning parameter, go ahead and use <em>all</em> the data as training data, with the selected tuning parameters.</p>
<p>CV is generally preferred to the hold-out set method, because we can fit a model that has overall lower error, but it’s computationally expensive.</p>
</div>
</div>
<div id="alternative-measures-of-model-goodness" class="section level3">
<h3><span class="header-section-number">12.5.3</span> Alternative measures of model goodness</h3>
<p>The coefficient of determination (<span class="math inline">\(R^2\)</span>) can be calculated whenever it makes sense to calculate MSE. It equals: <span class="math display">\[ R^2 = 1 - \frac{\text{MSE of your model}}{\text{MSE of the model that always predicts } \bar{y}}. \]</span> This number lies between 0 and 1, where a 1 represents perfect prediction on the set that you’re computing <span class="math inline">\(R^2\)</span> with.</p>
<p>When we have a distributional assumption (such as Gaussian errors), we can calculate the likelihood – or more often, the negative log likelihood (<span class="math inline">\(\ell\)</span>). If the density/mass function of <span class="math inline">\(y_i\)</span> is <span class="math inline">\(f_i\)</span>, and we have <span class="math inline">\(n\)</span> observations, then the negative log likelihood is <span class="math display">\[ \ell = -\sum_{i=1}^{n} \log(f_i(y_i)). \]</span></p>
</div>
<div id="feature-and-model-selection-setup" class="section level3">
<h3><span class="header-section-number">12.5.4</span> Feature and model selection: setup</h3>
<p>For supervised learning, we seek a model that gives us the lowest generalization error as possible. This involves two aspects:</p>
<ol style="list-style-type: decimal">
<li>Reduce the irreducible error.
<ul>
<li>This involves <strong>feature engineering</strong> and <strong>feature selection</strong>: finding and choosing predictors that give us as much information about the response as we can get.</li>
</ul></li>
<li>Reduce the reducible error (= bias &amp; variance)
<ul>
<li>This involves <strong>modelling</strong> and <strong>tuning</strong>, so that we can extract the information that the predictors hold about the response as best as we can. The better our model, the lower our reducible error is.</li>
<li>This has been the main focus of BAIT 509, via models such as loess, kNN, random forests, SVM, etc.</li>
</ul></li>
</ol>
<p>Recall for (2) that we avoid overfitting by tuning (choosing hyperparameters, such as <span class="math inline">\(k\)</span> in kNN) to optimize generalization error. We estimate generalization error either using the validation set approach, cross validation, or the out-of-bag approach for bagging.</p>
<p>The same thing applies to choosing features/predictors and choosing models, although model selection has a few extra components that should be considered.</p>
</div>
<div id="model-selection-1" class="section level3">
<h3><span class="header-section-number">12.5.5</span> Model selection</h3>
<p>The question here is, what supervised learning method should you use? There are a few things you should consider.</p>
<ol style="list-style-type: decimal">
<li>Quantitative choice</li>
</ol>
<p>Suppose you’ve gone ahead and fit your best random forest model, kNN model, linear regression model, etc. Which do you choose? You should have estimated the generalization error for each model (for example, on a validation set) – choose the one that gives the lowest error.</p>
<p>You might find that some models have roughly the same error. In this case, feel free to use all of these to make predictions. You can either look at all predictions, or take an average of the model outputs (called <strong>model averaging</strong>). Considering all models may be quite informative, though – for example, if all models are suggesting the same thing for a new case, then the decision is clearer than if they all say different things.</p>
<ol start="2" style="list-style-type: decimal">
<li>Qualitative choice</li>
</ol>
<p>Sometimes, after exploring the data, it makes sense to add model assumptions. For example, perhaps your response looks linear in your predictors. If so, it may be reasonable to assume linearity, and fit a linear regression model.</p>
<p>Note that adding assumptions like this generally reduce the variance in your model fit – but is prone to bias if the assumption is far from the truth. As usual, adding assumptions is about reducing the bias-variance tradeoff.</p>
<ol start="3" style="list-style-type: decimal">
<li>Human choice (interpretability)</li>
</ol>
<p>Sometimes it’s helpful for a model to be interpretable. For example, the slopes in linear regression hold meaning; odds ratios in logistic regression hold meaning; nodes in a decision tree have meaning. If this is the case, then interpretability should also be considered.</p>
</div>
<div id="feature-predictor-selection" class="section level3">
<h3><span class="header-section-number">12.5.6</span> Feature (predictor) selection</h3>
<p>Recall that, when tuning a supervised learning method (such as choosing <span class="math inline">\(k\)</span> in kNN), we can make the training error arbitrarily small – but this results in overfitting the training data. The same thing applies to the number of predictors you add.</p>
<p>Here’s an example. I’ll generate 100 observations of 1 response and 99 predictor variables totally randomly, fit a linear regression model with all the predictors, and calculate MSE:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">38</span>)
dat &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">100</span>*<span class="dv">100</span>), <span class="dt">ncol=</span><span class="dv">100</span>))
<span class="kw">names</span>(dat)[<span class="dv">1</span>] &lt;-<span class="st"> &quot;y&quot;</span>
fit &lt;-<span class="st"> </span><span class="kw">lm</span>(y~., <span class="dt">data=</span>dat)
<span class="kw">mean</span>((dat$y -<span class="st"> </span><span class="kw">predict</span>(fit))^<span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 7.519591e-29</code></pre>
<p>The MSE is 0 (up to computational precision) – the response is perfectly predicted on the training set.</p>
<p>If we consider the number of predictors as a tuning parameter, then we can optimize this by estimating generalization error, as usual.</p>
<p>But there are approaches that we can use that’s specific to feature selection, that we’ll discuss next. You are not expected to apply these for your project! This is just for your information.</p>
<div id="specialized-metrics-for-feature-selection" class="section level4">
<h4><span class="header-section-number">12.5.6.1</span> Specialized metrics for feature selection</h4>
<p><em>You are not required to use this method for your project.</em></p>
<p>Using these specialized metrics, we don’t need to bother holding out data to estimate generalization error: they have a penalty built into them based on the number of predictors that the model uses.</p>
<ul>
<li>The <strong>adjusted <span class="math inline">\(R^2\)</span></strong> is a modified version of <span class="math inline">\(R^2\)</span>.</li>
<li>The <strong>AIC</strong> and <strong>BIC</strong> are modified versions of the negative log likelihood.</li>
</ul>
<p>There are others, like Mallows’ <span class="math inline">\(C_p\)</span>.</p>
<p>Optimize these on the training data – they’re designed to (try to) prevent overfitting.</p>
<p>But, with <span class="math inline">\(p\)</span> predictors, we’d have <span class="math inline">\(2^p\)</span> models to calculate these statistics for! That’s <em>a lot</em> of models when <span class="math inline">\(p\)</span> is not even all that large. Here’s how the number of models grows as <span class="math inline">\(p\)</span> increases:</p>
<p><img src="100-The_problem_with_adding_too_many_parameters__files/figure-html/unnamed-chunk-14-1.png" width="480" style="display: block; margin: auto;" /></p>
<p>For 10 predictors, that’s 1000 models. For 20 predictors, that’s over 1,000,000.</p>
</div>
<div id="greedy-selection" class="section level4">
<h4><span class="header-section-number">12.5.6.2</span> Greedy Selection</h4>
<p><em>You are not required to use this method for your project.</em></p>
<p>Instead of fitting all models, we can take a “greedy approach”. This may not result in the optimal model, but the hope is that we get close. One of three methods are typically used:</p>
<ol style="list-style-type: decimal">
<li>Forward Selection</li>
</ol>
<p>The idea here is to start with the null model: no predictors. Then, add one predictor at a time, each time choosing the best one in terms of generalization error OR in terms of one of the specialized measures discussed above. Sometimes, a hypothesis test is used to determine whether the addition of the predictor is significant enough.</p>
<ol start="2" style="list-style-type: decimal">
<li>Backward Selection</li>
</ol>
<p>The idea here is to start with the full model: all predictors. Then, gradually remove predictors that are either insignificant according to a hypothesis test, or that gives the greatest reduction in one of the specialized measures discussed above.</p>
<ol start="3" style="list-style-type: decimal">
<li>Stepwise Selection</li>
</ol>
<p>The idea here is to combine forward and backward selection. Instead of only adding or only removing predictors, we can consider either at each iteration: adding or removing.</p>
</div>
<div id="regularization" class="section level4">
<h4><span class="header-section-number">12.5.6.3</span> Regularization</h4>
<p><em>You are not required to use this method for your project.</em></p>
<p>When training a model, we can write the training procedure as the optimization of a <strong>loss function</strong>. For example, in regression, we want to minimize the sum of squared errors.</p>
<p><strong>Regularization</strong> adds a penalty directly to this loss function, that grows as the number of predictors grows. This is in contrast to the specialized measures (like adjusted <span class="math inline">\(R^2\)</span>) that adds the penalty to the error term <em>after</em> the model is fit. There are different types of regularization, but typically those that involve an L1 regularizer are used in feature selection.</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="estimating-assumption-free-the-world-of-supervised-learning-techniques.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="describing-relationships.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/vincenzocoia/Interpreting-Regression/edit/master/100-The_problem_with_adding_too_many_parameters_.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["Interpreting-Regression.pdf", "Interpreting-Regression.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
