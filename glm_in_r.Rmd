---
title: "GLM's in R"
output: html_document
---

This document introduces the `glm` function in R for fitting a Generlized Linear Model (GLM). We'll work with the `titanic_train` dataset in the `titanic` package.

```{r}
library(ggplot2)
library(titanic)
dat <- na.omit(titanic_train)
str(dat)
```

## Binomial Regression

Consider the regression of `Survived` on `Age`. Let's take a look at the data with jitter:

```{r, fig.height=2, warning=FALSE}
ggplot(dat, aes(Age, Survived)) +
    geom_jitter(height=0.1, alpha=0.25)
```

Recall that the linear regression can be done with the `lm` function:

```{r}
res_lm <- lm(Survived ~ Age, data=dat)
summary(res_lm)
```

In this case, the regression line is ```r res_lm$coefficients[1]``` + ```r res_lm$coefficients[2]``` `Age`.

A GLM can be fit in a similar way with the `glm` function -- we just need to indicate what type of regression we're doing (binomial? poission?) and the link function. We are doing bernoulli (binomial) regression, since the response is binary (0 or 1); lets choose a `probit` link function (called `g` in your Lecture 1 notes, in Equation (2)).

```{r}
res_glm <- glm(factor(Survived) ~ Age, data=dat, family=binomial(link="probit"))
```

The `family` argument takes a __function__, indicating the type of regression. See `?family` for the various types of regression allowed by `glm`. For Lab 1, you will need `binomial` and `poisson`.

Notice that `glm` outputs an object that's a special case of the `lm` object:

```{r}
class(res_lm)
class(res_glm)
```

This means that the familiar functions like `summary` and `predict` that can be applied to an `lm` object will also work here. Let's see a summary of the GLM regression:

```{r}
summary(res_glm)
```

We can make predictions too, but this is not as straight-forward as in `lm`:

```{r}
pred <- predict(res_glm)
qplot(dat$Age, pred) + labs(x="Age", y="Default Predictions")
```

Negative predictions? Huh?? Well, it turns out this is just the linear predictor, ```r res_glm$coefficients[1]``` + ```r res_glm$coefficients[2]``` `Age` ($\eta$ in your Lecture 1 notes) -- let's add that line to the plot:

```{r, warning=FALSE}
qplot(dat$Age, predict(res_glm)) + 
    labs(x="Age", y="Default Predictions") +
    geom_abline(intercept = res_glm$coefficients[1],
                slope = res_glm$coefficients[2],
                colour="blue")
```

The documentation for the `predict` function on `glm` objects can be found by typing `?predict.glm`. Notice that the `predict` function allows you to specify the *type* of predictions to be made. To make predictions on the mean (probability of `Survived=1`), indicate `type="response"`, which is the equivalent of applying the inverse link function ($g^{-1}$ in your Lecture notes) to the linear predictor.

```{r}
pred <- predict(res_glm, type="response")
qplot(dat$Age, pred) + labs(x="Age", y="Mean Predictions")
```

Look closely -- these predictions don't actually fall on a straight line. They follow an inverse probit function (N(0,1) cdf):

```{r}
mu <- function(x) pnorm(res_glm$coefficients[1] + res_glm$coefficients[2] * x)
qplot(dat$Age, pred) + 
    labs(x="Age", y="Mean Predictions") +
    stat_function(fun=mu, colour="blue") +
    scale_x_continuous(limits=c(-200, 200))
```

Point of reflection: even though the mean is approximately linear, is it still preferable to use the GLM, or a linear model with no assumptions on the error distribution?