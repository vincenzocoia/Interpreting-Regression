--- 
title: "Interpreting Regression"
author: "Vincenzo Coia"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "My tutorials for regression analysis, in the form of a bookdown book."
---

<!--chapter:end:index.Rmd-->

---
title: "DSCI 562 Lab 4 Tutorial: GAM"
output: html_document
---

```{r, warning=FALSE, echo=FALSE}
library(knitr)
library(ggplot2)
opts_chunk$set(warning=FALSE, fig.width=5, fig.height=3, fig.align="center")
```

## Generalized Additive Models

To fit a GAM in R, we could use:

1. the function `gam` in the `mgcv` package, or
2. the function `gam` in the `gam` package. 

Differences between the two functions are discussed in the "Details" section of the `gam` documentation in the `mgcv` package. Choose one, but don't load both! `mgcv` tends to be updated more frequently, and is generally more flexible (compare the Index pages), so that's what is used in this tutorial. But the `gam` package has similar workings. 

The `gam` function works similarly to other regression functions, but the formula specification is different. Let's go through different formula specifications, doing regression on the `mtcars` dataset in R.

The formula `mpg ~ disp + wt` gives you a _linear model_. It indicates that `disp` and `wt` both enter the model in a linear fashion.

```{r}
library(mgcv)
fit1 <- gam(mpg ~ disp + wt, data=mtcars)
fit2 <- lm(mpg ~ disp + wt, data=mtcars)
summary(fit1)
summary(fit2)
```

Notice that the coefficient estimates are the same.

To make a term non-parametric, wrap the `s` function around the term (for _splines_; comes with the `mgcv` package). The `gam` package also has a `lo` function, for _loess_ smoothing.

```{r}
fit3 <- gam(mpg ~ s(disp) + s(wt), data=mtcars)
summary(fit3)
```

Now, each predictor enters the model in a non-parametric, additive form. The nonparametric functions can be accessed by calling `plot`. For documentation, see `?plot.gam`. Let's plot the "bivariate" scatterplots behind these curves too (these bivariate data actually use partial residuals).

```{r}
plot(fit3, residuals=TRUE)
```

Looks like the "weight" variable (`wt`) is quite linear. We can let it be linear, while the `disp` variable remains nonparametric. "Wiggliness" of the smoothed fit can be controlled through the `k` argument of the `s` function, but this is chosen in a "smart" way by default.

```{r}
fit4 <- gam(mpg ~ s(disp, k=3) + wt, data=mtcars)
summary(fit4)
plot(fit4, residuals=TRUE)
```

You can even combine predictors into a common smooth function:

```{r}
fit5 <- gam(mpg ~ s(disp, qsec) + wt, data=mtcars)
summary(fit5)
```

For each, the `predict` and `residuals` functions work in the same old way. Let's use them to make a residual plot:

```{r}
qplot(predict(fit5), residuals(fit5)) +
    geom_abline(intercept=0, slope=0, linetype="dashed") +
    xlab("Prediction (mean)") + ylab("Residuals")
```

For their documentation, see `?predict.gam` and `?residuals.gam`.

<!--chapter:end:gam_in_r.Rmd-->

---
title: "GLM's in R"
output: html_document
---

This document introduces the `glm` function in R for fitting a Generlized Linear Model (GLM). We'll work with the `titanic_train` dataset in the `titanic` package.

```{r}
library(ggplot2)
library(titanic)
dat <- na.omit(titanic_train)
str(dat)
```

## Binomial Regression

Consider the regression of `Survived` on `Age`. Let's take a look at the data with jitter:

```{r, fig.height=2, warning=FALSE}
ggplot(dat, aes(Age, Survived)) +
    geom_jitter(height=0.1, alpha=0.25)
```

Recall that the linear regression can be done with the `lm` function:

```{r}
res_lm <- lm(Survived ~ Age, data=dat)
summary(res_lm)
```

In this case, the regression line is ```r res_lm$coefficients[1]``` + ```r res_lm$coefficients[2]``` `Age`.

A GLM can be fit in a similar way with the `glm` function -- we just need to indicate what type of regression we're doing (binomial? poission?) and the link function. We are doing bernoulli (binomial) regression, since the response is binary (0 or 1); lets choose a `probit` link function (called `g` in your Lecture 1 notes, in Equation (2)).

```{r}
res_glm <- glm(factor(Survived) ~ Age, data=dat, family=binomial(link="probit"))
```

The `family` argument takes a __function__, indicating the type of regression. See `?family` for the various types of regression allowed by `glm`. For Lab 1, you will need `binomial` and `poisson`.

Notice that `glm` outputs an object that's a special case of the `lm` object:

```{r}
class(res_lm)
class(res_glm)
```

This means that the familiar functions like `summary` and `predict` that can be applied to an `lm` object will also work here. Let's see a summary of the GLM regression:

```{r}
summary(res_glm)
```

We can make predictions too, but this is not as straight-forward as in `lm`:

```{r}
pred <- predict(res_glm)
qplot(dat$Age, pred) + labs(x="Age", y="Default Predictions")
```

Negative predictions? Huh?? Well, it turns out this is just the linear predictor, ```r res_glm$coefficients[1]``` + ```r res_glm$coefficients[2]``` `Age` ($\eta$ in your Lecture 1 notes) -- let's add that line to the plot:

```{r, warning=FALSE}
qplot(dat$Age, predict(res_glm)) + 
    labs(x="Age", y="Default Predictions") +
    geom_abline(intercept = res_glm$coefficients[1],
                slope = res_glm$coefficients[2],
                colour="blue")
```

The documentation for the `predict` function on `glm` objects can be found by typing `?predict.glm`. Notice that the `predict` function allows you to specify the *type* of predictions to be made. To make predictions on the mean (probability of `Survived=1`), indicate `type="response"`, which is the equivalent of applying the inverse link function ($g^{-1}$ in your Lecture notes) to the linear predictor.

```{r}
pred <- predict(res_glm, type="response")
qplot(dat$Age, pred) + labs(x="Age", y="Mean Predictions")
```

Look closely -- these predictions don't actually fall on a straight line. They follow an inverse probit function (N(0,1) cdf):

```{r}
mu <- function(x) pnorm(res_glm$coefficients[1] + res_glm$coefficients[2] * x)
qplot(dat$Age, pred) + 
    labs(x="Age", y="Mean Predictions") +
    stat_function(fun=mu, colour="blue") +
    scale_x_continuous(limits=c(-200, 200))
```

Point of reflection: even though the mean is approximately linear, is it still preferable to use the GLM, or a linear model with no assumptions on the error distribution?

<!--chapter:end:glm_in_r.Rmd-->

---
title: "Interpreting Regression: Rough Outline"
output: github_document
---

# 

# Intro to Regression

## What is regression?

It's the estimation of a probabilistic quantity of a numeric random variable, conditional on some predictors. 

[Explain this]

## The two main tasks that regression tries to solve

prediction and interpretation

Example 1: (Basic numeric example, perhaps with y in R)

Example 2: (Another numeric example, perhaps with y being a count)

Example 3: (binary response) There's a 5% that a person will get pregnant through an in-vivo insemination procedure. It's well-know that age is an important factor. It also depends on how long after the leutenizing hormone (LH) first spiked.

- Prediction: Given the person's age, what is the chance of pregnancy?
- Interpretation: How does age influence the chance of pregnancy? What about LH? What about LH for people over 40?

### Prediction

[Explain it here; start univariate]

# The model-fitting paradigms in R and python

<!--chapter:end:interpreting_regression-rough_outline.Rmd-->

---
title: "Mixed Effects Models in R: tutorial"
author: "Vincenzo Coia"
date: '2017-01-15'
output: html_document
---

```{r, warning=FALSE, echo=FALSE}
library(knitr)
opts_chunk$set(warning=FALSE, fig.width=5, fig.height=3, fig.align="center")
```

## Mixed-Effects Models

Two R packages exist for working with mixed effects models: `lme4` and `nlme`. We'll be using the `lme4` package (check out [this](http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models) discussion on Cross Validated for a comparison of the two packages).

```{r}
library(lme4)
```

In Lab 1, we compared linear regression (function `lm`) with GLM's (function `glm`). In Lab 2, we consider adding a random effect to either of these:

- A linear model with random effects is a _Linear Mixed-Effects Model_, and is fit using the `lmer` function.
- A generalized linear model with random effects is a _Generalized Linear Mixed-Effects Model_, and is fit using the `glmer` function.

We'll work with the `esoph` data set, to see how the number of controls `ncontrols` affects the number of cases `ncases` based on age group `agegp`. Here's what the data look like (with a tad bit of vertical jitter):

```{r}
library(ggplot2)
library(dplyr)
dat <- esoph
p <- ggplot(dat, aes(ncontrols, ncases)) +
    geom_jitter(aes(colour=agegp), height=0.25)
p
```

Since the response is a count variable, we'll go ahead with a Poisson regression -- a Generalized Linear Mixed-Effects Model. The model is
$$ Y_{ij} \mid X_{ij} = x_{ij} \sim \text{Poisson}\left(\lambda_{ij}\right) $$
for each observation $i$ on the $j$'th age group, where $Y_{ij}$ is the number of cases, $X_{ij}$ is the number of controls, and $\lambda_{ij}$ is the conditional mean of $Y_{ij}.$ We model $\lambda_{ij}$ as
$$ \log\left(\lambda_{ij}\right) = \left(\beta_0 + b_{0j}\right) + \left(\beta_1 + b_{1j}\right) x_{ij}, $$
where $b_{0j}$ and $b_{1j}$ are joint (bivariate) normally distributed with zero mean. 

What does this model mean? First, it means that the mean is exponential in the explanatory variable, since we chose a $\log$ link function. Second, each age group ($j$) gets its own mean curve, via its own linear predictor. But we're saying that these linear predictors are related: the collection of slopes and intercepts across age groups are centered around $\beta_0$ and $\beta_1$ (respectively, called the _fixed effects_), and the slope and intercept of each age group departs from this center according to some Gaussian random noise (the $b$ terms, called the _random effects_).

Let's fit the model. Then we'll go through the formula, and the output.

```{r}
fit <- glmer(ncases ~ ncontrols + (1 + ncontrols | agegp), 
             data=dat, 
             family=poisson)
summary(fit)
```

To specify the formula, the fixed effects part is the same as usual: `ncases ~ ncontrols` gives you `ncases = beta0 + beta1 * ncontrols`. Note that the intercept is put in there by default. Then, we need to indicate which explanatory variables are getting the random effects -- including the intercept this time (with a 1), if you want it (in this case, we do). The random effects can be indicated in parentheses, separated by `+`, followed by a `|`, after which the variable(s) that you wish to group by are indicated. So `|` can be interpreted as "grouped by".

The output of the model fit is similar to what you've seen before (in `glm` for example), but the "random effects" part is new. That gives us the estimates of the joint normal distribution of the random effects -- through the variances, and correlation matrix to the right (only the lower-diagonal of the correlation matrix is given, because that matrix is symmetric anyway).

Let's see what the intercepts and slopes for each age group are, and let's plot the estimated mean curves:

```{r}
(coef_fit <- coef(fit)$agegp)
## Colours with stat_function are not nice to deal with. Do manually.
p + stat_function(aes(colour="25-34"), fun = function(x) exp(coef_fit[1,1] + coef_fit[1,2]*x)) +
    stat_function(aes(colour="35-44"), fun = function(x) exp(coef_fit[2,1] + coef_fit[2,2]*x)) +
    stat_function(aes(colour="45-54"), fun = function(x) exp(coef_fit[3,1] + coef_fit[3,2]*x)) +
    stat_function(aes(colour="55-64"), fun = function(x) exp(coef_fit[4,1] + coef_fit[4,2]*x)) +
    stat_function(aes(colour="65-74"), fun = function(x) exp(coef_fit[5,1] + coef_fit[5,2]*x)) +
    stat_function(aes(colour="75+"),   fun = function(x) exp(coef_fit[6,1] + coef_fit[6,2]*x))
```

A (response-) residual plot is somewhat sensible to look at here:

```{r}
plot(fit)
```

Looks fairly centered at zero, so the shape of the mean curves are satisfactory.

<!--chapter:end:lme_in_r.Rmd-->

---
title: "From Linear Regression to Mixed Effects Models"
output: github_document
---

## Motivation for LME

Let's take a look at the `esoph` data set, to see how the number of controls `ncontrols` affects the number of cases `ncases` of cancer for each age group `agegp`. Here's what the data look like (with a tad bit of vertical jitter):

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
suppressMessages(library(tidyverse))
dat <- as_tibble(esoph) %>% 
    mutate(agegp = as.character(agegp))
(p <- ggplot(dat, aes(ncontrols, ncases, group=agegp, colour=agegp)) +
    geom_jitter(height=0.25) +
    scale_colour_discrete("Age Group") +
    ylab("Number of Cases") + xlab("Number of Controls"))
```

It seems each age group has a different relationship. Should we then fit regression lines for each group separately? Here's what we get, if we do:

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
p + geom_smooth(method="lm", se=FALSE, size=0.5)
```

But, each group has so few observations, making the regression less powerful:

```{r, echo=FALSE}
dat %>% 
    group_by(agegp) %>% 
    summarise(n=length(ncases)) %>% 
    knitr::kable()
```

__Question__: can we borrow information across groups to strengthen regression, while still allowing each group to have its own regression line?

Yes -- we can use _Linear Mixed Effects_ (LME) models. An LME model is just a linear regression model for each group, with different slopes and intercepts, but the collection of slopes and intercepts _is assumed to come from some normal distribution_.

## Definition

With one predictor ($X$), we can write an LME as follows:
$$ Y = \left(\beta_0 + b_0\right) + \left(\beta_1 + b_1\right) X + \varepsilon,  $$
where the error term $\varepsilon$ has mean zero, and the $b_0$ and $b_1$ terms are normally distributed having a mean of zero, and some unknown variances and correlation. The $\beta$ terms are called the _fixed effects_, and the $b$ terms are called the _random effects_. Since the model has both types of effects, it's said to be a _mixed_ model -- hence the name of "LME". 

Note that we don't have to make _both_ the slope and intercept random. For example, we can remove the $b_0$ term, which would mean that each group is forced to have the same (fixed) intercept $\beta_0$. Also, we can add more predictors ($X$ variables).

## R Tools for Fitting

Two R packages exist for working with mixed effects models: `lme4` and `nlme`. We'll be using the `lme4` package (check out [this](http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models) discussion on Cross Validated for a comparison of the two packages).

Let's fit the model. We need to indicate a formula first in the `lmer` function, and indicate the data set we're using.

```{r}
library(lme4)
fit <- lmer(ncases ~ ncontrols + (ncontrols | agegp), 
            data=dat)
```

Let's take a closer look at the _formula_, which in this case is `ncases ~ ncontrols + (ncontrols | agegp)`. 

On the left of the `~` is the response variable, as usual (just like for `lm`). On the right, we need to specify both the fixed and random effects. The fixed effects part is the same as usual: `ncontrols` indicates the explanatory variables that get a fixed effect. Then, we need to indicate which explanatory variables get a random effect. The random effects can be indicated in parentheses, separated by `+`, followed by a `|`, after which the variable(s) that you wish to group by are indicated. So `|` can be interpreted as "grouped by".

Now let's look at the model output:

```{r}
summary(fit)
```

The random and fixed effects are indicated here.

- Under the "Random effects:" section, we have the variance of each random effect, and the lower part of the correlation matrix of these random effects.
- Under the "Fixed effects:" section, we have the estimates of the fixed effects, as well as the uncertainty in the estimate (indicated by the Std. Error).

We can extract the collection of slopes and intercepts for each group using the `coef` function:

```{r}
(par_coll <- coef(fit)[[1]])
```

Let's put these regression lines on the plot:

```{r, echo=FALSE, fig.width=4, fig.height=3, fig.align="center"}
## Put the slopes and intercepts with the data frame:
par_coll %>% 
    rownames_to_column("agegp") %>% 
    left_join(dat, by="agegp")
par_coll <- rownames_to_column(par_coll)
dat <- ddply(dat, ~ agegp, function(df){
    pars <- subset(par_coll, rowname==unique(df$agegp))
    int <- pars$`(Intercept)`
    slp <- pars$ncontrols
    cbind(df, intercept=int, slope=slp)
})

## Plot
ggplot(dat, aes(ncontrols, ncases, group=agegp, colour=agegp)) +
    geom_jitter(height=0.25) +
    geom_abline(aes(intercept=intercept, slope=slope, colour=agegp)) +
    scale_colour_discrete("Age Group") +
    ylab("Number of Cases") + xlab("Number of Controls")
```

So, each group still gets its own regression line, but tying the parameters together with a normal distribution gives us a more powerful regression.

<!--chapter:end:lme.Rmd-->

---
title: "DSCI 562 Tutorial: Missing Data"
output: pdf_document
---

```{r, warning=FALSE, echo=FALSE}
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(tidyverse))
opts_chunk$set(warning=FALSE, fig.width=5, fig.height=3, fig.align="center")
```

Let's take a closer look at mean imputation vs. multiple imputation.

## Mean Imputation

Let's consider a simple linear regression example, with one explanatory variable. We'll generate 100 data points, and make 10 of the response values missing. 

```{r}
set.seed(13)
x <- rnorm(100)
y <- -1 + 2 * x + rnorm(100)
y[1:10] <- NA
```

Here are the data:

```{r}
x
y
```

Here's the scatterplot with the missing data removed, and the corresponding linear regression fit:

```{r}
p <- qplot(x, y) + geom_smooth(method="lm", se=FALSE)
p
```

The mean imputation method replaces the `NA`'s with an estimate for the mean of $Y$. The simplest case is to use the sample average of the response. The imputed observations are shown in red, and the resulting `lm` fit is also in red.

```{r}
ybar <- mean(y, na.rm=TRUE)
datrm <- na.omit(data.frame(x=x, y=y))
datimp <- data.frame(x=x[1:10], y=ybar)
p + geom_point(data=datimp, colour="red") +
    geom_smooth(data=rbind(datrm, datimp), method="lm", se=FALSE, colour="red")
```

Notice that the new regression line is flatter.

Another mean-imputation method is to replace the `NA`'s with an alternative mean estimate: the regression predictions. 

```{r}
fit2 <- lm(y ~ x, na.action=na.omit)
yhat <- predict(fit2, newdata=data.frame(x=x[1:10]))
datimp2 <- data.frame(x=x[1:10], y=yhat)
p + geom_point(data=datimp2, colour="red") +
    geom_smooth(data=rbind(datrm, datimp2), method="lm", se=FALSE, colour="red", size=0.5)
```

The regression line has not changed. This method seems smarter, but it still has consequences, since the imputed data suggests that the dataset is bound closer to the regression line than reality. So the residual variance is biased to be smaller.

These are both mean imputation methods. So, in your Lab 2 assignment, you can use any mean imputation method -- your explanation of the comparison will just depend on what you choose.



## Multiple Imputation

Recall that _multiple imputation_ is a technique for handling missing data. It replaces the missing data with _many_ plausible values, to obtain mutliple data sets. An analysis is done on each data set, and the results are combined.

A very powerful R package to assist with multiple imputation is the `mice` package. Some key things that it does:

- Displays patterns in missing data.
- Imputes data to obtain multiple data sets.
- Pools multiple analyses into one.

We'll look at the `airquality` dataset in R.

```{r}
library(mice)
head(airquality)
```

### Patterns

Where are the `NA`s?

```{r}
md.pattern(airquality)
```

A "1" indicates that an observation is present, and a "0" indicates absense. The periphery of the matrix are counts: to the right, are the number of `NA`s in the row; at the bottom, are the number of `NA`s in each column; to the left, are the number of observations having a missing data pattern indicated in the matrix. 

Caution: you can't always trust the `md.pattern` function. Try it on the titanic dataset, for instance.

So we can see that there are 7 missing Solar Radiation observations, and 37 missing Ozone observations. We could check that in another way as follows:

```{r}
sum(is.na(airquality$Solar.R))
sum(is.na(airquality$Ozone))
```

### Multiple Imputation

There are many methods of doing an imputation. But generally, they use other columns in the data set to do prediction on the missing data. 

The function to do this is `mice`. Let's impute 50 data sets using the "Predictive Mean Matching" method.

```{r}
(dats <- mice(airquality, m=50, method="pmm", seed=123, printFlag=FALSE))
```

The `m` argument is the number of imputed datasets. `method` is the method (you can check out the other methods in the "Details" part of the documentation of `mice`). Because there's a random component to the imputation, `seed` indicates the seed to initiate the random number generator -- useful for reproducibility! Finally, I didn't want `mice` to be verbose with its output, so I silenced it with `printFlag=FALSE`. 

`dats` isn't just a list of 50 datasets. It has more information bundled in it. The info is bundled in an object of type "mids":

```{r}
class(dats)
```

But we can extract the data sets. Want to see the fourth imputed data set? Here it is:

```{r}
head(complete(dats, 4))
```

### Pooling

The `mice` package allows you to pool many types of regression analyses. Let's try a simple linear regression to predict `Ozone` from `Solar.R`, `Wind`, and `Temp`. You'll need to use base R's `with` function. 

```{r}
fits <- with(dats, lm(Ozone ~ Solar.R + Wind + Temp))
```

If you were to print `fits` to the screen, it would look like a list of 50 regression fits -- one for each of the imputed data sets. But it's not. Take a look:

```{r}
names(fits)
```

Like `dats`, `fits` has more info in it. But it _does_ have the 50 regression fits. And they can be pooled using the `pool` function:

```{r}
(fit <- pool(fits))
summary(fit)
```

And there are the results of the pooled fit. This pooling works for more than just `lm`!

<!--chapter:end:missing_data_in_r.Rmd-->

---
title: "Non-identifiability"
author: "Vincenzo Coia"
output: html_document
---

Here is some help on Lab 4 Exercise 1(b). Exercise 1(b) is intended to get you to think about what the $h$ functions in a Generalized Additive Model (GAM) are.

An interpretation of the $h$ functions can only make sense in light of the *non-identifiability* issue of GAM's, so that's discussed first. Then, hints are given for the first two questions in Exercise 1(b). 

## Non-identifiability

What is "non-identifiability", exactly? It can happen for any model that's not carefully specified (not just GAM's). Let's look at an example first.

In simple linear regression, why not write the model
$$ Y = \beta_0 + \alpha_0 + \beta_1 X + \varepsilon, $$
where $\mathbb{E}(\varepsilon)=0$? It's because three parameters are too many to describe a line. In other words, \textbf{more than one parameter selection can give you the same regression line}. For example, the model $Y=1+X+\varepsilon$ can be written with
$$ \beta_0 = 0, \alpha_0 = 1, \beta_1 = 1, $$
or
$$ \beta_0 = -1, \alpha_0 = 2, \beta_1 = 1, $$
etc. In fact, as long as $\alpha_0 = 1 - \beta_0$, and $\beta_1=1$, we get the same regression line. 

In general, and roughly speaking, when more than one parameter selection gives you the same model, there's a non-identifiability issue. It leads to problems in estimation and estimator properties. It also leads to an \emph{interpretation} problem: the parameters don't have a meaning, since they can represent more than one thing in the model.

This is even true in non-parametric cases, such as the GAM. Let's look at a two-predictor GAM:
$$ Y = \beta_0 + h_1\left(X_1\right) + h_2\left(X_2\right) + \varepsilon, $$
where $\beta_0$ is any real number, $h_1$ and $h_2$ are any smooth functions, and $\mathbb{E}(\varepsilon)=0$. As it is, this model is non-identifiable: if you pick a $\beta_0$, $h_1$, and $h_2$, I can find another set of $\beta_0$, $h_1$, and $h_2$ that gives the same regression surface. How? I can just add a constant $c$ to your $\beta_0$, and subtract that constant from your, say, $h_1$ (i.e., "vertically shift" your $h_1$ function downwards by $c$). 

So, the "parameters" (which includes the $h$ functions) in a GAM are non-identifiable -- the $h$ functions can be vertically shifted, and $\beta_0$ can just compensate for these shifts to give the same regression surface.

To make the model identifiable, we force the $h$ functions to be vertically centered at zero. Here's how: we ensure that after transforming the $j$'th predictor to $h_j\left(X_j\right)$, the resulting data are centered at 0. Mathematically, we ensure that
\begin{equation} \label{eq:restrict}
\frac{1}{n}\sum_{i=1}^{n}h_j\left(x_{ij}\right) = 0 
\end{equation}
for each predictor $j$, where $x_{ij}$ for $i=1,\ldots,n$ are the observations. 

## Question 1b

__Notation__: Let's call $\hat{\beta}_0$ the estimate of $\beta_0$, and the the functions $\hat{h}_1$ and $\hat{h}_2$ the estimates of $h_1$ and $h_2$, respectively.

The prediction on observation $i$, denoted $\hat{Y}_i$, is
$$ \hat{Y}_i = \hat{\beta}_0 + \hat{h}_1\left(x_{i1}\right) + \hat{h}_1\left(x_{i2}\right). $$
This will help with the first question:

> Suppose the `gam` fit is called `fit`. Why is `mean(predict(fit))` the same as the estimate of the intercept?


Here's a hint: `predict(fit)` gives you the vector $\hat{Y}_1, \ldots, \hat{Y}_n$. Then, `mean` averages them. The question is asking you to indicate why we have
$$ \frac{1}{n}\sum_{i=1}^{n}\hat{Y}_i = \hat{\beta}_0. $$
The answer uses Equation \eqref{eq:restrict}.

The next question asks you to think about how you'd recover an $h$ function. It asks:

> For each $h$ function, write an R function that evaluates the $h$ function over a grid of values, without calling the \code{plot} function on the fit. Show that the function works by evaluating it over a small grid of values.

Suppose you want to evaluate function $\hat{h}_1$ at some generic point $x_0$. You can do this using the `predict` function, and somehow specifying $x_0$ in the `newdata` argument (in place of "predictor 1"). But `predict` will give you all three components of the model, added together: the $\hat{\beta}_0$ part, plus the $\hat{h}_1$ part (evaluated at whatever is in the "predictor 1" column), plus the $\hat{h}_2$ part (evaluated at whatever is in the "predictor 2" column). Your job is to "isolate" the $\hat{h}_1$ part, evaluated at $x_0$. We can subtract out $\hat{\beta}_0$, which is specified in the model output. But you can't just subtract out the $\hat{h}_2$ part, because we don't know it. Your job is to use a property of $\hat{h}_2$ (hint: Equation \eqref{eq:restrict}) to remove it. 

You can also think of it this way: if `mean(predict(fit))` "zeroes-out" both $h$ functions, how can you modify the prediction data so that one of the $h$ functions *doesn't* zero-out, but instead evaluates at some desired point?

<!--chapter:end:non_identifiability_gams.Rmd-->

---
title: "DSCI 562 Lab 4 Tutorial: Robust Regression"
output: html_document
---


```{r, warning=FALSE, echo=FALSE}
library(knitr)
library(ggplot2)
opts_chunk$set(warning=FALSE, fig.width=5, fig.height=3, fig.align="center")
```


## Robust Regression

There are many R packages out there to assist with robust estimation. It depends on the task at hand. We'll go over a few.

### Robust Regression

For robust linear regression, there are a few options. 

There's the `rlm` function in the `MASS` package. It works in a similar way as the `lm` function. Can also use the functions `predict`, `residuals`, `coefficients`, etc. on the output. I like this option because it allows for different $\psi$ functions besides the Huber.

```{r}
library(MASS)
(fit6 <- rlm(mpg ~ disp + wt, data=mtcars))
(fit7 <- rlm(mpg ~ disp + wt, data=mtcars, psi=psi.bisquare))
```

The package `robustbase` has the function `lmrob` for linear models, but also has `glmrob` for GLM's. Similarly, the `robust` package has similar functions `lmRob` and `glmRob`. 

A robust version of GAM's can be obtained with the `robustgam` function in the `robustgam` package. 

A robust version of LME's can be obtained with the `rlmer` function in the `robustlmm` package. 

### Heavy Tailed Regression

For a heavy tailed extension of `lm`, one can use the `tlm` function in the `hett` package. The package `heavy` has some regression techniques using heavy tailed distributions. 

<!--chapter:end:robust_in_r.Rmd-->

---
title: "Splines and Loess Regression"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width=5, fig.height=3, fig.align="center")
```

This tutorial describes spline and loess regression in R.

## Splines

You can think of splines as regression between knots. We'll use the `splines` package to do this.

Here's some generated data:

```{r}
library(ggplot2)
x <- rnorm(1000)
y <- x^2 + rnorm(1000)
qplot(x, y, alpha=I(0.5)) + theme_bw()
```

First, we need to "set up" the regression by placing knots.

```{r}
library(splines)
x2 <- ns(x, knots=c(-2, 0, 2))
```

Now we can do regression between these knots, with the natural spline shape (as opposed to linear):

```{r}
fit <- lm(y ~ x2)
qplot(x, y, alpha=I(0.5)) +
    geom_line(data=data.frame(x=x, y=predict(fit)), 
              mapping=aes(x, y), colour="blue") + 
    theme_bw()
```

## Loess

Loess is "local regression", and is based on the idea of estimating the mean response based on similar observed data in the predictor space.

__Kernel smoothing__ is a method that estimates the mean using a weighted sample average, where observations that are further away in the predictor space get downweighted more, according to some kernel function.

__Local polynomials__ is a method that takes kernel smoothing one step further: instead of a weighted sample average, we use nearby data (in the predictor space) to fit a polynomial regression, and then fit a polynomial regression model.

There is a more basic version, too: a __"moving window"__, described next, before seeing how loess is done in R.

### The "Moving Window"

The "moving window" approach is a special type of kernel smoother. For a given value of the predictor $X=x$, the mean response is estimated as the sample average of all response values whose predictor values are "near" $x$ -- within some distance $h$.

For example, if you want to estimate the mean number of "runs" of a baseball team when walks=100 and hits=1000, only look at cases where "walks" is approximately 100, "hits" is approximately 1000, and then average the response.

It's a special case of kernel regression, with a kernel function of 
$$ k\left(t\right) = I\left(|t-x| < h \right), $$ 
sometimes called the "boxcar" function. 

__Note the similarity to kNN!__ kNN regression uses the nearest $k$ points, resulting in a variable distance $h$, whereas the moving window regression uses a fixed distance $h$, resulting in a variable number of points $k$ used.

### `ggplot2`

`ggplot2` comes with a fairly powerful tool for plotting a smoother, with `geom_smooth`.

```{r}
qplot(x, y) +
    geom_smooth(method="loess") + 
    theme_bw()
```

You can choose the bandwidth through the `span` argument:

```{r}
qplot(x, y) +
    geom_smooth(method="loess", span=0.1) +
    theme_bw()
```

Too wiggly. The default looks fine.

Note that `geom_smooth` can fit `lm` and `glm` fits too:

```{r}
qplot(x, y) +
    geom_smooth(method="lm", formula=y~x+I(x^2)) + 
    theme_bw()
```

### Manual method

You can use the `loess` or `ksmooth` function in R to do kernel smoothing yourself. You get more flexibility here, since you can get things such as predictions. It works just like the other regression functions:

```{r}
(fit <- loess(y ~ x))
```

We can make predictions as usual:

```{r}
yhat <- predict(fit)
qplot(x, y) +
    geom_line(data=data.frame(x=x, y=yhat), mapping=aes(x, y), 
              colour="blue") + 
    theme_bw()
```

If you want the standard errors of the predictions, you can indicate this with `se=TRUE` in the `predict` function. 

Some key things that you might want to change in your kernel smoothing regression, through arguments in the `loess` function:

- Bandwidth/smoothing parameter. Indicate through the `span` argument. 
- Degree of the local polynomial fitted. Indicate through the `degree` argument.
- Kernel function.

The kernel function is not readily specified in `loess`. But you can use the `ksmooth`, where you're allowed to specify a "box" or "normal" kernel.

<!--chapter:end:splines_and_loess_in_r.Rmd-->

---
title: "Timeseries objects in R"
output: github_document
---

__To add__: `times()` function to extract times from a `ts` object. How to deal with the `start` and `end` arguments when declaring a `ts` object.

This tutorial demonstrates `timeseries` objects, and `stl` decomposition.

Let's make a periodic time series with a trend. The data can be contained in a vector:

```{r}
p <- 10
n <- 20*p
dat <- 100 + sqrt(1:n) + 5*sin(1:n * 2*pi/p) + rnorm(n)
```

It's sometimes useful to make an object of type `timeseries`. Do this with the `ts` function in R. But, if there's a cycle, we'll need to indicate that in the `frequency` argument, which is the number of observations per cycle. In this case, the period is `r p`. 

```{r}
(datts <- ts(dat, frequency = p))
```

You can plot this object too. You'll get a nice looking time series plot:

```{r fig.height=3}
plot(datts)
```

And now you can decompose the trend, seasonal component, and error terms with `stl`. Note that `stl` requires a `timeseries` object! Be sure to put `s.window="periodic"` in the `stl` function to use the periodicity of the `timeseries` object. Notice that there are options to change the bandwidths of the loess estimation, along with the degree of the local polynomial, with the `_.degree` and `_.window` arguments.  

```{r}
fit <- stl(datts, s.window="periodic")
```

The estimates are contained in the `$time.series` part of the output:

```{r}
head(fit$time.series)
```

<!--chapter:end:timeseries_objects_in_r.Rmd-->

